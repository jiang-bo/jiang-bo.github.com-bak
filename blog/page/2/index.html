
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>非纯种程序猿</title>
  <meta name="author" content="jiang-bo">

  
  <meta name="description" content="main() public static void main(String args[]) { try { StringUtils.startupShutdownMessage(DataNode.class, args, LOG); DataNode datanode = &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://jiangbo.me/blog/page/2/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/ender.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="非纯种程序猿" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-34477986-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:jiangbo.me" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
    <li><a href="/">
        <span class="blue_light">
            非纯种程序猿
        </span>
       
    </a></li>
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/about-me">About me</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/10/18/hdfs-datanode-startup/">HDFS源码学习（13）——DataNode启动过程</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-10-18T21:57:00+08:00" pubdate data-updated="true">Oct 18<span>th</span>, 2012</time>
        
         | <a href="/blog/2012/10/18/hdfs-datanode-startup/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h2>main()</h2>

<pre><code>  public static void main(String args[]) {
    try {
      StringUtils.startupShutdownMessage(DataNode.class, args, LOG);
      DataNode datanode = createDataNode(args, null);
      if (datanode != null)
        datanode.join();
    } catch (Throwable e) {
      LOG.error(StringUtils.stringifyException(e));
      System.exit(-1);
    }
  }
</code></pre>

<h2>createDataNode()</h2>

<pre><code>  public static DataNode createDataNode(String args[],
                                 Configuration conf) throws IOException {
    // 初始化datanode
    DataNode dn = instantiateDataNode(args, conf);
    // 启动datanode后台线程
    runDatanodeDaemon(dn);
    return dn;
  }
</code></pre>

<h3>1.instantiateDataNode（）</h3>

<pre><code>  public static DataNode instantiateDataNode(String args[],
                                      Configuration conf) throws IOException {
    // 处理配置
    if (conf == null)
      conf = new Configuration();
    if (!parseArguments(args, conf)) {
      printUsage();
      return null;
    }
    if (conf.get("dfs.network.script") != null) {
      LOG.error("This configuration for rack identification is not supported" +
          " anymore. RackID resolution is handled by the NameNode.");
      System.exit(-1);
    }
    // 获取data目录配置
    String[] dataDirs = conf.getStrings("dfs.data.dir");
    dnThreadName = "DataNode: [" +
                        StringUtils.arrayToString(dataDirs) + "]";
    //创建datanode实例
    return makeInstance(dataDirs, conf);
  }
</code></pre>

<h4>1.1. makeInfstance()</h4>

<p>该方法主要用于检查给定的data目录中至少有一个可以创建，并实例化DataNode</p>

<pre><code>  public static DataNode makeInstance(String[] dataDirs, Configuration conf)
    throws IOException {
    ArrayList&lt;File&gt; dirs = new ArrayList&lt;File&gt;();
    for (int i = 0; i &lt; dataDirs.length; i++) {
      File data = new File(dataDirs[i]);
      try {
        DiskChecker.checkDir(data);
        dirs.add(data);
      } catch(DiskErrorException e) {
        LOG.warn("Invalid directory in dfs.data.dir: " + e.getMessage());
      }
    }
    if (dirs.size() &gt; 0) 
      return new DataNode(conf, dirs);
    LOG.error("All directories in dfs.data.dir are invalid.");
    return null;
  }
</code></pre>

<h4>1.2 new DataNode()</h4>

<pre><code>  DataNode(Configuration conf, 
           AbstractList&lt;File&gt; dataDirs) throws IOException {
    // 设置配置信息
    super(conf);
    datanodeObject = this;
    supportAppends = conf.getBoolean("dfs.support.append", false);
    this.conf = conf;
    try {
      // 启动DataNode
      startDataNode(conf, dataDirs);
    } catch (IOException ie) {
      shutdown();
      throw ie;
    }
  }
</code></pre>

<h5>1.2.1 startDataNode()</h5>

<p>代码较长，仅列出主要步骤：</p>

<ol>
<li>设置配置信息</li>
<li>向NameNode发起RPC请求，获取版本和StorageID信息</li>
<li>获取启动配置</li>
<li>初始化存储信息，构建FSDataSet</li>
<li>获取可用的端口号</li>
<li>调整注册信息中的机器名，加上端口号</li>
<li>初始化DataXceiverServer</li>
<li>设置blockReport和heartbeat各自的时间间隔</li>
<li>初始化blockScanner</li>
<li>初始胡并启动servlet info server，提供内容查询的http服务</li>
<li>初始化ipc server，该ipc server主要用于完成DataNode间的block recover。</li>
</ol>


<h3>runDatanodeDaemon()</h3>

<pre><code>  public static void runDatanodeDaemon(DataNode dn) throws IOException {
    if (dn != null) {
      //register datanode
      dn.register();
      dn.dataNodeThread = new Thread(dn, dnThreadName);
      dn.dataNodeThread.setDaemon(true); // needed for JUnit testing
      dn.dataNodeThread.start();
    }
  }
</code></pre>

<h4>2.1 向NameNode注册 —— dn.register();</h4>

<pre><code>  private void register() throws IOException {
    if (dnRegistration.getStorageID().equals("")) {
      setNewStorageID(dnRegistration);
    }
    while(shouldRun) {
      try {
        // reset name to machineName. Mainly for web interface.
        dnRegistration.name = machineName + ":" + dnRegistration.getPort();
        // 通过NameProtocal向NameNode注册
        dnRegistration = namenode.register(dnRegistration);
        break;
      } catch(SocketTimeoutException e) {  // namenode is busy
        LOG.info("Problem connecting to server: " + getNameNodeAddr());
        try {
          Thread.sleep(1000);
        } catch (InterruptedException ie) {}
      }
    }
    assert ("".equals(storage.getStorageID()) 
            &amp;&amp; !"".equals(dnRegistration.getStorageID()))
            || storage.getStorageID().equals(dnRegistration.getStorageID()) :
            "New storageID can be assigned only if data-node is not formatted";
    if (storage.getStorageID().equals("")) {
      storage.setStorageID(dnRegistration.getStorageID());
      storage.writeAll();
      LOG.info("New storage id " + dnRegistration.getStorageID()
          + " is assigned to data-node " + dnRegistration.getName());
    }
    if(! storage.getStorageID().equals(dnRegistration.getStorageID())) {
      throw new IOException("Inconsistent storage IDs. Name-node returned "
          + dnRegistration.getStorageID() 
          + ". Expecting " + storage.getStorageID());
    }

    if (supportAppends) {
      Block[] bbwReport = data.getBlocksBeingWrittenReport();
      long[] blocksBeingWritten = BlockListAsLongs.convertToArrayLongs(bbwReport);
      //如果支持append，则报告正在写入的block信息
      namenode.blocksBeingWrittenReport(dnRegistration, blocksBeingWritten);
    }
    // 调整下一次的BR时间，使其在下次heartbeat时进行
    scheduleBlockReport(initialBlockReportDelay);
  }
</code></pre>

<h4>2.2 启动datanode线程 —— dn.dataNodeThread.start();</h4>

<p>datanode线程本身非常简单，不停调用offerSevice提供服务：</p>

<pre><code>  public void run() {
    LOG.info(dnRegistration + "In DataNode.run, data = " + data);

    // start dataXceiveServer
    dataXceiverServer.start();
    new Thread(new CrashVolumeChecker()).start();//added by wukong

    while (shouldRun) {
      try {
        startDistributedUpgradeIfNeeded();
        offerService();
      } catch (Exception ex) {
        LOG.error("Exception: " + StringUtils.stringifyException(ex));
        if (shouldRun) {
          try {
            Thread.sleep(5000);
          } catch (InterruptedException ie) {
          }
        }
      }
    }

    LOG.info(dnRegistration + ":Finishing DataNode in: "+data);
    shutdown();
  }
</code></pre>

<h5>2.2.1 offerService()</h5>

<p>offerService的核心是周期性进行heartbeat和blockReport，主要流程如下：</p>

<p><img src="/images/hdfs/offerService.png" alt="offerService" /></p>

<pre><code>  public void offerService() throws Exception {

    LOG.info("using BLOCKREPORT_INTERVAL of " + blockReportInterval + "msec" + 
       " Initial delay: " + initialBlockReportDelay + "msec");
    LOG.info("using DELETEREPORT_INTERVAL of " + deletedReportInterval + "msec");
    LOG.info("using HEARTBEAT_INTERVAL of " + heartBeatInterval + "msec");
    LOG.info("using HEARTBEAT_EXPIRE_INTERVAL of " + heartbeatExpireInterval + "msec");

    //
    // Now loop for a long time....
    //

    while (shouldRun) {
      try {
        long startTime = now();

        //
        // Every so often, send heartbeat or block-report
        //

        if (startTime - lastHeartbeat &gt; heartBeatInterval /* 3 secs*/) {
          //
          // All heartbeat messages include following info:
          // -- Datanode name
          // -- data transfer port
          // -- Total capacity
          // -- Bytes remaining
          //
          lastHeartbeat = startTime;
          DatanodeCommand[] cmds = namenode.sendHeartbeat(dnRegistration,
                                                       data.getCapacity(),
                                                       data.getDfsUsed(),
                                                       data.getRemaining(),
                                                       xmitsInProgress.get(),
                                                       getXceiverCount());
          myMetrics.heartbeats.inc(now() - startTime);
          //LOG.info("Just sent heartbeat, with name " + localName);
          if (!processCommand(cmds))
            continue;
        }

        reportReceivedBlocks();

        DatanodeCommand cmd = blockReport();
        processCommand(cmd);

        // start block scanner
        if (blockScanner != null &amp;&amp; blockScannerThread == null &amp;&amp;
            upgradeManager.isUpgradeCompleted()) {
          LOG.info("Starting Periodic block scanner.");
          blockScannerThread = new Daemon(blockScanner);
          blockScannerThread.start();
        }

        //
        // There is no work to do;  sleep until hearbeat timer elapses, 
        // or work arrives, and then iterate again.
        //
        long waitTime = heartBeatInterval - (System.currentTimeMillis() - lastHeartbeat);
        synchronized(receivedAndDeletedBlockList) {
          if (waitTime &gt; 0 &amp;&amp; receivedAndDeletedBlockList.size() == 0) {
            try {
              receivedAndDeletedBlockList.wait(waitTime);
            } catch (InterruptedException ie) {
            }
            delayBeforeBlockReceived();
          }
        } // synchronized

      } catch(RemoteException re) {
        String reClass = re.getClassName();
        if (UnregisteredDatanodeException.class.getName().equals(reClass) ||
            DisallowedDatanodeException.class.getName().equals(reClass) ||
            IncorrectVersionException.class.getName().equals(reClass)) {
          LOG.warn("DataNode is shutting down: " + 
                   StringUtils.stringifyException(re));
          shutdown();
          return;
        }
        LOG.warn(StringUtils.stringifyException(re));
      } catch (IOException e) {
        LOG.warn(StringUtils.stringifyException(e));
      }
    } // while (shouldRun)
  } // offerService
</code></pre>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/10/18/hdfs-datanode-structure/">HDFS源码学习（12）——DataNode主要数据结构</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-10-18T21:56:00+08:00" pubdate data-updated="true">Oct 18<span>th</span>, 2012</time>
        
         | <a href="/blog/2012/10/18/hdfs-datanode-structure/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>HDFS中DataNode主要负责维护block->stream bytes的映射关系，即实际block数据的存储。
一个datanode的磁盘上存储目录下实际的文件部署结构如下：</p>

<pre><code>data/
├── blocksBeingWritten
├── current
│   ├── VERSION
│   ├── blk_-1148021215131449924
│   ├── blk_-1148021215131449924_1001.meta
│   ├── blk_-8598609183581346893
│   ├── blk_-8598609183581346893_1002.meta
│   ├── blk_6693595845022390257
│   ├── blk_6693595845022390257_1003.meta
│   └── dncp_block_verification.log.curr
├── detach
├── storage
└── tmp
</code></pre>

<p>data目录的路径是hdfs-site.xml中配置的dfs.data.dir的路径，表示每个datanode上数据存储的目录</p>

<p>1) blocksBeingWritten：当前正在写入的block，写完之后会将block移至current目录</p>

<p>2) current：当前已经写入的block文件目录</p>

<p>2.1) VERSION为存储的VERSION文件，包括namespaceId，存储Id，存储版本，存储类型，创建时间戳等信息</p>

<p>2.2）blk-*:文件为实际的block数据文件</p>

<p>2.3）blk-*_xxx.meta: block的元信息文件</p>

<p>2.4）dncp_*.log.curr: 当前copy文件</p>

<p>3) detach：copy-on-write使用的目录</p>

<p>4) tmp： 临时目录，DataNode启动时会检查 tmp的数据并删除。</p>

<h2>Storage相关</h2>

<p>Storage用于描述存储的类型，状态，目录等信息。
其主要结构如下：
<img src="/images/hdfs/Storage.png" alt="Storage" /></p>

<h3>StorageInfo</h3>

<p>StorageInfo表示一个存储的通用信息，包括：</p>

<ol>
<li>layoutVersion： 存储文件中的版本号</li>
<li>namespaceId： 存储所属的命名空间ID</li>
<li>ctime： 该存储创建的时间戳</li>
</ol>


<h3>Storage</h3>

<p>存储信息的抽象类，管理一个server（NameNode或DataNode）上的存储目录。</p>

<p>Storage有两个关键属性：</p>

<ol>
<li>storageType: 表示该存储所属的节点类型（NameNode或是DataNode）</li>
<li>storageDirs: 该存储上存储目录的列表(ArrayList<StorageDirectory>),StorageDirectory表示一个存储目录。</li>
</ol>


<h4>StorageDirectory</h4>

<p>表示一个存储目录，有三个属性：</p>

<ol>
<li>root：根目录</li>
<li>lock：当前目录的文件锁</li>
<li>dirType：目录类型</li>
</ol>


<h4>StorageSate</h4>

<p>表示存储的状态：</p>

<ol>
<li>NON_EXISTENT: 目录不存在</li>
<li>NOT_FORMATTED: 目录未格式化</li>
<li>COMPLETE_UPGRADE: 升级完成</li>
<li>RECOVER_UPGRADE: 撤销升级</li>
<li>COMPLETE_FINALIZE: 提交完成</li>
<li>COMPLETE_ROLLBACK: 回滚完成</li>
<li>RECOVER_ROLLBACK: 撤销回滚</li>
<li>COMPLETE_CHECKPOINT: checkpoint完成</li>
<li>RECOVER_CHECKPOINT: 撤销checkpoint</li>
<li>NORMAL: 正常</li>
</ol>


<h3>DataStorage</h3>

<p>DataStorage是DataNode上使用的存储类，指定了datanode上各类存储文件的前缀：</p>

<ol>
<li>subdir：子目录前缀</li>
<li>blk_：块文件前缀</li>
<li>dncp_：拷贝文件前缀</li>
</ol>


<h2>DatanodeBlockInfo</h2>

<p>DataNode使用DatanodeBlockInfo管理block和其元数据之间的映射关系，结构如下：</p>

<p><img src="/images/hdfs/DatanodeBlockInfo.png" alt="DatanodeBlockInfo" /></p>

<ol>
<li>volmun：block所属的卷</li>
<li>file：block文件</li>
<li>detached：是否完成copy-on-write</li>
</ol>


<h2>FSDataSet相关</h2>

<p>DataNode通过FSDataSet来完成数据的存储。FSDataset类结构如下：
<img src="/images/hdfs/FSDataset.png" alt="FSDataset" /></p>

<h3>FSVolume</h3>

<p>FSVolumne用于进行block文件所属的卷管理，统计存储目录额使用情况，其中：</p>

<ol>
<li>currentDir： 当前数据目录, 对应data/current目录</li>
<li>dataDir： 数据目录</li>
<li>tmpDir： 临时目录, 对应data/tmp目录</li>
<li>dtacheDir: 用于实现写时复制的文件，对应data/detach目录</li>
<li>usage: 目录使用的空间</li>
<li>dfsusage: dfs使用的空间</li>
<li>reseved: 空余空间</li>
<li>blocksBeingWritten: 正在写入的block，对应data/blocksBeingWritten目录</li>
</ol>


<h3>FSVolumeSet</h3>

<p>FSVolumeSet是FSVolume的集合，提供了所有容量，剩余空间等方法。其中getNextVolume中提供了round-robin策略选取下一个volume，从而实现简单的IO负载均衡，提高IO处理能力。</p>

<h3>FSDataSet</h3>

<p>FSDataSet是在FSVolumeSet之上进行封装实现FSDatasetInterface借口，向外提供块查询和操作方法。</p>

<p>其中有几个主要属性:</p>

<ol>
<li>volumes: 卷集合（FSVolumeSet）</li>
<li>ongoingCreates: 当前活动的文件</li>
<li>maxBlocksPerDir: 每个目录下最多能存放发block数，可通过dfs.datanode.numblocks配置</li>
<li>volumeMap：块与块文件的映射信息(HashMap&lt;Block, DatanodeBlockInfo>)，当前集合中所有的块信息均维护在该map中</li>
</ol>


<h3>FSDir</h3>

<p>用于构建block块在datanode磁盘上的层次结构，默认情况下每个目录下最多64个子目录，最多能存储64个块。目录初始化时会递归扫描目录下的所有子目录和文件，构建一个树形结构。</p>

<p>addBlock时，首先尝试在当前目录新加块，如果当前目录没有空闲空间，则尝试在子目录中添加，如果没有子目录，则新建一个子目录。</p>

<h3>BlockAndFile</h3>

<p>Block与其文件名的封装</p>

<h3>ActiveFile</h3>

<p>表示一个当前活动中的文件</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/10/18/hdfs-secondary-namenode/">HDFS源码学习（11）——SecondaryNameNode</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-10-18T21:55:00+08:00" pubdate data-updated="true">Oct 18<span>th</span>, 2012</time>
        
         | <a href="/blog/2012/10/18/hdfs-secondary-namenode/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h2>概述</h2>

<p>SecondaryNameNode在HDFS中的主要作用是帮助master NameNode周期性执行checkpoint操作。</p>

<p>NameNode将运行过程中对文件的修改记录保存在EditLog中。当NameNode重新启动时会从FSImage中加载命名空间镜像，并将EditLog中的内容合并到FSImage中，将合并后的FSImage写入到磁盘，同时清空EditLog，共后续使用。但如果NameNode长时间不重启，随时间增长，EditLog将会越来越大（每次文件操作都要记录），大量占用NameNode磁盘空间，且会导致下一次重启花费大量时间在合并Editlog上。</p>

<p>为了解决这个问题，SecondaryNameNode会定期从NameNode下载最新的FSImage和EditLog，合并editLog日志到FSImage，将合并后的FSImage上传到NameNode，并清空NameNode上的Editlog，将EditLog日志大小控制在一定限度下。</p>

<h2>代码解析</h2>

<p>SecondaryNameNode代码结构如下：</p>

<p><img src="/images/hdfs/SecondaryNameNode.png" alt="SecondaryNameNode" /></p>

<p>SecondaryNameNode本身就是实现了Runnable接口，即一个可执行线程。</p>

<p>checkpointImage表示当前SecondNameNode上的FsImage镜像，该类CheckpointStorage扩展自FSImage。</p>

<p>其中有两个主要的可配置属性：</p>

<ol>
<li>checkpointPeriod: 两次检查点的间隔时间，可通过fs.checkpoint.period配置</li>
<li>checkpointSize: EditLog文件的最大值，当EditLog超过这个最大值时会强制之行checkpoint，可通过fs.checkpoint.size配置，默认是64M</li>
</ol>


<h3>run()</h3>

<p>run方法代码如下：</p>

<pre><code>  public void run() {

    //
    // Poll the Namenode (once every 5 minutes) to find the size of the
    // pending edit log.
    //
    long period = 5 * 60;              // 5 minutes
    long lastCheckpointTime = 0;
    if (checkpointPeriod &lt; period) {
      period = checkpointPeriod;
    }

    while (shouldRun) {
      try {
        Thread.sleep(1000 * period);
      } catch (InterruptedException ie) {
        // do nothing
      }
      if (!shouldRun) {
        break;
      }
      try {
        long now = System.currentTimeMillis();

        long size = namenode.getEditLogSize();
        if (size &gt;= checkpointSize || 
            now &gt;= lastCheckpointTime + 1000 * checkpointPeriod) {
          doCheckpoint();
          lastCheckpointTime = now;
        }
      } catch (IOException e) {
        LOG.error("Exception in doCheckpoint: ");
        LOG.error(StringUtils.stringifyException(e));
        e.printStackTrace();
      } catch (Throwable e) {
        LOG.error("Throwable Exception in doCheckpoint: ");
        LOG.error(StringUtils.stringifyException(e));
        e.printStackTrace();
        Runtime.getRuntime().exit(-1);
      }
    }
  }
</code></pre>

<p>其核心就是周期性（默认每个5分钟)调用doCheckpoint().</p>

<h3>doCheckpoint()</h3>

<pre><code>  void doCheckpoint() throws IOException {

    // 准备合并所需的空间
    startCheckpoint();

    // 通知NameNode将修改信息记录到新的editlog中，并获取一个用于上传合并后的fsimage的token
    CheckpointSignature sig = (CheckpointSignature)namenode.rollEditLog();

    // error simulation code for junit test
    if (ErrorSimulator.getErrorSimulation(0)) {
      throw new IOException("Simulating error0 " +
                            "after creating edits.new");
    }
    //从NameNode获取fsimage和editslog
    downloadCheckpointFiles(sig);   // Fetch fsimage and edits
    //合并editlog到fsimage
    doMerge(sig);                   // Do the merge

    //上传合并后的fsimage到NameNode
    putFSImage(sig);

    // error simulation code for junit test
    if (ErrorSimulator.getErrorSimulation(1)) {
      throw new IOException("Simulating error1 " +
                            "after uploading new image to NameNode");
    }
    // 通知NameNode使用该fsimage作为最新的镜像
    namenode.rollFsImage();
    checkpointImage.endCheckpoint();

    LOG.warn("Checkpoint done. New Image Size: " 
              + checkpointImage.getFsImageName().length());
  }
</code></pre>

<h4>1. startCheckpoint()</h4>

<p>该方法主要用于准备合并所需的磁盘空间，代码如下：</p>

<pre><code>  private void startCheckpoint() throws IOException {
    checkpointImage.unlockAll();
    // 关闭当前Editlog
    checkpointImage.getEditLog().close();
    // 检查当前checkpoints目录，如果不存在则创建一个新目录，如果目录中存在异常，则尝试恢复该目录
    checkpointImage.recoverCreate(checkpointDirs, checkpointEditsDirs);
    // 为新的checkpoint准备目录空间，将当前的目录空间更名为lastcheckpoint.™p，新建一个current目录 
    checkpointImage.startCheckpoint();
  }
</code></pre>

<h4>2. namenode.rollEditLog();</h4>

<p>namenode.rollEditLog()实际通过NameNodeProtocol调用NameNode.rollEditLog()方法，并最终调用FSImage.rollEditLog()，该方法主要完成：</p>

<ol>
<li>调用FSEditLog.rollEditLog()关闭当前editLog，新建一个editLog：edits.new</li>
<li>返回一个CheckpointSignature做为上传合并后镜像的token</li>
</ol>


<h5>2.1. FSEditLog.rollEditLog()</h5>

<p>该方法主要完成：</p>

<ol>
<li>关闭当前editlog， 打开一个新的editlog： edit.new</li>
<li>返回editlog的最新更新时间</li>
</ol>


<p>代码结构如下</p>

<pre><code>  synchronized void rollEditLog() throws IOException {
    //检查edit.new是否已经存在，如果存在，检查是否所有目录都存在，如果是则认为edits.new已经建好了，直接返回
    //
    if (existsNew()) {
      for (Iterator&lt;StorageDirectory&gt; it = 
               fsimage.dirIterator(NameNodeDirType.EDITS); it.hasNext();) {
        File editsNew = getEditNewFile(it.next());
     if (!editsNew.exists()) { 
          throw new IOException("Inconsistent existance of edits.new " +
                                editsNew);
        }
      }
      return; // nothing to do, edits.new exists!
    }

    //关闭当前的editLog
    close();                     // close existing edit log

    //
    // 新建一个editLog： edits.new
    //
    for (Iterator&lt;StorageDirectory&gt; it = 
           fsimage.dirIterator(NameNodeDirType.EDITS); it.hasNext();) {
      StorageDirectory sd = it.next();
      try {
        EditLogFileOutputStream eStream = 
             new EditLogFileOutputStream(getEditNewFile(sd));
        eStream.create();
        editStreams.add(eStream);
      } catch (IOException e) {
        // remove stream and this storage directory from list
        processIOError(sd);
       it.remove();
      }
    }
  }
</code></pre>

<h4>3. downloadCheckpointFiles()</h4>

<p>该方法用于从NameNode下载FSImage和FSEditLog，代码结构如下：</p>

<pre><code>  private void downloadCheckpointFiles(CheckpointSignature sig
                                      ) throws IOException {

    checkpointImage.cTime = sig.cTime;
    checkpointImage.checkpointTime = sig.checkpointTime;

    // 获取fsimage
    String fileid = "getimage=1";
    File[] srcNames = checkpointImage.getImageFiles();
    assert srcNames.length &gt; 0 : "No checkpoint targets.";
    TransferFsImage.getFileClient(fsName, fileid, srcNames);
    LOG.info("Downloaded file " + srcNames[0].getName() + " size " +
             srcNames[0].length() + " bytes.");

    // 获取editlog
    fileid = "getedit=1";
    srcNames = checkpointImage.getEditsFiles();
    assert srcNames.length &gt; 0 : "No checkpoint targets.";
    TransferFsImage.getFileClient(fsName, fileid, srcNames);
    LOG.info("Downloaded file " + srcNames[0].getName() + " size " +
        srcNames[0].length() + " bytes.");

    // 标示checkpoint所需文件已经准备完成
    checkpointImage.checkpointUploadDone();
  }
</code></pre>

<h4>4. doMerge()</h4>

<p>doMerge主要完成editLog与fsimage的合并，实际调用的checkpointImage.doMerge(sig);</p>

<pre><code>private void doMerge(CheckpointSignature sig) throws IOException {
  getEditLog().open();
  StorageDirectory sdName = null;
  StorageDirectory sdEdits = null;
  Iterator&lt;StorageDirectory&gt; it = null;
  it = dirIterator(NameNodeDirType.IMAGE);
  if (it.hasNext())
    sdName = it.next();
  it = dirIterator(NameNodeDirType.EDITS);
  if (it.hasNext())
    sdEdits = it.next();
  if ((sdName == null) || (sdEdits == null))
    throw new IOException("Could not locate checkpoint directories");
  // 加载fsimage
  loadFSImage(FSImage.getImageFile(sdName, NameNodeFile.IMAGE));
  // 加载editlog，并合并到fsimage

  loadFSEdits(sdEdits);
  // 校验新fsimage的一致性，主要包括版本，更新时间，namespaceId
  sig.validateStorageInfo(this);
  // 将fsImage存储到本地，并创建新edits
  saveFSImage();
}
</code></pre>

<p>该方法与NameNode启动时类似:</p>

<ol>
<li>加载fsiamge</li>
<li>加载editslog，并作用到fsimage中</li>
<li>校验合并后的fsimage</li>
<li>将新fsimage存储到本地，并新建空的edit是目录</li>
</ol>


<h4>5. putFSImage(sig);</h4>

<p>该方法比较简单，主要通过TransferFsImage工具类将合并后的fsimage上传到NameNode</p>

<pre><code>  private void putFSImage(CheckpointSignature sig) throws IOException {
    String fileid = "putimage=1&amp;port=" + infoPort +
      "&amp;machine=" +
      InetAddress.getLocalHost().getHostAddress() +
      "&amp;token=" + sig.toString();
    LOG.info("Posted URL " + fsName + fileid);
    TransferFsImage.getFileClient(fsName, fileid, (File[])null);
  }
</code></pre>

<h4>6. namenode.rollFsImage();</h4>

<p>上传完新fsiamge之后，SecondaryNameNode通过namenode.rollFsImage()通知NameNode使用新的fsimage.ckpt作为最新镜像，并清空editslog。该请求最终由NameNode上的FsImage.rollFsImage()处理，代码如下：</p>

<pre><code> void rollFSImage() throws IOException {
    if (ckptState != CheckpointStates.UPLOAD_DONE) {
      throw new IOException("Cannot roll fsImage before rolling edits log.");
    }
    //
    // 校验fsimage.ckpt和edits.new是否存在于所有目录
    if (!editLog.existsNew()) {
      throw new IOException("New Edits file does not exist");
    }
    for (Iterator&lt;StorageDirectory&gt; it = 
                       dirIterator(NameNodeDirType.IMAGE); it.hasNext();) {
      StorageDirectory sd = it.next();
      File ckpt = getImageFile(sd, NameNodeFile.IMAGE_NEW);
      if (!ckpt.exists()) {
        throw new IOException("Checkpoint file " + ckpt +
                              " does not exist");
      }
    }
    //删除旧的edits，并将edits.new重命名为edits
    editLog.purgeEditLog(); // renamed edits.new to edits

    //
    // 重命名fsimage
    for (Iterator&lt;StorageDirectory&gt; it = 
                       dirIterator(NameNodeDirType.IMAGE); it.hasNext();) {
      StorageDirectory sd = it.next();
      File ckpt = getImageFile(sd, NameNodeFile.IMAGE_NEW);
      File curFile = getImageFile(sd, NameNodeFile.IMAGE);
      // renameTo fails on Windows if the destination file 
      // already exists.
      if (!ckpt.renameTo(curFile)) {
        curFile.delete();
        if (!ckpt.renameTo(curFile)) {
          // Close edit stream, if this directory is also used for edits
          if (sd.getStorageDirType().isOfType(NameNodeDirType.EDITS))
            editLog.processIOError(sd);
        // add storage to the removed list
          removedStorageDirs.add(sd);
          it.remove();
        }
      }
    }

    //
    // 更新所有目录的fstime
    //
    this.layoutVersion = FSConstants.LAYOUT_VERSION;
    this.checkpointTime = FSNamesystem.now();
    for (Iterator&lt;StorageDirectory&gt; it = 
                           dirIterator(); it.hasNext();) {
      StorageDirectory sd = it.next();
      // delete old edits if sd is the image only the directory
      if (!sd.getStorageDirType().isOfType(NameNodeDirType.EDITS)) {
        File editsFile = getImageFile(sd, NameNodeFile.EDITS);
        editsFile.delete();
      }
      // delete old fsimage if sd is the edits only the directory
      if (!sd.getStorageDirType().isOfType(NameNodeDirType.IMAGE)) {
        File imageFile = getImageFile(sd, NameNodeFile.IMAGE);
        imageFile.delete();
      }
      try {
        sd.write();
      } catch (IOException e) {
        LOG.error("Cannot write file " + sd.getRoot(), e);
        // Close edit stream, if this directory is also used for edits
        if (sd.getStorageDirType().isOfType(NameNodeDirType.EDITS))
          editLog.processIOError(sd);
      //add storage to the removed list
        removedStorageDirs.add(sd);
        it.remove();
      }
    }
    ckptState = FSImage.CheckpointStates.START;
  }
</code></pre>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/10/18/hdfs-namenode-and-datanode-communication/">HDFS源码学习（10）——NameNode与DataNode间的通信</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-10-18T21:54:00+08:00" pubdate data-updated="true">Oct 18<span>th</span>, 2012</time>
        
         | <a href="/blog/2012/10/18/hdfs-namenode-and-datanode-communication/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>NameNode和DataNode间的通信分为四种场景：</p>

<ol>
<li>初始时DataNode注册：</li>
<li>周期性心跳检测：</li>
<li>周期性blockreport：</li>
<li>完成一个副本的写入：</li>
</ol>


<h2>一、初始时DataNode注册</h2>

<p>DataNode在启动时会向NameNode注册，注册时需要提交的信息有DatanodeRegistration表示。结构如下：</p>

<p><img src="/images/hdfs/DatanodeRegistration.png" alt="DatanodeRegistration" /></p>

<p>主要包括：</p>

<ol>
<li>name：机器名（主机名+服务端口号）</li>
<li>infoPort: 状态信息服务端口好</li>
<li>ipcPort： 提供ipc服务的端口号</li>
</ol>


<p>此外，该类中的storageID是该datanode在集群中的唯一id，在注册时有NameNode分配</p>

<p>注册的主要流程如下：
<img src="/images/hdfs/register.png" alt="DataNodeRegister" /></p>

<h2>二、心跳检测（heartbeat）</h2>

<p>DataNode通过周期性调用namenode.sendHeartbeat()来完成心跳检测.主要流程如下：</p>

<p><img src="/images/hdfs/sendHeartbeat.png" alt="sendHeartbeat" /></p>

<h2>三、blockReport</h2>

<p>DataNode周期性向NameNode发送blockReport，告知自己最新的block信息：
<img src="/images/hdfs/blockReport.png" alt="blockReport" /></p>

<h2>四、完成副本写入</h2>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/10/18/hdfs-namenode-safe-mode/">HDFS源码学习（9）——安全模式（SafeMode）</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-10-18T21:52:00+08:00" pubdate data-updated="true">Oct 18<span>th</span>, 2012</time>
        
         | <a href="/blog/2012/10/18/hdfs-namenode-safe-mode/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h2>一、SafeModeInfo</h2>

<p>SafeModeInfo维护了系统安全模式下的状态信息，每当系统进入安全模式时都会创建一个SafeModeInfo实例维护状态信息，离开时会销毁这个实例。
该类结构如下</p>

<p><img src="/images/hdfs/SafeModeInfo.png" alt="SafeModeInfo" /></p>

<p>其中threshold和extension为可配置项：</p>

<ol>
<li>threshold表示离开安全模式时打到最低备份数的block的比例</li>
<li>extension表示进入安全模式的最低时长</li>
</ol>


<h2>二、SafeModeMonitor</h2>

<p>FSNameSystem中SafeModeMonitor代码结构如下：</p>

<pre><code>  class SafeModeMonitor implements Runnable {
    /** interval in msec for checking safe mode: {@value} */
    private static final long recheckInterval = 1000;

    /**
     */
    public void run() {
      while (fsRunning &amp;&amp; (safeMode != null &amp;&amp; !safeMode.canLeave())) {
        try {
          Thread.sleep(recheckInterval);
        } catch (InterruptedException ie) {
        }
      }
      // leave safe mode and stop the monitor
      try {
        leaveSafeMode(true);
      } catch(SafeModeException es) { // should never happen
        String msg = "SafeModeMonitor may not run during distributed upgrade.";
        assert false : msg;
        throw new RuntimeException(msg, es);
      }
      smmthread = null;
    }
  }
</code></pre>

<p>其核心就是每个1秒检测一次是否能够离开模式（safeMode.canLeave()），如果可以，则尝试离开并停止SafeModeMonitor线程（leaveSafeMode(true)）</p>

<h3>1.1. 是否能离开 —— safeMode.canLeave()</h3>

<p>能够离开安全模式的标准是：
1. 已进入安全模式的时长大于等于 extension
2. 安全的block数比例打到门槛值</p>

<pre><code>synchronized boolean canLeave() {
  if (reached == 0)
    return false;
  if (now() - reached &lt; extension) {
    reportStatus("STATE* Safe mode ON.", false);
    return false;
  }
  return !needEnter();
}

/** 
 * There is no need to enter safe mode 
 * if DFS is empty or {@link #threshold} == 0
 */
boolean needEnter() {
  return getSafeBlockRatio() &lt; threshold;
}
</code></pre>

<h3>1.2. 离开安全模式 —— leaveSafeMode(true);</h3>

<pre><code>  public void leaveSafeMode(boolean checkForUpgrades) throws SafeModeException {
    writeLock();
    try {
    if (!isInSafeMode()) {
      NameNode.stateChangeLog.info("STATE* Safe mode is already OFF."); 
      return;
    }
    //获取升级状态，如在升级中，不能离开安全模式
    if(getDistributedUpgradeState())
      throw new SafeModeException("Distributed upgrade is in progress",
                                  safeMode);
    //调用SafeModeInfo.leave()离开安全模式
    safeMode.leave(checkForUpgrades);
    } finally {
      writeUnlock();
    }
  }
</code></pre>

<h3>1.2.1 SafeModeInfo.leave()</h3>

<pre><code>    synchronized void leave(boolean checkForUpgrades) {
      if(checkForUpgrades) {
        // 验证是否需要升级
        boolean needUpgrade = false;
        try {
          needUpgrade = startDistributedUpgradeIfNeeded();
        } catch(IOException e) {
          FSNamesystem.LOG.error(StringUtils.stringifyException(e));
        }
        if(needUpgrade) {
          //如果需要升级，进入手动安全模式
          safeMode = new SafeModeInfo();
          return;
        }
      }
      // 如果备份队列未初始化完，继续初始化该队列
      if (!isPopulatingReplQueues()) {
        initializeReplQueues();
      }
      long timeInSafemode = now() - systemStart;
      NameNode.stateChangeLog.info("STATE* Leaving safe mode after " 
                                    + timeInSafemode/1000 + " secs.");
      NameNode.getNameNodeMetrics().safeModeTime.set((int) timeInSafemode);

      if (reached &gt;= 0) {
        NameNode.stateChangeLog.info("STATE* Safe mode is OFF."); 
      }
      reached = -1;
      safeMode = null;
      NameNode.stateChangeLog.info("STATE* Network topology has "
                                   +clusterMap.getNumOfRacks()+" racks and "
                                   +clusterMap.getNumOfLeaves()+ " datanodes");
      NameNode.stateChangeLog.info("STATE* UnderReplicatedBlocks has "
                                   +neededReplications.size()+" blocks");
    }
</code></pre>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/blog/page/3/">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2012/12/21/hdfs-raid/">HDFS RAID</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/12/10/hdfs-blockreaderlocal/">HDFS-2246:使用BlockReaderLocal优化本地block读取</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/10/23/mount-hdfs-with-fuse-dfs/">使用FUSE-DFS mount HDFS</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/10/18/hdfs-dataxceiver/">HDFS源码学习（15）——DataXceiverServer</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/10/18/hdfs-client-code/">HDFS源码学习（14）——Client代码结构</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/10/18/hdfs-datanode-startup/">HDFS源码学习（13）——DataNode启动过程</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/10/18/hdfs-datanode-structure/">HDFS源码学习（12）——DataNode主要数据结构</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/10/18/hdfs-secondary-namenode/">HDFS源码学习（11）——SecondaryNameNode</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/10/18/hdfs-namenode-and-datanode-communication/">HDFS源码学习（10）——NameNode与DataNode间的通信</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/10/18/hdfs-namenode-safe-mode/">HDFS源码学习（9）——安全模式（SafeMode）</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/jiang-bo">@jiang-bo</a> on GitHub
  
  <script type="text/javascript">
    $.domReady(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'jiang-bo',
            count: 0,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>





  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2012 - jiang-bo -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'jiangbo';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>











</body>
</html>
