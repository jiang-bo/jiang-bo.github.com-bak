
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>非纯种程序猿</title>
  <meta name="author" content="jiang-bo">

  
  <meta name="description" content="HDFS中的副本管理通过FSNameSystem.java中的ReplicationMonitor线程来完成。该线程代码结构较为简单。 class ReplicationMonitor implements Runnable { static final int &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://jiangbo.me/blog/page/3">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/ender.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="非纯种程序猿" type="application/atom+xml">
  <!-- font-family: 'Knewave', cursive; -->
<link href='http://fonts.googleapis.com/css?family=Knewave' rel='stylesheet' type='text/css'>
<!-- font-family: 'Cantata One', serif; -->
<link href='http://fonts.googleapis.com/css?family=Cantata+One' rel='stylesheet' type='text/css'>

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-34477986-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1>
      <a href="/">非纯种程序猿</a>
      
  </h1>
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:jiangbo.me" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Home</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/about-me">About Me</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/10/18/hdfs-namenode-replica-management/">HDFS源码学习（5）——副本管理（Replica Management)</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-10-18T21:43:00+08:00" pubdate data-updated="true">Oct 18<span>th</span>, 2012</time>
        
         | <a href="/blog/2012/10/18/hdfs-namenode-replica-management/#disqus_thread">Comments</a>
        
        
          | 

<span class="categories">
  
    <a class='category' href='/blog/categories/hdfs/'>HDFS</a>
  
</span>


        
      </p>
    
  </header>


  <div class="entry-content"><p>HDFS中的副本管理通过FSNameSystem.java中的ReplicationMonitor线程来完成。该线程代码结构较为简单。</p>

<pre><code>  class ReplicationMonitor implements Runnable {
    static final int INVALIDATE_WORK_PCT_PER_ITERATION = 32;
    static final float REPLICATION_WORK_MULTIPLIER_PER_ITERATION = 2;
    public void run() {
      while (fsRunning) {
        try {
          computeDatanodeWork();
          processPendingReplications();
          Thread.sleep(replicationRecheckInterval);
        } catch (InterruptedException ie) {
          LOG.warn("ReplicationMonitor thread received InterruptedException.", ie);
          break;
        } catch (IOException ie) {
          LOG.warn("ReplicationMonitor thread received exception. " + ie +  " " +
              StringUtils.stringifyException(ie));
        } catch (Throwable t) {
          LOG.fatal("ReplicationMonitor thread received Runtime exception. " + t + " " +
              StringUtils.stringifyException(t));
          Runtime.getRuntime().exit(-1);
        }
      }
    }
  }
</code></pre>

<p>该线程只是周期性调用computeDatanodeWork()和processPendingReplications()。</p>

<h2>一、computeDatanodeWork()</h2>

<pre><code>  public int computeDatanodeWork() throws IOException {
    int workFound = 0;
    int blocksToProcess = 0;
    int nodesToProcess = 0;
    // blocks should not be replicated or removed if safe mode is on
    if (isInSafeMode())
      return workFound;
    //计算需要备份的block数和节点数
    synchronized(heartbeats) {
      blocksToProcess = (int)(heartbeats.size() 
          * ReplicationMonitor.REPLICATION_WORK_MULTIPLIER_PER_ITERATION);
      nodesToProcess = (int)Math.ceil((double)heartbeats.size() 
          * ReplicationMonitor.INVALIDATE_WORK_PCT_PER_ITERATION / 100);
    }
    //执行备份
    workFound = computeReplicationWork(blocksToProcess); 

    // Update FSNamesystemMetrics counters
    pendingReplicationBlocksCount = pendingReplications.size();
    underReplicatedBlocksCount = neededReplications.size();
    scheduledReplicationBlocksCount = workFound;
    corruptReplicaBlocksCount = corruptReplicas.size();

    //HADOOP-5549 : Fix bug of schedule both replication and deletion work in one iteration
    workFound += computeInvalidateWork(nodesToProcess);
    return workFound;
  }
</code></pre>

<h3>1.1. computeReplicationWork()</h3>

<pre><code>  private int computeReplicationWork(
                                  int blocksToProcess) throws IOException {
    // stall only useful for unit tests (see TestFileAppend4.java)
    if (stallReplicationWork)  {
      return 0;
    }

    // 选取需要备份的block
    List&lt;List&lt;Block&gt;&gt; blocksToReplicate =
      chooseUnderReplicatedBlocks(blocksToProcess);

    // 执行备份
    return computeReplicationWorkForBlocks(blocksToReplicate);
  }
</code></pre>

<h3>1.1.1 选取需要备份的block —— chooseUnderReplicatedBlocks()</h3>

<pre><code>  List&lt;List&lt;Block&gt;&gt; chooseUnderReplicatedBlocks(int blocksToProcess) {
    // 初始化返回值数据结构，返回值是一个二维优先级列表
    List&lt;List&lt;Block&gt;&gt; blocksToReplicate =
      new ArrayList&lt;List&lt;Block&gt;&gt;(UnderReplicatedBlocks.LEVEL);
    for (int i = 0; i &lt; UnderReplicatedBlocks.LEVEL; i++) {
      blocksToReplicate.add(new ArrayList&lt;Block&gt;());
    }

    writeLock();
    try {
      synchronized (neededReplications) {
        if (neededReplications.size() == 0) {
          return blocksToReplicate;
        }

        for (int priority = 0; priority&lt;UnderReplicatedBlocks.LEVEL; priority++) {
        //遍历所有需要备份的block列表（UnderReplicatedBlocks结构）
        BlockIterator neededReplicationsIterator = neededReplications.iterator(priority);
        int numBlocks = neededReplications.size(priority);
        //检查该优先级列表中是否已经开始备份（relIndex数组中保存的是当前每个优先级列表中已备份的block索引）
        if (replIndex[priority] &gt; numBlocks) {
          replIndex[priority] = 0;
        }
        // skip to the first unprocessed block, which is at replIndex
        for (int i = 0; i &lt; replIndex[priority] &amp;&amp; neededReplicationsIterator.hasNext(); i++) {
          neededReplicationsIterator.next();
        }
        // 计算该优先级下需要备份的block数，低优先级的block备份数不超过总配额的20%
        int blocksToProcessIter = getQuotaForThisPriority(blocksToProcess,
            numBlocks, neededReplications.getSize(priority+1));
        blocksToProcess -= blocksToProcessIter;

        //便利改优先级列表将该优先级下的block添加到返回值中
        for (int blkCnt = 0; blkCnt &lt; blocksToProcessIter; blkCnt++, replIndex[priority]++) {
          if (!neededReplicationsIterator.hasNext()) {
            // start from the beginning
            replIndex[priority] = 0;
            neededReplicationsIterator = neededReplications.iterator(priority);
            assert neededReplicationsIterator.hasNext() :
              "neededReplications should not be empty.";
          }

          Block block = neededReplicationsIterator.next();
          blocksToReplicate.get(priority).add(block);
        } // end for
        }
      } // end try
      return blocksToReplicate;
    } finally {
      writeUnlock();
    }
  }
</code></pre>

<h3>1.1.2. 备份blocks —— computeReplicationWorkForBlocks（）</h3>

<pre><code>  int computeReplicationWorkForBlocks(List&lt;List&lt;Block&gt;&gt; blocksToReplicate) {
    int requiredReplication, numEffectiveReplicas, priority;
    List&lt;DatanodeDescriptor&gt; containingNodes;
    DatanodeDescriptor srcNode;
    INodeFile fileINode = null;

    int scheduledWork = 0;
    List&lt;ReplicationWork&gt; work = new LinkedList&lt;ReplicationWork&gt;();

    writeLock();
    try {
      synchronized (neededReplications) {
        for (priority = 0; priority &lt; blocksToReplicate.size(); priority++) {
          for (Block block : blocksToReplicate.get(priority)) {
            // block should belong to a file
            //获取该block所属的INode
            fileINode = blocksMap.getINode(block);
            // abandoned block not belong to a file
            if (fileINode == null ) {
              neededReplications.remove(block, priority); // remove from neededReplications
              replIndex[priority]--;
              continue;
            }
            //获取该文件需要的副本数
            requiredReplication = fileINode.getReplication();

            // 获取一个源datanode节点
            containingNodes = new ArrayList&lt;DatanodeDescriptor&gt;();
            NumberReplicas numReplicas = new NumberReplicas();
            srcNode = chooseSourceDatanode(block, containingNodes, numReplicas);
            if (srcNode == null) // block can not be replicated from any node
            {
              continue;
            }

          // 检查正在备份中的副本数是否满足备份需要，满足则不需要再备份
            numEffectiveReplicas = numReplicas.liveReplicas() +
              pendingReplications.getNumReplicas(block);
            if (numEffectiveReplicas &gt;= requiredReplication) {
              neededReplications.remove(block, priority); // remove from neededReplications
              replIndex[priority]--;
              continue;
            }
            //添加到待备份列表中
            work.add(new ReplicationWork(block, fileINode, requiredReplication
                - numEffectiveReplicas, srcNode, containingNodes, priority));
          }
        }
      }
    } finally {
      writeUnlock();
    }

    // 选取一个备份目标datanode
    for(ReplicationWork rw : work){
      DatanodeDescriptor targets[] = chooseTarget(rw);
      rw.targets = targets;
    }

    writeLock();
    try {
      for(ReplicationWork rw : work){
        DatanodeDescriptor[] targets = rw.targets;
        if(targets == null || targets.length == 0){
          rw.targets = null;
          continue;
        }
        synchronized (neededReplications) {
          Block block = rw.block;
          priority = rw.priority;
          // 重新检查INode和备份数，因为全局锁已经释放
          // block should belong to a file
          fileINode = blocksMap.getINode(block);
          // abandoned block not belong to a file
          if (fileINode == null ) {
            neededReplications.remove(block, priority); // remove from neededReplications
            rw.targets = null;
            replIndex[priority]--;
            continue;
          }
          requiredReplication = fileINode.getReplication();


          NumberReplicas numReplicas = countNodes(block);
          numEffectiveReplicas = numReplicas.liveReplicas() +
            pendingReplications.getNumReplicas(block);
          if (numEffectiveReplicas &gt;= requiredReplication) {
            neededReplications.remove(block, priority); // remove from neededReplications
            replIndex[priority]--;
            rw.targets = null;
            continue;
          }

          // 将block添加到datanode的需要备份的block列表中
          rw.srcNode.addBlockToBeReplicated(block, targets);

          scheduledWork++;

          //设置namenode的block调度计数器
          for (DatanodeDescriptor dn : targets) {
            dn.incBlocksScheduled();
          }

          // Move the block-replication into a "pending" state.
          // The reason we use 'pending' is so we can retry
          // replications that fail after an appropriate amount of time.
          //将该block移至pendingReplications（PendingReplicationBlocks）中，表示该block的状态为'pending'(正在备份中)。'pending'表示如果失败了还可以重试
          pendingReplications.add(block, targets.length);
          NameNode.stateChangeLog.debug(
            "BLOCK* block " + block
              + " is moved from neededReplications to pendingReplications");

          // remove from neededReplications
          //从 neededReplication列表中移除该block
          if (numEffectiveReplicas + targets.length &gt;= requiredReplication) {
            neededReplications.remove(block, priority); // remove from neededReplications
            replIndex[priority]--;
          }
        }
      }
    } finally {
      writeUnlock();
    }

    // 更新 metrics
    updateReplicationMetrics(work);

    // 打印debug信息
    if(NameNode.stateChangeLog.isInfoEnabled()){
      // log which blocks have been scheduled for replication
      for(ReplicationWork rw : work){
        // report scheduled blocks
        DatanodeDescriptor[] targets = rw.targets;
        if (targets != null &amp;&amp; targets.length != 0) {
          StringBuffer targetList = new StringBuffer("datanode(s)");
          for (int k = 0; k &lt; targets.length; k++) {
            targetList.append(' ');
            targetList.append(targets[k].getName());
          }
          NameNode.stateChangeLog.info(
            "BLOCK* ask "
              + rw.srcNode.getName() + " to replicate "
              + rw.block + " to " + targetList);
        }
      }
    }

    // 记录一次备份操作
    if (NameNode.stateChangeLog.isDebugEnabled()) {
      NameNode.stateChangeLog.debug("BLOCK* neededReplications = "
          + neededReplications.size() + " pendingReplications = "
          + pendingReplications.size());
    }
    return scheduledWork;
  }
</code></pre>

<h4>1.1.2.1. 选取源datanode —— chooseSourceDatanode（）</h4>

<p>获取一个源datanode节点</p>

<pre><code>  private DatanodeDescriptor chooseSourceDatanode(
                                    Block block,
                                    List&lt;DatanodeDescriptor&gt; containingNodes,
                                    NumberReplicas numReplicas) {
    containingNodes.clear();
    DatanodeDescriptor srcNode = null;
    int live = 0;
    int decommissioned = 0;
    int corrupt = 0;
    int excess = 0;
    Iterator&lt;DatanodeDescriptor&gt; it = blocksMap.nodeIterator(block);
    Collection&lt;DatanodeDescriptor&gt; nodesCorrupt = corruptReplicas.getNodes(block);
    while(it.hasNext()) {
      DatanodeDescriptor node = it.next();
      Collection&lt;Block&gt; excessBlocks = 
        excessReplicateMap.get(node.getStorageID());
      if ((nodesCorrupt != null) &amp;&amp; (nodesCorrupt.contains(node)))
        corrupt++;
      else if (node.isDecommissionInProgress() || node.isDecommissioned())
        decommissioned++;
      else if (excessBlocks != null &amp;&amp; excessBlocks.contains(block)) {
        excess++;
      } else {
        live++;
      }
      containingNodes.add(node);
      // Check if this replica is corrupt
      // If so, do not select the node as src node
      if ((nodesCorrupt != null) &amp;&amp; nodesCorrupt.contains(node))
        continue;
      if(node.getNumberOfBlocksToBeReplicated() &gt;= maxReplicationStreams)
        continue; // already reached replication limit
      // the block must not be scheduled for removal on srcNode
      if(excessBlocks != null &amp;&amp; excessBlocks.contains(block))
        continue;
      // never use already decommissioned nodes
      if(node.isDecommissioned())
        continue;
      // we prefer nodes that are in DECOMMISSION_INPROGRESS state
      if(node.isDecommissionInProgress() || srcNode == null) {
        srcNode = node;
        continue;
      }
      if(srcNode.isDecommissionInProgress())
        continue;
      // switch to a different node randomly
      // this to prevent from deterministically selecting the same node even
      // if the node failed to replicate the block on previous iterations
      if(r.nextBoolean())
        srcNode = node;
    }
    if(numReplicas != null)
      numReplicas.initialize(live, decommissioned, corrupt, excess);
    return srcNode;
  }
</code></pre>

<h4>1.1.2.2. 选取目标datanode —— chooseTarget(rw);</h4>

<pre><code>  private DatanodeDescriptor[] chooseTarget(ReplicationWork work) {
    if (!neededReplications.contains(work.block)) {
      return null;
    }
    if (work.blockSize == BlockCommand.NO_ACK) {
      LOG.warn("Block " + work.block.getBlockId() + 
          " of the file " + work.fileINode.getFullPathName() + 
          " is invalidated and cannot be replicated.");
      return null;
    }
    if (work.blockSize == DFSUtil.DELETED) {
      LOG.warn("Block " + work.block.getBlockId() + 
          " of the file " + work.fileINode.getFullPathName() + 
          " is a deleted block and cannot be replicated.");
      return null;
    }
    //实际调用replicator(BlockPlacementPolicy)的chooseTarget方法选取target
    return replicator.chooseTarget(work.fileINode,
        work.numOfReplicas, work.srcNode,
        work.containingNodes, null, work.blockSize);
  }
</code></pre>

<h2>副本存放策略</h2>

<p><img src="img/BlockAllocation.png" alt="BlockAllocation" /></p>

<p>如图所示，HDFS默认的副本存放策略为：</p>

<ol>
<li>第一个副本存放在当前datanode的本地</li>
<li>第二个副本存放在与第一个副本所在datanode不在同一机架上的一个datanode上</li>
<li>第三个副本存放在与第二个副本同一机架但不同datanode上</li>
</ol>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/10/18/hdfs-heartbeat/">HDFS源码学习(4)——DataNode心跳检测（HeartBeat）</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-10-18T21:38:00+08:00" pubdate data-updated="true">Oct 18<span>th</span>, 2012</time>
        
         | <a href="/blog/2012/10/18/hdfs-heartbeat/#disqus_thread">Comments</a>
        
        
          | 

<span class="categories">
  
    <a class='category' href='/blog/categories/hdfs/'>HDFS</a>
  
</span>


        
      </p>
    
  </header>


  <div class="entry-content"><p>HDFS中DataNode的心跳检测通过FSNameSystem中的HeartbeatMonitor完成。代码结构如下：</p>

<pre><code>  class HeartbeatMonitor implements Runnable {
    /**
     */
    public void run() {
      while (fsRunning) {
        try {
          heartbeatCheck();
        } catch (Exception e) {
          FSNamesystem.LOG.error(StringUtils.stringifyException(e));
        }
        try {
          Thread.sleep(heartbeatRecheckInterval);
        } catch (InterruptedException ie) {
        }
      }
    }
  }
</code></pre>

<p>代码很简单，心跳检测线程周期性调用heartbeatCheck()。</p>

<h2>一、心跳检查——heartbeatCheck()</h2>

<p>该方法主要用于检测是否有过期的心跳检测，如有，检测其上的block是否已经进行过重新备份。该线程每次只处理一个datanode。</p>

<pre><code>  void heartbeatCheck() {
    if (isInSafeMode()) {
      // 安全模式下不做心跳检测
      return;
    }
    boolean allAlive = false;
    while (!allAlive) {
      boolean foundDead = false;
      DatanodeID nodeID = null;

      // 获取第一个dead datanode
      synchronized(heartbeats) {
        for (Iterator&lt;DatanodeDescriptor&gt; it = heartbeats.iterator();
             it.hasNext();) {
          DatanodeDescriptor nodeInfo = it.next();
          if (isDatanodeDead(nodeInfo)) {
            foundDead = true;
            nodeID = nodeInfo;
            break;
          }
        }
      }

      // 申请fsnamesystem锁，删除dead datanode
      if (foundDead) {
        writeLock();
        try {
          synchronized(heartbeats) {
            synchronized (datanodeMap) {
              DatanodeDescriptor nodeInfo = null;
              try {
                nodeInfo = getDatanode(nodeID);
              } catch (IOException e) {
                nodeInfo = null;
              }
              if (nodeInfo != null &amp;&amp; isDatanodeDead(nodeInfo)) {
                NameNode.stateChangeLog.info("BLOCK* NameSystem.heartbeatCheck: "
                                             + "lost heartbeat from " + nodeInfo.getName());
                removeDatanode(nodeInfo);
              }
            }
          }
        } finally {
          writeUnlock();
        }
      }
      allAlive = !foundDead;
    }
  }
</code></pre>

<h3>1.1 判断是否已死 —— isDatanodeDead（）</h3>

<p>判断一个datanode是否已经dead的标准很简单，当前距该节点最后的更新时间差是否已经超过心跳检测的过期时间限制</p>

<pre><code>private boolean isDatanodeDead(DatanodeDescriptor node) {
    return (node.getLastUpdate() &lt;
            (now() - heartbeatExpireInterval));
  }
</code></pre>

<h3>1.2 删除datanode —— removeDatanode（）</h3>

<pre><code>  private void removeDatanode(DatanodeDescriptor nodeInfo) {
    synchronized (heartbeats) {
      if (nodeInfo.isAlive) {
        updateStats(nodeInfo, false);
        //从heartbeats中移除
        heartbeats.remove(nodeInfo);
        //更新datanode状态
        nodeInfo.isAlive = false;
      }
    }

    nodeInfo.hasInitialBlockReport = false;
    for (Iterator&lt;Block&gt; it = nodeInfo.getBlockIterator(); it.hasNext();) {
      //移除该节点上的block
      removeStoredBlock(it.next(), nodeInfo);
    }
    unprotectedRemoveDatanode(nodeInfo);
    clusterMap.remove(nodeInfo);
  }
</code></pre>

<h4>1.2.1 removeStoredBlock（）</h4>

<p>该方法更新block->datanode的映射(blocksMap)，如果block还有效，有可能导致block备份发生</p>

<pre><code>  void removeStoredBlock(Block block, DatanodeDescriptor node) {
    if (NameNode.stateChangeLog.isDebugEnabled()) {
      NameNode.stateChangeLog.debug("BLOCK* NameSystem.removeStoredBlock: "
                                    +block + " from "+node.getName());
    }
    assert (hasWriteLock());
    if (!blocksMap.removeNode(block, node)) {
      if (NameNode.stateChangeLog.isDebugEnabled()) {
        NameNode.stateChangeLog.debug("BLOCK* NameSystem.removeStoredBlock: "
                                      +block+" has already been removed from node "+node);
      }
      return;
    }

    //
    //检查是否需要备份删除的block
    INode fileINode = blocksMap.getINode(block);
    if (fileINode != null) {
      //减小当前系统中安全block（备份数满足最小值的block）数量
      decrementSafeBlockCount(block);
      //更新需要备份的block数量
      updateNeededReplications(block, -1, 0);
    }

    //
    // 从excessblocks中删除改block，并从excessReplicateMap删除改datanode
    Collection&lt;Block&gt; excessBlocks = excessReplicateMap.get(node.getStorageID());
    if (excessBlocks != null) {
      if (excessBlocks.remove(block)) {
        excessBlocksCount--;
        if (NameNode.stateChangeLog.isDebugEnabled()) {
          NameNode.stateChangeLog.debug("BLOCK* NameSystem.removeStoredBlock: "
              +block+" is removed from excessBlocks");
        }
        if (excessBlocks.size() == 0) {
          excessReplicateMap.remove(node.getStorageID());
        }
      }
    }

    // 从corruptReplicas中移除该block
    corruptReplicas.removeFromCorruptReplicasMap(block, node);
  }
</code></pre>

<h5>1.2.1.1  BlocksMap.removeNode（）</h5>

<p>其中BlocksMap.removeNode()方法如下：</p>

<pre><code>  boolean removeNode(Block b, DatanodeDescriptor node) {
    BlockInfo info = blocks.get(b);
    if (info == null)
      return false;

    // 从datanode 的blocklist中移除block，并从block的datalist中移除datanode
    boolean removed = node.removeBlock(info);

    if (info.getDatanode(0) == null     // no datanodes left
              &amp;&amp; info.inode == null) {  // does not belong to a file
      //从blocksmap中移除该block
      blocks.remove(b);  // remove block from the map
    }
    return removed;
  }
</code></pre>

<h5>1.2.1.2 减小当前安全的block数 —— decrementSafeBlockCount()</h5>

<p>减小当前副本数安全的的block数，此举有可能触发系统进入安全模式（safemode）</p>

<pre><code>  void decrementSafeBlockCount(Block b) {
    if (safeMode == null) // mostly true
      return;

    safeMode.decrementSafeBlockCount((short)countNodes(b).liveReplicas());
  }
</code></pre>

<p>其中safeMode.decrementSafeBlockCount()代码如下：</p>

<pre><code>synchronized void decrementSafeBlockCount(short replication) {

  if (replication == safeReplication-1) {
    //安全的block数减一
    this.blockSafe--;
    //检查是否需要进入到safemode
    checkMode();
  }
}
</code></pre>

<p>SafeModeInfo.checkMode()代码如下：</p>

<pre><code>    private void checkMode() {
      //当安全的block数比例降至安全值以下，进入安全模式
      if (needEnter()) {
        enter();
        // check if we are ready to initialize replication queues
        if (canInitializeReplQueues() &amp;&amp; !isPopulatingReplQueues()) {
          //初始化副本队列
          initializeReplQueues();
        }
        reportStatus("STATE* Safe mode ON.", false);
        return;
      }
      // 如果安全模式已经关闭或者门槛小于0，则跳出安全模式
      if (!isOn() ||                           // safe mode is off
          extension &lt;= 0 || threshold &lt;= 0) {  // don't need to wait
        this.leave(true); // leave safe mode
        return;
      }

      //之前已经进入安全模式，直接返回
      if (reached &gt; 0) {  // threshold has already been reached before
        reportStatus("STATE* Safe mode ON.", false);
        return;
      }
      // 启动SafeModeMonitor线程
      reached = now();
      smmthread = new Daemon(new SafeModeMonitor());
      smmthread.start();
      reportStatus("STATE* Safe mode extension entered.", true);
    }
</code></pre>

<h5>1.2.1.3 更新需要备份的列表 —— updateNeededReplications（）</h5>

<pre><code>  /* updates a block in under replication queue */
  void updateNeededReplications(Block block,
                        int curReplicasDelta, int expectedReplicasDelta) {
    writeLock();
    try {
    //计算当前副本数
    NumberReplicas repl = countNodes(block);
    //期望的副本数
    int curExpectedReplicas = getReplication(block);
    //将该block更新到需要备份的列表中（neededReplications）
    neededReplications.update(block, 
                              repl.liveReplicas(), 
                              repl.decommissionedReplicas(),
                              curExpectedReplicas,
                              curReplicasDelta, expectedReplicasDelta);
    } finally {
      writeUnlock();
    }
  }
</code></pre>

<h4>1.2.2 移除datanode —— unprotectedRemoveDatanode</h4>

<pre><code>  void unprotectedRemoveDatanode(DatanodeDescriptor nodeDescr) {
    //重置清空datanode中block信息
    nodeDescr.resetBlocks();
    //从invlidateSet中移除datanode
    removeFromInvalidates(nodeDescr.getStorageID());
    if (NameNode.stateChangeLog.isDebugEnabled()) {
      NameNode.stateChangeLog.debug(
                                    "BLOCK* NameSystem.unprotectedRemoveDatanode: "
                                    + nodeDescr.getName() + " is out of service now.");
    }
  }
</code></pre>

<h5>1.2.2.1 removeFromInvalidates()</h5>

<pre><code>  private void removeFromInvalidates(String storageID) {
    //从recentInvalidateSet中移除该datanode
    Collection&lt;Block&gt; blocks = recentInvalidateSets.remove(storageID);
    if (blocks != null) {
      //从正在删除的block总数中减去当前节点上的block总数
      pendingDeletionBlocksCount -= blocks.size();
    }
  }
</code></pre>

<h2>二、 处理心跳检测请求 —— handleHeartbeat()</h2>

<p>NameNode只负责创建一个HeartbeatMonitor来通过每个datanode的最新更新时间周期性检查是否有过期的datanode，而每个datanode是否的最新更新时间是由datanode主动向namenode报告的，namenode通过handleHeartbeat()处理心跳请求。</p>

<pre><code>  DatanodeCommand[] handleHeartbeat(DatanodeRegistration nodeReg,
      long capacity, long dfsUsed, long remaining,
      int xceiverCount, int xmitsInProgress) throws IOException {
    DatanodeCommand cmd = null;
    synchronized (heartbeats) {
      synchronized (datanodeMap) {
        DatanodeDescriptor nodeinfo = null;
        try {
          nodeinfo = getDatanode(nodeReg);
        } catch(UnregisteredDatanodeException e) {
          return new DatanodeCommand[]{DatanodeCommand.REGISTER};
        }

        // 检查该datanode是否需要被关闭，可以通过设置datanode的adminState为DECOMMISSIONED来关闭一个datanode
        if (nodeinfo != null &amp;&amp; shouldNodeShutdown(nodeinfo)) {
          setDatanodeDead(nodeinfo);
          throw new DisallowedDatanodeException(nodeinfo);
        }

        if (nodeinfo == null || !nodeinfo.isAlive) {
          return new DatanodeCommand[]{DatanodeCommand.REGISTER};
        }

        updateStats(nodeinfo, false);
        nodeinfo.updateHeartbeat(capacity, dfsUsed, remaining, xceiverCount);
        updateStats(nodeinfo, true);

        //检查租约恢复状态
        cmd = nodeinfo.getLeaseRecoveryCommand(Integer.MAX_VALUE);
        if (cmd != null) {
          return new DatanodeCommand[] {cmd};
        }

        ArrayList&lt;DatanodeCommand&gt; cmds = new ArrayList&lt;DatanodeCommand&gt;(2);
        //检查正在备份中的副本
        cmd = nodeinfo.getReplicationCommand(
              maxReplicationStreams - xmitsInProgress);
        if (cmd != null) {
          cmds.add(cmd);
        }
        //检查无效的block
        cmd = nodeinfo.getInvalidateBlocks(blockInvalidateLimit);
        if (cmd != null) {
          cmds.add(cmd);
        }
        if (!cmds.isEmpty()) {
          return cmds.toArray(new DatanodeCommand[cmds.size()]);
        }
      }
    }

    //检查是否需要升级系统
    cmd = getDistributedUpgradeCommand();
    if (cmd != null) {
      return new DatanodeCommand[] {cmd};
    }
    return null;
  }
</code></pre>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/10/18/hdfs-namenode-thread/">HDFS源码学习（3）——NameNode中的线程</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-10-18T21:37:00+08:00" pubdate data-updated="true">Oct 18<span>th</span>, 2012</time>
        
         | <a href="/blog/2012/10/18/hdfs-namenode-thread/#disqus_thread">Comments</a>
        
        
          | 

<span class="categories">
  
    <a class='category' href='/blog/categories/hdfs/'>HDFS</a>
  
</span>


        
      </p>
    
  </header>


  <div class="entry-content"><p>NameNode存在三种运行模式：</p>

<ol>
<li>Normal： NameNode正常服务的状态</li>
<li>Safe mode：NameNode重启时进入Safe mode，该模式下整个系统是只读的，以便于NameNode手机DataNode信息</li>
<li>Backup mode：备份NameNode处于Backup mode，被动的接收主NameNode的检查点信息</li>
</ol>


<p>在NameNode中存在如下几种线程：</p>

<ol>
<li>DataNode 健康检查管理线程</li>
<li>副本管理线程</li>
<li>租约管理（lease Management）</li>
<li>IPC Handler 线程</li>
</ol>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/10/18/hdfs-namenode-startup/">HDFS源码学习(2)——NameNode初始化</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-10-18T21:35:00+08:00" pubdate data-updated="true">Oct 18<span>th</span>, 2012</time>
        
         | <a href="/blog/2012/10/18/hdfs-namenode-startup/#disqus_thread">Comments</a>
        
        
          | 

<span class="categories">
  
    <a class='category' href='/blog/categories/hdfs/'>HDFS</a>
  
</span>


        
      </p>
    
  </header>


  <div class="entry-content"><h2>main()</h2>

<pre><code>  public static void main(String argv[]) throws Exception {
    try {
      StringUtils.startupShutdownMessage(NameNode.class, argv, LOG);
      //创建nameNode
      NameNode namenode = createNameNode(argv, null);
      if (namenode != null)
        namenode.join();
    } catch (Throwable e) {
      LOG.error(StringUtils.stringifyException(e));
      System.exit(-1);
    }
  }
</code></pre>

<h3>createNameNode()</h3>

<pre><code>  public static NameNode createNameNode(String argv[], 
                                 Configuration conf) throws IOException {
    if (conf == null)
      conf = new Configuration();
      //从命令行参数中提取启动配置项数据
    StartupOption startOpt = parseArguments(argv);
    if (startOpt == null) {
      printUsage();
      return null;
    }
    //设置启动参数
    setStartupOption(conf, startOpt);

    switch (startOpt) {
      case FORMAT:
        boolean aborted = format(conf, true);
        System.exit(aborted ? 1 : 0);
      case FINALIZE:
        aborted = finalize(conf, true);
        System.exit(aborted ? 1 : 0);
      default:
    }

    //新建NameNode
    NameNode namenode = new NameNode(conf);
    return namenode;
  }
</code></pre>

<h3>NameNode()</h3>

<pre><code>  public NameNode(Configuration conf) throws IOException {
    super(conf);
    try {
    //初始化
      initialize(getConf());
    } catch (IOException e) {
      this.stop();
      throw e;
    }
  }
</code></pre>

<h3>initialize()</h3>

<pre><code>  private void initialize(Configuration conf) throws IOException {
    InetSocketAddress socAddr = NameNode.getAddress(conf);
    int handlerCount = conf.getInt("dfs.namenode.handler.count", 10);
    // 关键-&gt;创建一个RPC Server
    this.server = RPC.getServer(this, socAddr.getHostName(), socAddr.getPort(),
                                handlerCount, false, conf);

    // The rpc-server port can be ephemeral... ensure we have the correct info
    this.serverAddress = this.server.getListenerAddress(); 
    FileSystem.setDefaultUri(conf, getUri(serverAddress));
    LOG.info("Namenode up at: " + this.serverAddress);

    myMetrics = new NameNodeMetrics(conf, this);

    //关键-&gt;创建一个FSNameSystem
    this.namesystem = new FSNamesystem(this, conf);
    //启动HTTP Server
    startHttpServer(conf);
    //启动RPC Server
    this.server.start();  //start RPC server   

    startTrashEmptier(conf);
  }
</code></pre>

<h2>FSNameSystem()</h2>

<pre><code>  FSNamesystem(NameNode nn, Configuration conf) throws IOException {
    try {
      //初始化FSNameSystem
      initialize(nn, conf);
      userPasswordInformation = new UserPasswordInformation(conf);
      extendAccessControlList = new ExtendAccessControlList(conf);
    } catch(IOException e) {
      LOG.error(getClass().getSimpleName() + " initialization failed.", e);
      close();
      throw e;
    }
  }
</code></pre>

<h3>FSNameSystem.initialize()</h3>

<pre><code>  private void initialize(NameNode nn, Configuration conf) throws IOException {
    this.systemStart = now();
    this.fsLock = new ReentrantReadWriteLock(); // non-fair locking
    setConfigurationParameters(conf);

    this.nameNodeAddress = nn.getNameNodeAddress();
    this.registerMBean(conf); // register the MBean for the FSNamesystemStutus

    //创建FSDirectory
    this.dir = new FSDirectory(this, conf);
    StartupOption startOpt = NameNode.getStartupOption(conf);

    //加载FSImage
    this.dir.loadFSImage(getNamespaceDirs(conf),
                         getNamespaceEditsDirs(conf), startOpt);
    long timeTakenToLoadFSImage = now() - systemStart;
    LOG.info("Finished loading FSImage in " + timeTakenToLoadFSImage + " msecs");
    NameNode.getNameNodeMetrics().fsImageLoadTime.set(
                              (int) timeTakenToLoadFSImage);
    this.safeMode = new SafeModeInfo(conf);
    setBlockTotal();
    //创建PendingReplicationBlocks
    pendingReplications = new PendingReplicationBlocks(
                            conf.getInt("dfs.replication.pending.timeout.sec", 
                                        -1) * 1000L);
    //创建心跳检查线程                                          
    this.hbthread = new Daemon(new HeartbeatMonitor());
    //创建租约管理线程
    this.lmthread = new Daemon(leaseManager.new Monitor());
    //创建副本管理线程
    this.replthread = new Daemon(new ReplicationMonitor());
    hbthread.start();
    lmthread.start();
    replthread.start();

    // 副本超额block管理线程
    this.overreplthread = new Daemon(new OverReplicationMonitor());
    overreplthread.start();

    this.hostsReader = new HostsFileReader(conf.get("dfs.hosts",""),
                                           conf.get("dfs.hosts.exclude",""));
    //创建退役节点管理线程
    this.dnthread = new Daemon(new DecommissionManager(this).new Monitor(
        conf.getInt("dfs.namenode.decommission.interval", 30),
        conf.getInt("dfs.namenode.decommission.nodes.per.interval", 5)));
    dnthread.start();

    this.dnsToSwitchMapping = ReflectionUtils.newInstance(
        conf.getClass("topology.node.switch.mapping.impl", ScriptBasedMapping.class,
            DNSToSwitchMapping.class), conf);

    /* If the dns to swith mapping supports cache, resolve network 
     * locations of those hosts in the include list, 
     * and store the mapping in the cache; so future calls to resolve
     * will be fast.
     */
    if (dnsToSwitchMapping instanceof CachedDNSToSwitchMapping) {
      dnsToSwitchMapping.resolve(new ArrayList&lt;String&gt;(hostsReader.getHosts()));
    }
    //创建副本定位器用于定位副本存放位置
    this.replicator = BlockPlacementPolicy.getInstance(
        conf,
        this,
        this.clusterMap,
        this.hostsReader,
        this.dnsToSwitchMapping,
        this);
  }
</code></pre>

<h2>FSDirectory(this, conf)</h2>

<p>新建FSDirecotry</p>

<pre><code> FSDirectory(FSNamesystem ns, Configuration conf) {
    //创建一个FSImage，并实例化构建FSDirectory
    this(new FSImage(), ns, conf);
    fsImage.setCheckpointDirectories(FSImage.getCheckpointDirs(conf, null),
                                FSImage.getCheckpointEditsDirs(conf, null));
  }
</code></pre>

<h3>this(new FSImage(), ns, conf);</h3>

<pre><code>FSDirectory(FSImage fsImage, FSNamesystem ns, Configuration conf) {
    this.bLock = new ReentrantReadWriteLock(); // non-fair
    this.cond = bLock.writeLock().newCondition();
    //创建根目录
    rootDir = new INodeDirectoryWithQuota(INodeDirectory.ROOT_NAME,
        ns.createFsOwnerPermissions(new FsPermission((short)0755)),
        Integer.MAX_VALUE, -1);
    this.fsImage = fsImage;
    namesystem = ns;
    initialize(conf);
  }
</code></pre>

<h3>FSDirectory.initialize()</h3>

<pre><code>  private void initialize(Configuration conf) {
    MetricsContext metricsContext = MetricsUtil.getContext("dfs");
    directoryMetrics = MetricsUtil.createRecord(metricsContext, "FSDirectory");
    directoryMetrics.setTag("sessionId", conf.get("session.id"));
  }
</code></pre>

<h3>FSDirectory.loadFSImage()</h3>

<pre><code>  void loadFSImage(Collection&lt;File&gt; dataDirs,
                   Collection&lt;File&gt; editsDirs,
                   StartupOption startOpt) throws IOException {
    // format before starting up if requested
    if (startOpt == StartupOption.FORMAT) {
      fsImage.setStorageDirectories(dataDirs, editsDirs);
      fsImage.format();
      startOpt = StartupOption.REGULAR;
    }
    try {
      //从datadir和editdirs加载FSImage
      if (fsImage.recoverTransitionRead(dataDirs, editsDirs, startOpt)) {
        fsImage.saveNamespace(true);
      }
      //初始化Editlog
      FSEditLog editLog = fsImage.getEditLog();
      assert editLog != null : "editLog must be initialized";
      if (!editLog.isOpen())
        editLog.open();
      fsImage.setCheckpointDirectories(null, null);
    } catch(IOException e) {
      fsImage.close();
      throw e;
    }
    writeLock();
    try {
      this.ready = true;
      cond.signalAll();
    } finally {
      writeUnlock();
    }
  }
</code></pre>

<h3>FSImage.recoverTransitionRead（）</h3>

<pre><code>  boolean recoverTransitionRead(Collection&lt;File&gt; dataDirs,
                             Collection&lt;File&gt; editsDirs,
                                StartupOption startOpt
                                ) throws IOException {
    assert startOpt != StartupOption.FORMAT : 
      "NameNode formatting should be performed before reading the image";

    // none of the data dirs exist
    if (dataDirs.size() == 0 || editsDirs.size() == 0)  
      throw new IOException(
        "All specified directories are not accessible or do not exist.");

    if(startOpt == StartupOption.IMPORT 
        &amp;&amp; (checkpointDirs == null || checkpointDirs.isEmpty()))
      throw new IOException("Cannot import image from a checkpoint. "
                          + "\"fs.checkpoint.dir\" is not set." );

    if(startOpt == StartupOption.IMPORT 
        &amp;&amp; (checkpointEditsDirs == null || checkpointEditsDirs.isEmpty()))
      throw new IOException("Cannot import image from a checkpoint. "
                          + "\"fs.checkpoint.edits.dir\" is not set." );

    setStorageDirectories(dataDirs, editsDirs);
    // 1.检查所有目录的状态和一致性
    Map&lt;StorageDirectory, StorageState&gt; dataDirStates = 
             new HashMap&lt;StorageDirectory, StorageState&gt;();
    boolean isFormatted = false;
    for (Iterator&lt;StorageDirectory&gt; it = 
                      dirIterator(); it.hasNext();) {
      StorageDirectory sd = it.next();
      StorageState curState;
      try {
        curState = sd.analyzeStorage(startOpt);
        // sd is locked but not opened
        switch(curState) {
        case NON_EXISTENT:
          // name-node fails if any of the configured storage dirs are missing
          throw new InconsistentFSStateException(sd.getRoot(),
                                                 "storage directory does not exist or is not accessible.");
        case NOT_FORMATTED:
          break;
        case NORMAL:
          break;
        default:  // recovery is possible
          sd.doRecover(curState);      
        }
        if (curState != StorageState.NOT_FORMATTED 
            &amp;&amp; startOpt != StartupOption.ROLLBACK) {
          sd.read(); // read and verify consistency with other directories
          isFormatted = true;
        }
        if (startOpt == StartupOption.IMPORT &amp;&amp; isFormatted)
          // import of a checkpoint is allowed only into empty image directories
          throw new IOException("Cannot import image from a checkpoint. " 
              + " NameNode already contains an image in " + sd.getRoot());
      } catch (IOException ioe) {
        sd.unlock();
        throw ioe;
      }
      dataDirStates.put(sd,curState);
    }

    if (!isFormatted &amp;&amp; startOpt != StartupOption.ROLLBACK 
                     &amp;&amp; startOpt != StartupOption.IMPORT)
      throw new IOException("NameNode is not formatted.");
    if (layoutVersion &lt; LAST_PRE_UPGRADE_LAYOUT_VERSION) {
      checkVersionUpgradable(layoutVersion);
    }
    if (startOpt != StartupOption.UPGRADE
          &amp;&amp; layoutVersion &lt; LAST_PRE_UPGRADE_LAYOUT_VERSION
          &amp;&amp; layoutVersion != FSConstants.LAYOUT_VERSION)
        throw new IOException(
                          "\nFile system image contains an old layout version " + layoutVersion
                          + ".\nAn upgrade to version " + FSConstants.LAYOUT_VERSION
                          + " is required.\nPlease restart NameNode with -upgrade option.");
    // check whether distributed upgrade is reguired and/or should be continued
    verifyDistributedUpgradeProgress(startOpt);

    // 2. Format unformatted dirs.
    this.checkpointTime = 0L;
    for (Iterator&lt;StorageDirectory&gt; it = 
                     dirIterator(); it.hasNext();) {
      StorageDirectory sd = it.next();
      StorageState curState = dataDirStates.get(sd);
      switch(curState) {
      case NON_EXISTENT:
        assert false : StorageState.NON_EXISTENT + " state cannot be here";
      case NOT_FORMATTED:
        LOG.info("Storage directory " + sd.getRoot() + " is not formatted.");
        LOG.info("Formatting ...");
        sd.clearDirectory(); // create empty currrent dir
        break;
      default:
        break;
      }
    }
</code></pre>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/10/18/hdfs-namenode-datastructure/">HDFS源码学习（1）——NameNode主要数据结构</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-10-18T11:04:00+08:00" pubdate data-updated="true">Oct 18<span>th</span>, 2012</time>
        
         | <a href="/blog/2012/10/18/hdfs-namenode-datastructure/#disqus_thread">Comments</a>
        
        
          | 

<span class="categories">
  
    <a class='category' href='/blog/categories/hdfs/'>HDFS</a>
  
</span>


        
      </p>
    
  </header>


  <div class="entry-content"><h2>FSNameSystem</h2>

<p>FSNameSystem是HDFS文件系统实际执行的核心，提供各种增删改查文件操作接口。其内部维护多个数据结构之间的关系：</p>

<ol>
<li>fsname->block列表的映射</li>
<li>所有有效blocks集合</li>
<li>block与其所属的datanodes之间的映射（该映射是通过block reports动态构建的，维护在namenode的内存中。每个datanode在启动时向namenode报告其自身node上的block）</li>
<li>每个datanode与其上的blocklist的映射</li>
<li>采用心跳检测根据LRU算法更新的机器（datanode）列表</li>
</ol>


<h3>FSDirectory</h3>

<p>FSDirectory用于维护当前系统中的文件树。</p>

<p>其内部主要组成结构包括一个INodeDirectoryWithQuota作为根目录(rootDir)和一个FSImage来持久化文件树的修改操作。</p>

<h4>INode</h4>

<p>HDFS中文件树用类似VFS中INode的方式构建，整个HDFS中文件被表示为INodeFile，目录被表示为INodeDirectory。INodeDiretoryWithQuota是INodeDirectory的扩展类，即带配额的文件目录</p>

<p><img src="/images/hdfs/INode.png" alt="INode" /></p>

<p>INodeFile表示INode书中的一个文件，扩展自INode，除了名字(name)，父节点(parent)等之外，一个主要元素是blocks，一个BlockInfo数组，表示该文件对应的block信息。</p>

<h3>BlocksMap</h3>

<p>BlocksMap用于维护Block -> { INode, datanodes, self ref } 的映射
<img src="/images/hdfs/BlocksMap.png" alt="BlocksMap" />
BlocksMap结构比较简单，实际上就是一个Block到BlockInfo的映射。</p>

<h4>Block</h4>

<p>Block是HDFS中的基本读写单元，主要包括：</p>

<ol>
<li>blockId: 一个long类型的块id</li>
<li>numBytes: 块大小</li>
<li>generationStamp: 块更新的时间戳</li>
</ol>


<h4>BlockInfo</h4>

<p>BlockInfo扩展自Block，除基本信息外还包括一个inode引用，表示该block所属的文件；以及一个神奇的三元组数组Object[] triplets，用来表示保存该block的datanode信息，假设系统中的备份数量为3。那么这个数组结构如下：</p>

<p><img src="/images/hdfs/triplets.png" alt="triplets" /></p>

<ol>
<li>DN1，DN2，DN3分别表示存有改block的三个datanode的引用(DataNodeDescriptor）</li>
<li>DN1-prev-blk表示在DN1上block列表中当前block的前置block引用</li>
<li>DN1-next-blk表示在DN1上block列表中当前block的后置block引用</li>
</ol>


<p>DN2,DN3的prev-blk和next-blk类似。
HDFS采用这种结构存放block->datanode list的信息主要是为了节省内存空间，block->datanodelist之间的映射关系需要占用大量内存，如果同样还要将datanode->blockslist的信息保存在内存中，同样要占用大量内存。采用三元组这种方式能够从其中一个block获得到改block所属的datanode上的所有block列表。</p>

<h4>FSImage</h4>

<p>FSImage用于持久化文件树的变更以及系统启动时加载持久化数据。
HDFS启动时通过FSImage来加载磁盘中原有的文件树，系统Standby之后，通过FSEditlog来保存在文件树上的修改，FSEditLog定期将保存的修改信息刷到FSImage中进行持久化存储。
FSImage中文件元信息的存储结构如下（参见FImage.saveFSImage()方法）</p>

<p><img src="/images/hdfs/FSImage.png" alt="FSImage" /></p>

<h5>FSImage头部信息</h5>

<ol>
<li>layoutVersion(int):image layout版本号，0.19版本的hdfs中为-18</li>
<li>namespaceId(int): 命名空间ID，系统初始化时生成，在一个namenode生命周期内保持不变，datanode想namenode注册是返回改id作为registerId，以后每次datanode与namenode通信时都携带该id，不认识的id的请求将被拒绝。</li>
<li>numberItemOfTree(long): 系统中的文件总数</li>
<li>generationTimeStamp: 生成image的时间戳</li>
</ol>


<h5>INode信息</h5>

<p>FSImage头之后是numberItemOfTree个INode信息，INode信息分为文件(INodeFile)和文件目录(INodeDirectory)两类，两者大体一致，分为INode头，Blocks区（目录没有blocks）和文件权限。</p>

<p><strong>INode头</strong></p>

<ol>
<li>nameLen(short): 文件名长度</li>
<li>filename(String): 文件名</li>
<li>replication(short): 备份数量</li>
<li>modificationTime(long): 最近修改时间</li>
<li>accessTime(long): 最近访问时间</li>
<li>preferedBlockSize(long): 块大小（目录为0）</li>
<li>block num(int): 块数量（目录为-1）</li>
</ol>


<p><strong>Blocks区</strong></p>

<ol>
<li>blockId(long)</li>
<li>numBytes(long,block大小)</li>
<li>generationTimeStamp(long, 更新时间戳）</li>
</ol>


<p><strong>文件权限</strong></p>

<ol>
<li>username(String): 文件用户名</li>
<li>group(String): 所属组</li>
<li>fileperm(short): 文件权限</li>
</ol>


<h5>underconstructionFile区</h5>

<p>layoutverion&lt;-18版本的fsimage还包括正在构建的文件区。与普通Inode信息类似，均有inode头和blocks区以及文件权限，除此之外，underConstructionFile还包括：</p>

<p><strong>client信息</strong></p>

<ol>
<li>clientName：client明</li>
<li>clientMachine： client机器名</li>
</ol>


<p><strong>已分配的datanode信息</strong></p>

<ol>
<li>ipcport： 服务端口</li>
<li>capacity: 容量</li>
<li>dfsuse： 已使用的空间</li>
<li>remaining： 剩余空间</li>
<li>lastupdate： 最新更新时间</li>
<li>xceiverCount</li>
<li>location： datanode位置</li>
<li>hostName：主机名</li>
<li>state： admin管理状态</li>
</ol>


<h2>其他结构</h2>

<h3>CorruptReplicasMap</h3>

<p>CorruptReplicasMap通过一个TreeMap维护corrupt状态block的blocks&#8211;>datanodedescriptor(s)映射。一个block备份在多个datanode中，当其中的一个或多个datanode上的block损坏时，会将该datanode加到treeMap中该block对应的datanodeDescriptor集合中。FSNameSystem通过该Map来维护所有损坏的block与其对应datanode的关系。</p>

<h3>Map&lt;String, LightWeightHashSet<Block>> recentInvalidateSets</h3>

<p>维护最近失效的block集合，map中为storageId->ArrayList<Block>，当某个block的一个datanode上副本失效时会将改block和对应的datanode的storeageId添加到recentInvalidateSet中，当datanode想namenode进行heartbeat时，namenode会检查该datanode中是否有损坏的block，如有，则通知datanode删除改block。</p>

<h3>NavigableMap&lt;String, DatanodeDescriptor>  datanodeMap</h3>

<p>datanodeMap用于维护datanode->block的映射</p>

<h3>ArrayList<DatanodeDescriptor> heartbeats</h3>

<p>维护多有当前活着的节点</p>

<h3>UnderReplicatedBlocks neededReplications</h3>

<p>通过一个优先级队列来维护当前需要备份的block集合，副本数越少的block优先级越高，0为最高级，表示当前只有一个副本。</p>

<h3>PendingReplicationBlocks pendingReplications;</h3>

<p>维护当前正在备份的block集合，并且进行备份请求的时间统计，并通过一个后台线程（PendingReplicationMonitor）来周期性（默认为5分钟）的统计超时的备份请求，当发生超时时，会将这个block重新添加到neededReplications列表中。
<img src="/images/hdfs/PendingReplicationBlocks.png" alt="PendingReplicationBlocks" /></p>

<h3>LightWeightLinkedSet<Block> overReplicatedBlocks</h3>

<p>当前需要检查是否备份过多的block集合</p>

<h3>Map&lt;String, Collection<Block>> excessReplicateMap</h3>

<p>维护系统中datanode与其上的超额备份block的集合，这些超额的备份将被删除。</p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/blog/page/4/">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/blog/page/2/">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section class="first odd">
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2012/10/18/hdfs-dataxceiver/">HDFS源码学习（15）——DataXceiverServer</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/10/18/hdfs-client-code/">HDFS源码学习（14）——Client代码结构</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/10/18/hdfs-datanode-startup/">HDFS源码学习（13）——DataNode启动过程</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/10/18/hdfs-datanode-structure/">HDFS源码学习（12）——DataNode主要数据结构</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/10/18/hdfs-secondary-namenode/">HDFS源码学习（11）——SecondaryNameNode</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/10/18/hdfs-namenode-and-datanode-communication/">HDFS源码学习（10）——NameNode与DataNode间的通信</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/10/18/hdfs-namenode-safe-mode/">HDFS源码学习（9）——安全模式（SafeMode）</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/10/18/hdfs-namenode-backup-mode/">HDFS源码学习（8）——Backup Mode</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/10/18/hdfs-namenode-block-management/">HDFS源码学习（7）——Block管理</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/10/18/hdfs-namenode-lease-management/">HDFS源码学习（6）——租约管理（lease management)</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/jiang-bo">@jiang-bo</a> on GitHub
  
  <script type="text/javascript">
    $.domReady(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'jiang-bo',
            count: 0,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>





  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2012 - jiang-bo -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a> and <a href="https://github.com/gluttony/object-octopress-theme">Object</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'jiangbo';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>











</body>
</html>
