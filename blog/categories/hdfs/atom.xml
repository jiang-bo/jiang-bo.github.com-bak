<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: HDFS | 非纯种程序猿]]></title>
  <link href="http://jiangbo.me/blog/categories/hdfs/atom.xml" rel="self"/>
  <link href="http://jiangbo.me/"/>
  <updated>2012-12-10T16:15:22+08:00</updated>
  <id>http://jiangbo.me/</id>
  <author>
    <name><![CDATA[jiang-bo]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[HDFS-2246:使用BlockReaderLocal优化本地block读取]]></title>
    <link href="http://jiangbo.me/blog/2012/12/10/hdfs-blockreaderlocal/"/>
    <updated>2012-12-10T16:09:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/12/10/hdfs-blockreaderlocal</id>
    <content type="html"><![CDATA[<h2>一、背景</h2>

<p>HDFS中对Block的读取使用DataXceiver通过Socket发送Packet数据进行，client端通过一个BlockReader来接收Socket中的Block数据。详见<a href="">HDFS数据读取流程</a>。大致流程如下：</p>

<ol>
<li>client端调用FileSystem.open()获取一个DFSInputStream</li>
<li>client端调用DFSInputStream.read(byte[] buffer, int off, int len)读取数据</li>
<li>client端通过DFSInputStream.blockSeekTo()向NameNode发起请求，定位到需要读取的block所在的Datanode，构建一个BlockReader用于读去对应DataNode上的block数据，返回该DataNodeInfo</li>
<li>BlockReader主要负责同DataNode间建立一个Socket链接用于读取数据，BlockReader首先向DataNode发送请求头（包括操作类型，blockId，block时间戳，起始偏移量，读取数据的长度，client名）</li>
<li>DataNode接收到DataXceiverServer接收到client端请求后，构建一个DataXceiver处理该请求。</li>
<li>DataXceiver首先解析请求头，获取请求操作类型，当发现是READ_BLOCK操作后，调用相应的readBlock()方法处理</li>
<li>readBlock方法解析请求中需要的blockId，构建一个BlockSender用于读取磁盘上的block文件数据并发送给client</li>
<li>BlockSender读取磁盘上的block文件，将数据按照chunk通过socket发送给client的blockReader，同时，BlockSender在发送chunk后需要从meta文件中读取该chunk的checksum数据，同样发送给client，用于该chunk的checksum校验。</li>
<li>client端通过BlockReader.readChunks()接收BlockSender发送的chunk数据，并进行checksum校验，校验成功后向DataNode发送checksumOk。</li>
<li>循环6-10，直至当前block的数据全部被读取完成。</li>
<li>循环执行3-11, 直至需要读取的文件数据都被读取完。</li>
</ol>


<p>这个过程是在集群环境想，client读取datanode上数据的一个正常流程，但事实上当client和datanode位于同一个物理节点上时（如Hadoop集群中，task运行在datanode上），这个过程显的有些多余，client可以直接通过本地文件系统api读取文件，而不需要走繁杂的socket流程。</p>

<h2>二、设计实现</h2>

<p>HDFS-2246中提供了一个BlockReaderLocal的实现，当client发现从NameNode返回的Block所属的datanode和client位于同一节点上时，构建一个BlockReaderLocal用于读取本地文件。
上述3-10的流程将简化为：</p>

<ol>
<li>client端向NameNode发起请求获取block所属的datanode信息后，判断该datanode是否和client位于同一节点，是且开启了本地读取功能，则构建一个BlockReaderLocal读取本地文件，否则构建一个BlockReader按照原流程进行。</li>
<li>BlockReaderLocal通过DataNode.getBlockLocaPathInfo()从DataNode获取block的本地文件路径信息。</li>
<li>BlockReaderLocal构建InputStream读取block文件和meta文件信息</li>
<li>对于需要checksum的场景（默认），通过blockReaderLocal.readChunks()按chunk读取本地文件，同时读取meta文件中该chunk的checksum数据，进行校验</li>
<li>对于跳过checksum的场景，直接通过InputStream.read()读取block数据。</li>
</ol>


<h3>扩展DataNode协议接口</h3>

<p>client端需要能够从DataNode获取block文件的本地文件路径信息。因此扩展ClientDataNodeProtocol，增加一个</p>

<pre><code>BlockLocalPathInfo getBlockLocalPathInfo(Block block) throws IOException;
</code></pre>

<p>接口用于获取block的本地路径信息</p>

<h3>本地文件读取</h3>

<p>BlockReaderLocal共过BufferedInputStream直接读取本地文件，注意此处HDFS-2246的patch中使用的是FileInputStream，实际测试过程中发现，FileInputStream对本地文件的读取性能较差， 替换为使用BufferedInputStream</p>

<h3>checksum较验</h3>

<p>为了完成checksum校验，BlockReaderLocal同时需要读取block的meta文件，每当block文件读取一个chunk时需要从meta文件读取一个checksum数据，进行checksum校验，通过校验后进行下一个chunk的读取和校验。由于BlockReaderLocal读取的是本地文件，避免的网络传输对数据的影响，因此可以配置跳过checksum检查，以提高读取性能。默认是需要做checksum的。</p>

<h3>本机判断</h3>

<p>当前patch中的实现主要用IP来判断是否block所在的datanode与client是否位于同一节点上。</p>

<h2>测试</h2>

<p>通过TestDFSIO工具测试一个单节点的集群，2个文件，每个文件1000M
./bin/hadoop jar hadoop-0.19.1-dc-test.jar TestDFSIO -read -nrFiles 2 -fileSize 1000
测试结果对比:
socket读取：</p>

<pre><code>---
12/12/07 13:52:57 INFO mapred.FileInputFormat: ----- TestDFSIO ----- : read
12/12/07 13:52:57 INFO mapred.FileInputFormat:            Date &amp; time: Fri Dec 07 13:52:57 CST 2012
12/12/07 13:52:57 INFO mapred.FileInputFormat:        Number of files: 2
12/12/07 13:52:57 INFO mapred.FileInputFormat: Total MBytes processed: 2000
12/12/07 13:52:57 INFO mapred.FileInputFormat:      Throughput mb/sec: 283.5270768358378
12/12/07 13:52:57 INFO mapred.FileInputFormat: Average IO rate mb/sec: 283.5281982421875
12/12/07 13:52:57 INFO mapred.FileInputFormat:  IO rate std deviation: 0.5685961122141402
</code></pre>

<p>本地读取</p>

<pre><code>---
12/12/07 13:48:59 INFO mapred.FileInputFormat: ----- TestDFSIO ----- : read
12/12/07 13:48:59 INFO mapred.FileInputFormat:            Date &amp; time: Fri Dec 07 13:48:59 CST 2012
12/12/07 13:48:59 INFO mapred.FileInputFormat:        Number of files: 2
12/12/07 13:48:59 INFO mapred.FileInputFormat: Total MBytes processed: 2000
12/12/07 13:48:59 INFO mapred.FileInputFormat:      Throughput mb/sec: 369.61744594344856
12/12/07 13:48:59 INFO mapred.FileInputFormat: Average IO rate mb/sec: 369.6180725097656
12/12/07 13:48:59 INFO mapred.FileInputFormat:  IO rate std deviation: 0.4800772
</code></pre>

<p>另外通过Patch中提供的TestShortCircuitLocalRead工具，测试结果如下：</p>

<p>本地读取并进行checksum校验</p>

<pre><code>true no 1 32000000
---
Iteration 20 took 115453
Iteration 20 took 115803
Iteration 20 took 115748
</code></pre>

<p>socket读取并进行checksum校验（默认）</p>

<pre><code>no no 1 32000000
---
Iteration 20 took 128820
Iteration 20 took 135305
Iteration 20 took 129145
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用FUSE-DFS mount HDFS]]></title>
    <link href="http://jiangbo.me/blog/2012/10/23/mount-hdfs-with-fuse-dfs/"/>
    <updated>2012-10-23T10:24:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/23/mount-hdfs-with-fuse-dfs</id>
    <content type="html"><![CDATA[<h2>介绍</h2>

<p>Hadooop源码中自带了contrib/fuse-dfs模块，用于实现通过libhdfs和fuse将HDFS mount到*inux的本地。</p>

<h2>编译</h2>

<h3>环境</h3>

<ol>
<li>Linux: 2.6.18-164.el5 x86_64</li>
<li>JDK: 1.6.0_23 64bit</li>
<li>Hadoop: 0.19.1 下面假设源码目录为$HADOOP_SRC_HOME</li>
<li>Ant: 1.8.4</li>
<li>GCC: 4.1.2(系统默认)</li>
</ol>


<h3>编译libhdfs</h3>

<h4>修改configure执行权限</h4>

<pre><code>$chmod +x $HADOOP_SRC_HOME/src/c++/pipes/configure
$chmod +x $HADOOP_SRC_HOME/src/c++/utils/configure
</code></pre>

<h4>修改Makefile，调整编译模式</h4>

<p>64位机中，需要修改libhdfs的Makefile，将GCC编译的输出模式由32(-m32)位改为64(-m64)位</p>

<pre><code>CC = gcc
LD = gcc
CFLAGS =  -g -Wall -O2 -fPIC
LDFLAGS = -L$(JAVA_HOME)/jre/lib/$(OS_ARCH)/server -ljvm -shared -m64(这里) -Wl,-x
PLATFORM = $(shell echo $$OS_NAME | tr [A-Z] [a-z])
CPPFLAGS = -m64(还有这里) -I$(JAVA_HOME)/include -I$(JAVA_HOME)/include/$(PLATFORM)
</code></pre>

<h4>编译</h4>

<p>在$HADOOP_HOME目录下执行</p>

<pre><code>$ ant compile -Dcompile.c++=true -Dlibhdfs=true
</code></pre>

<p>编译结果将生成libhdfs库，位于$HADOOP_SRC_HOME/build/libhdfs目录下</p>

<h3>编译fuse-dfs</h3>

<h4>安装fuse库</h4>

<p>fuse-dfs依赖fuse库，可通过</p>

<pre><code>sudo lsmod|grep fuse
</code></pre>

<p>检查是否已经安装，如没有，可通过：</p>

<pre><code>yum -y install fuse fuse-devel fuse-libs
</code></pre>

<p>安装相关依赖库。</p>

<h4>设置编译库路径</h4>

<p>设置编译库路径，将libhdfs的库加入到编译路径中</p>

<pre><code>export LD_LIBRARY_PATH=/usr/lib:/usr/local/lib:$HADOOP_SRC_HOME/build/c++/Linux-amd64-64/lib:$JAVA_HOME/jre/lib/amd64/server
</code></pre>

<h4>编译</h4>

<p>编译contrib/fuse-dfs模块：</p>

<pre><code>ant compile-contrib -Dlibhdfs=1 -Dfusedfs=1
</code></pre>

<p>编译完成将会生成$HADOOP_HOME/build/contrib/fuse-dfs/目录，内有：</p>

<pre><code>fuse-dfs]$ ls
fuse_dfs  fuse_dfs_wrapper.sh  test
</code></pre>

<p>其中fuse_dfs是可执行程序，fuse_dfs_wrapper.sh是包含一些环境变量设置的脚本，不过其中大部分需要修改:(</p>

<h4>修改fuse_dfs_warpper.sh</h4>

<pre><code>#Hadoop安装目录
export HADOOP_HOME=/home/bo.jiangb/yunti-trunk/build/hadoop-0.19.1-dc
#将fuse_dfs加入到PATH
export PATH=$HADOOP_HOME/contrib/fuse_dfs:$PATH
#将hadoop的jar加入到CLASSPATH
for f in ls $HADOOP_HOME/lib/*.jar $HADOOP_HOME/*.jar ; do
export  CLASSPATH=$CLASSPATH:$f
done
#设置机器模式
export OS_ARCH=amd64
#设置JAVA_HOME
export  JAVA_HOME=/home/admin/tools/jdk1.6
#将libhdfs加入到链接库路径中
export LD_LIBRARY_PATH=$JAVA_HOME/jre/lib/$OS_ARCH/server:/home/bo.jiangb/yunti-trunk/build/libhdfs:/usr/local/lib
./fuse_dfs $@
</code></pre>

<h2>使用</h2>

<h3>mount</h3>

<ol>
<li><p>新建一个空目录</p>

<p> $mkdir /tmp/dfs</p></li>
<li><p>挂载dfs
$./fuse_dfs_wrapper.sh dfs://master_node(namenode地址):port /tmp/dfs -d
-d表示debug模式，如果正常，可以将-d参数去掉。</p></li>
</ol>


<h3>unmount</h3>

<p>卸载可通过：</p>

<pre><code>fusermount -u /tmp/dfs
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS源码学习（15）——DataXceiverServer]]></title>
    <link href="http://jiangbo.me/blog/2012/10/18/hdfs-dataxceiver/"/>
    <updated>2012-10-18T22:00:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/18/hdfs-dataxceiver</id>
    <content type="html"><![CDATA[<p>HDFS中有两种类型的通信机制，一种是进行消息传递的Hadoop IPC机制，一种是用于处理数据传输的DataXceiver机制。前者包括client<->namenode之间的通信，以及datanode<->namenode间通信，后者包括client<->datanode, datanode<->datanode间的数据传输。</p>

<h2>DataXceiverServer</h2>

<p>DataNode在启动时会通过DataXceiverServer开启一个Socket端口，负责block数据的读写。DataXceiverServer本身作为一个守护线程，监听dfs.datanode.address配置的数据读写服务端口。当有请求来时，新建一个DataXceiver线程处理请求。</p>

<h2>DataXceiver</h2>

<p>DataXceiver线程用于处理一个读/写数据流请求，其run方法入下主要是根据请求中不同的请求类型，调用响应的处理方法。</p>

<p>请求操作类型定义在DataTransferProtocol中，主要有：</p>

<ol>
<li>OP_WRITE_BLOCK： 写入Block数据，对应writeBlock()方法</li>
<li>OP_READ_BLOCK： 读取Block数据，对应readBlock()方法</li>
<li>OP_READ_METADATA： 读取Block元数据，对应readMetadata()方法</li>
<li>OP_REPLACE_BLOCK： 替换Block，将block发送到目标datanode上，用于IO负载均衡；对应replaceBlock()方法。</li>
<li>OP_COPY_BLOCK：复制Block，将block发送到proxy source上，用于IO负载均衡；对应copyBlock()方法。</li>
<li>OP_BLOCK_CHECKSUM：获取Block的checksum；对应getBlockChecksum()方法。</li>
</ol>


<p>请处理返回的状态也定义在该类中：</p>

<ol>
<li>OP_STATUS_SUCCESS： 成功</li>
<li>OP_STATUS_ERROR： 请求出错</li>
<li>OP_STATUS_ERROR_CHECKSUM： checksum校验出错</li>
<li>OP_STATUS_ERROR_INVALID： 读取无效block</li>
<li>OP_STATUS_ERROR_EXISTS：block不存在</li>
<li>OP_STATUS_CHECKSUM_OK： checksum校验正常</li>
</ol>


<h3>1.读取block——readBlock()</h3>

<p>OP_READ_BLOCK的请求数据格式如下：</p>

<p><img src="/images/hdfs/ReadBlock.png" alt="READ_BLOCK" /></p>

<p>返回数据格式如下：</p>

<p><img src="/images/hdfs/ReadResponse.png" alt="READ_Response" /></p>

<p>readBlock()主要从disk读取block数据，构建一个DataOutputStream数据流，并新建一个BlockSender将这个数据流发送出去（datanode或者client）。</p>

<p>BlockSender.sendBlock()发送的Block的流程大体如下：</p>

<ol>
<li>读取block的meta信息，获得checksum并发送</li>
<li>发送数据读取的偏移量</li>
<li>将block数据切分为packet，发送给client</li>
<li>所有packet发送完之后，关闭checksum文件和block文件</li>
</ol>


<h3>2.写入block——writeBlock()</h3>

<p>OP_WRITE_BLOCK的请求数据格式如下：</p>

<p><img src="/images/hdfs/WriteRequest.png" alt="WRITE_BLOCK" /></p>

<p>writeBlock()解析请求信息，构建一个BlockReceiver处理数据接收和写入，在client（或上一datanode节点）-当前datanode节点-下一datanode节点之间建立一个如下连接。</p>

<p><img src="/images/hdfs/WriteBlock.png" alt="WRITE_BLOCK" /></p>

<ol>
<li>BlockReceiver从上按packet一节点读取数据，写入到本地disk</li>
<li>如有下一备份节点，将该packet转发给下一节点</li>
<li>将该packet加入到ackqueue队列中等待ack消息</li>
<li>当下一节点完成该packet写入后会返回该packet对应的ack信息</li>
<li>PakcetResponder接收到ack信息后，将ackqueue中该packet删除，并向前置节点发送ack信息</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS源码学习（14）——Client代码结构]]></title>
    <link href="http://jiangbo.me/blog/2012/10/18/hdfs-client-code/"/>
    <updated>2012-10-18T21:59:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/18/hdfs-client-code</id>
    <content type="html"><![CDATA[<p>Client核心代码有DistributedFileSystem和DFSClient。</p>

<p><img src="/images/hdfs/Client.png" alt="Client" /></p>

<p>DistributedFileSystem扩展子FileSystem，在为客户端提供一个文件系统接口实现。其内部使用DFSClient完成各类文件操作。</p>

<p>DFSClient使用ClientProtocol与NameNode通信，完成文件元信息操作。并通过Socket连接完成与DataNode间的block读写操作。</p>

<p>DFSClient代码结构如下：</p>

<p><img src="/images/hdfs/DFSClient.png" alt="DFSClient" /></p>

<ol>
<li>LeaseChecker主要用于lease检查和续约。</li>
<li>DFSOutputStream用于提供带buffer的字节流写入功能。client在写入数据时先将数据缓存在本地。并将数据切分成多个packet（默认每个packet为64K）。每个packet又被拆分成多个chunk（默认512Byte），每个chunk都有一个checksum。client写满一个packet后会将该packet加入到一个dataqueue中。由DataStreamer线程负责将每个packet发送给datanode pipeline。发送完一个pakcet，streamer会将其从dataqueue移至ackqueue中。ResponseProcessor负责接收datanode发回的ack信息，每成功接收一个packet的ack信息，ResponseProcessor会将ackqueue中该packet删除。</li>
<li>DFSInputStream用于提供字节流的读取，其内部封装了与NN和DN的交互</li>
<li>DataStreamer: 负责向datanode pipeline发送packet。其本身是一个Daemon线程，从namenode获取blockId和block存放位置，将packet发送给pipeline中的datanode，每个packet都有一个seqId，每个packet发送完时都会收到datanode的ack信息。当收到所有packet的ack信息后（表示该block已发送完），streamer关闭该block。</li>
<li>ResponseProcessor:用于接收datanode返回ack信息，并将响应ackqueue中的packet删除</li>
</ol>


<h2>创建文件</h2>

<ol>
<li>client向NameNode发起创建文件请求</li>
<li>NameNode.create（）处理创建文件请求，检查是否有重名，当前是否处于Safe-mode，是否有权限创建文件， 校验通过后创建一个INode记录。</li>
<li>NameNode将创建文件的事件记录到EditLog中</li>
<li>INode被创建后，NameNode发放给Client一个lease，Client可以使用这个lease通过ClientProtocol访问，进行只读操作。（写操作需要等文件close）</li>
</ol>


<h2>写入流程</h2>

<p>client写入流程如下图所示：</p>

<p><img src="/images/hdfs/ClientWrite.png" alt="ClientWrite" /></p>

<ol>
<li>Client向NameNode发起创建文件的RPC请求</li>
<li>NameNode检查文件是否已经存在，是否有权创建等，成功则创建一个文件记录，并发放给Client一个lease</li>
<li>Client获得lease之后开始进行数据写入，写入的数据首先被缓存本地，并被拆分为多个packet，放置到dataqueue队列中</li>
<li>DataStreamer线程负责检查dataqueue队列，发现有数据时且没有可用block时，向NameNode发送addBlock()请求，申请一个分配一个block空间。NameNode返回给DataStreamer一个blockId和用于存放block的datanode list</li>
<li>DataStreamer将每个packet数据发送给datanode pipeline，并将该packet移至ackqueue</li>
<li>datanode pipeline中第一个datanode收到packet之后存储到本地block中并穿行备份至后续datanode中</li>
<li>pipeline中datanode存储好packet之后会逆序返回ack信息，并最终返回给client.</li>
<li>Client端ResponseProcessor捕获到每个packet的ack信息时会将响应ackqueue中的packet删除</li>
<li>当所有数据都写入完成后，client会向NameNode发起一个complete RPC请求，告知文件最新的时间戳和已经发送给datanode的block长度。NameNode检查所有block的副本信息，只有所有block的副本数均满足最低要求时，complete会返回成功。</li>
<li>最后，NameNode将收回client持有的lease。</li>
</ol>


<p>NameNode处理addBlock()请求的流程大致如下：</p>

<ol>
<li>校验client是否有该文件的lease</li>
<li>清理上一次写入记录，包括：a.提交上一次写入，b.更新lease有效期；c.将完成的写入记录到EditLog中</li>
<li>清理完毕之后，使用BlockManager分配指定副本数个block及其对应的datanode信息，返回给client</li>
</ol>


<p>DataNode处理block写入的流程大致如下：</p>

<ol>
<li>将block复制到本地磁盘</li>
<li>发送block received消息给NameNode告知写入了一个新的block</li>
<li>将block数据发送给datanode pipeline中下一个datanode，进行备份</li>
<li>返回一个ack消息给前一个调用者</li>
</ol>


<p>后续的datanode收到上一个datanode的备份block请求是做类似的操作。</p>

<h2>读取流程</h2>

<p>读取流程相对简单写，如下所示 ：</p>

<p><img src="/images/hdfs/ClientRead.png" alt="ClientWrite" /></p>

<ol>
<li>client向NameNode发起RPC请求，获取文件的blockLocation信息</li>
<li>NameNode返回一定长度（10*defaultBlockSize）block的datanode位置信息</li>
<li>Client根据返回的blockLocation信息选取距自己最近（同一节点&lt;通一机架&lt;同一机房）的datanode读取数据，读完一个block会对该block进行checksum校验。如果校验正确则关闭与该datanode连接，去读下一个block；如果校验失败，则通知NameNode该block在当前datanode上的副本损坏了，并继续从datanode列表中获取一个datanode，重新读取该block。</li>
<li>当本次获取的blockLocation中的block全部读完，且该文件还有block时，重复1，2，3过程，直至所有blcok全部读完。</li>
</ol>


<h2>关闭文件（complete）</h2>

<ol>
<li>当Client完成文件写入之后，会调用complete()通知NameNode文件写入完成了，该请求会提交文件写入的最后一个block信息并且告知NameNode写入的block总数以及最新时间戳。</li>
<li>NameNode收到请求后会检查是否所有的block的事物都已经提交了，并且每个block的副本数都达到了最小值。如果是则返回true，否则返回false。</li>
<li>Client收到返回值后如果失败则重试几次。</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS源码学习（13）——DataNode启动过程]]></title>
    <link href="http://jiangbo.me/blog/2012/10/18/hdfs-datanode-startup/"/>
    <updated>2012-10-18T21:57:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/18/hdfs-datanode-startup</id>
    <content type="html"><![CDATA[<h2>main()</h2>

<pre><code>  public static void main(String args[]) {
    try {
      StringUtils.startupShutdownMessage(DataNode.class, args, LOG);
      DataNode datanode = createDataNode(args, null);
      if (datanode != null)
        datanode.join();
    } catch (Throwable e) {
      LOG.error(StringUtils.stringifyException(e));
      System.exit(-1);
    }
  }
</code></pre>

<h2>createDataNode()</h2>

<pre><code>  public static DataNode createDataNode(String args[],
                                 Configuration conf) throws IOException {
    // 初始化datanode
    DataNode dn = instantiateDataNode(args, conf);
    // 启动datanode后台线程
    runDatanodeDaemon(dn);
    return dn;
  }
</code></pre>

<h3>1.instantiateDataNode（）</h3>

<pre><code>  public static DataNode instantiateDataNode(String args[],
                                      Configuration conf) throws IOException {
    // 处理配置
    if (conf == null)
      conf = new Configuration();
    if (!parseArguments(args, conf)) {
      printUsage();
      return null;
    }
    if (conf.get("dfs.network.script") != null) {
      LOG.error("This configuration for rack identification is not supported" +
          " anymore. RackID resolution is handled by the NameNode.");
      System.exit(-1);
    }
    // 获取data目录配置
    String[] dataDirs = conf.getStrings("dfs.data.dir");
    dnThreadName = "DataNode: [" +
                        StringUtils.arrayToString(dataDirs) + "]";
    //创建datanode实例
    return makeInstance(dataDirs, conf);
  }
</code></pre>

<h4>1.1. makeInfstance()</h4>

<p>该方法主要用于检查给定的data目录中至少有一个可以创建，并实例化DataNode</p>

<pre><code>  public static DataNode makeInstance(String[] dataDirs, Configuration conf)
    throws IOException {
    ArrayList&lt;File&gt; dirs = new ArrayList&lt;File&gt;();
    for (int i = 0; i &lt; dataDirs.length; i++) {
      File data = new File(dataDirs[i]);
      try {
        DiskChecker.checkDir(data);
        dirs.add(data);
      } catch(DiskErrorException e) {
        LOG.warn("Invalid directory in dfs.data.dir: " + e.getMessage());
      }
    }
    if (dirs.size() &gt; 0) 
      return new DataNode(conf, dirs);
    LOG.error("All directories in dfs.data.dir are invalid.");
    return null;
  }
</code></pre>

<h4>1.2 new DataNode()</h4>

<pre><code>  DataNode(Configuration conf, 
           AbstractList&lt;File&gt; dataDirs) throws IOException {
    // 设置配置信息
    super(conf);
    datanodeObject = this;
    supportAppends = conf.getBoolean("dfs.support.append", false);
    this.conf = conf;
    try {
      // 启动DataNode
      startDataNode(conf, dataDirs);
    } catch (IOException ie) {
      shutdown();
      throw ie;
    }
  }
</code></pre>

<h5>1.2.1 startDataNode()</h5>

<p>代码较长，仅列出主要步骤：</p>

<ol>
<li>设置配置信息</li>
<li>向NameNode发起RPC请求，获取版本和StorageID信息</li>
<li>获取启动配置</li>
<li>初始化存储信息，构建FSDataSet</li>
<li>获取可用的端口号</li>
<li>调整注册信息中的机器名，加上端口号</li>
<li>初始化DataXceiverServer</li>
<li>设置blockReport和heartbeat各自的时间间隔</li>
<li>初始化blockScanner</li>
<li>初始胡并启动servlet info server，提供内容查询的http服务</li>
<li>初始化ipc server，该ipc server主要用于完成DataNode间的block recover。</li>
</ol>


<h3>runDatanodeDaemon()</h3>

<pre><code>  public static void runDatanodeDaemon(DataNode dn) throws IOException {
    if (dn != null) {
      //register datanode
      dn.register();
      dn.dataNodeThread = new Thread(dn, dnThreadName);
      dn.dataNodeThread.setDaemon(true); // needed for JUnit testing
      dn.dataNodeThread.start();
    }
  }
</code></pre>

<h4>2.1 向NameNode注册 —— dn.register();</h4>

<pre><code>  private void register() throws IOException {
    if (dnRegistration.getStorageID().equals("")) {
      setNewStorageID(dnRegistration);
    }
    while(shouldRun) {
      try {
        // reset name to machineName. Mainly for web interface.
        dnRegistration.name = machineName + ":" + dnRegistration.getPort();
        // 通过NameProtocal向NameNode注册
        dnRegistration = namenode.register(dnRegistration);
        break;
      } catch(SocketTimeoutException e) {  // namenode is busy
        LOG.info("Problem connecting to server: " + getNameNodeAddr());
        try {
          Thread.sleep(1000);
        } catch (InterruptedException ie) {}
      }
    }
    assert ("".equals(storage.getStorageID()) 
            &amp;&amp; !"".equals(dnRegistration.getStorageID()))
            || storage.getStorageID().equals(dnRegistration.getStorageID()) :
            "New storageID can be assigned only if data-node is not formatted";
    if (storage.getStorageID().equals("")) {
      storage.setStorageID(dnRegistration.getStorageID());
      storage.writeAll();
      LOG.info("New storage id " + dnRegistration.getStorageID()
          + " is assigned to data-node " + dnRegistration.getName());
    }
    if(! storage.getStorageID().equals(dnRegistration.getStorageID())) {
      throw new IOException("Inconsistent storage IDs. Name-node returned "
          + dnRegistration.getStorageID() 
          + ". Expecting " + storage.getStorageID());
    }

    if (supportAppends) {
      Block[] bbwReport = data.getBlocksBeingWrittenReport();
      long[] blocksBeingWritten = BlockListAsLongs.convertToArrayLongs(bbwReport);
      //如果支持append，则报告正在写入的block信息
      namenode.blocksBeingWrittenReport(dnRegistration, blocksBeingWritten);
    }
    // 调整下一次的BR时间，使其在下次heartbeat时进行
    scheduleBlockReport(initialBlockReportDelay);
  }
</code></pre>

<h4>2.2 启动datanode线程 —— dn.dataNodeThread.start();</h4>

<p>datanode线程本身非常简单，不停调用offerSevice提供服务：</p>

<pre><code>  public void run() {
    LOG.info(dnRegistration + "In DataNode.run, data = " + data);

    // start dataXceiveServer
    dataXceiverServer.start();
    new Thread(new CrashVolumeChecker()).start();//added by wukong

    while (shouldRun) {
      try {
        startDistributedUpgradeIfNeeded();
        offerService();
      } catch (Exception ex) {
        LOG.error("Exception: " + StringUtils.stringifyException(ex));
        if (shouldRun) {
          try {
            Thread.sleep(5000);
          } catch (InterruptedException ie) {
          }
        }
      }
    }

    LOG.info(dnRegistration + ":Finishing DataNode in: "+data);
    shutdown();
  }
</code></pre>

<h5>2.2.1 offerService()</h5>

<p>offerService的核心是周期性进行heartbeat和blockReport，主要流程如下：</p>

<p><img src="/images/hdfs/offerService.png" alt="offerService" /></p>

<pre><code>  public void offerService() throws Exception {

    LOG.info("using BLOCKREPORT_INTERVAL of " + blockReportInterval + "msec" + 
       " Initial delay: " + initialBlockReportDelay + "msec");
    LOG.info("using DELETEREPORT_INTERVAL of " + deletedReportInterval + "msec");
    LOG.info("using HEARTBEAT_INTERVAL of " + heartBeatInterval + "msec");
    LOG.info("using HEARTBEAT_EXPIRE_INTERVAL of " + heartbeatExpireInterval + "msec");

    //
    // Now loop for a long time....
    //

    while (shouldRun) {
      try {
        long startTime = now();

        //
        // Every so often, send heartbeat or block-report
        //

        if (startTime - lastHeartbeat &gt; heartBeatInterval /* 3 secs*/) {
          //
          // All heartbeat messages include following info:
          // -- Datanode name
          // -- data transfer port
          // -- Total capacity
          // -- Bytes remaining
          //
          lastHeartbeat = startTime;
          DatanodeCommand[] cmds = namenode.sendHeartbeat(dnRegistration,
                                                       data.getCapacity(),
                                                       data.getDfsUsed(),
                                                       data.getRemaining(),
                                                       xmitsInProgress.get(),
                                                       getXceiverCount());
          myMetrics.heartbeats.inc(now() - startTime);
          //LOG.info("Just sent heartbeat, with name " + localName);
          if (!processCommand(cmds))
            continue;
        }

        reportReceivedBlocks();

        DatanodeCommand cmd = blockReport();
        processCommand(cmd);

        // start block scanner
        if (blockScanner != null &amp;&amp; blockScannerThread == null &amp;&amp;
            upgradeManager.isUpgradeCompleted()) {
          LOG.info("Starting Periodic block scanner.");
          blockScannerThread = new Daemon(blockScanner);
          blockScannerThread.start();
        }

        //
        // There is no work to do;  sleep until hearbeat timer elapses, 
        // or work arrives, and then iterate again.
        //
        long waitTime = heartBeatInterval - (System.currentTimeMillis() - lastHeartbeat);
        synchronized(receivedAndDeletedBlockList) {
          if (waitTime &gt; 0 &amp;&amp; receivedAndDeletedBlockList.size() == 0) {
            try {
              receivedAndDeletedBlockList.wait(waitTime);
            } catch (InterruptedException ie) {
            }
            delayBeforeBlockReceived();
          }
        } // synchronized

      } catch(RemoteException re) {
        String reClass = re.getClassName();
        if (UnregisteredDatanodeException.class.getName().equals(reClass) ||
            DisallowedDatanodeException.class.getName().equals(reClass) ||
            IncorrectVersionException.class.getName().equals(reClass)) {
          LOG.warn("DataNode is shutting down: " + 
                   StringUtils.stringifyException(re));
          shutdown();
          return;
        }
        LOG.warn(StringUtils.stringifyException(re));
      } catch (IOException e) {
        LOG.warn(StringUtils.stringifyException(e));
      }
    } // while (shouldRun)
  } // offerService
</code></pre>
]]></content>
  </entry>
  
</feed>
