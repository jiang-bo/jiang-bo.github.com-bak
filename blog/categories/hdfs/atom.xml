<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: HDFS | 非纯种程序猿]]></title>
  <link href="http://jiangbo.me/blog/categories/hdfs/atom.xml" rel="self"/>
  <link href="http://jiangbo.me/"/>
  <updated>2012-12-21T17:38:41+08:00</updated>
  <id>http://jiangbo.me/</id>
  <author>
    <name><![CDATA[jiang-bo]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[HDFS RAID]]></title>
    <link href="http://jiangbo.me/blog/2012/12/21/hdfs-raid/"/>
    <updated>2012-12-21T11:19:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/12/21/hdfs-raid</id>
    <content type="html"><![CDATA[<h2>一、背景</h2>

<p>HDFS是构建在普通机器上的分布式文件系统，而这类系统需要解决的一个首要问题就是容错，允许部分节点失效。而为了解决数据的可靠性，HDFS采用了副本策略。默认会为所有的block存放三个副本（具体参见HDFS设计文档）。
副本机制能够有效解决部分节点失效导致数据丢失的问题，但对于大规模的HDFS集群，副本机制会带来大量的存储资源消耗。例如为了存储1PB的数据，默认需要保留3个副本，这意味着实际存储所有副本需要至少3PB的空间。存储空间浪费达到200%。减小浪费的方式主要是减少副本数，而当副本数降低到小于3时，数据丢失的风险会非常高。而HDFS RAID的出现主要是解决降低副本数之后，通过RAID机制中的Erasured Code来确保数据的可用性。</p>

<script async class="speakerdeck-embed" data-id="3f1abed02d630130814722000a9d03e5" data-ratio="1.2994923857868" src="http://jiangbo.me//speakerdeck.com/assets/embed.js"></script>


<h2>二、整体结构</h2>

<p>HDFS RAID的实现（Facebook的实现）主要是在现有的HDFS之上增加了一个包装contrib。之所以不再HDFS上直接修改，原设计者的解释是“HDFS的核心代码已经够复杂了，不想让它更复杂”。</p>

<p><img src="/images/hdfs/Raid-Overview.png" alt="Overview" /></p>

<h3>2.1 使用的角度看HDFS RAID（Client端）</h3>

<p>HDFS RAID的使用场景主要有两个：raid数据管理和raid数据读取。</p>

<h4>2.1.1 Raid数据的管理</h4>

<p>对于DRFS的管理，包括DFS中那些文件需要进行raid化，查询raid文件的状态等，主要通过HDFS-RAID提供的RaidShell工具来完成。本质上RaidShell作为一个client工具，通过RPC与集群中的RaidNode通信，完成各种管理操作。</p>

<h4>2.1.2 Raid数据读写</h4>

<p>使用HDFS RAID的client端需要配置fs.hdfs.impl为DistributedRaidFileSytem，DRFS包装了DFS的读（只是读）请求，当block读取时发生block丢失（抛出MissingBlockException)或损坏(CorruptionException)时，DRFS会捕获这两个异常，并向RaidNode发送RPC对失效的数据进行恢复。</p>

<h3>2.2  RaidNode结构(Server端）</h3>

<p>RaidNode是HDFS-RAID中除NameNode和JobTracker之外的第三个master node，主要是接收client端的RPC请求和调度各守护线程完成数据的raid化和数据修复，parity文件删除等操作。</p>

<h4>2.2.1 两种实现</h4>

<p><strong>LocalRaidNode:</strong> 在RaidNode本地进行parity计算，parity文件的生成是一个计算密集型任务，而本地计算能力有限，因此该方式的扩展性有限。</p>

<p><strong>DistributedRaidNode:</strong> 通过提交mapreduce job来进行parity计算</p>

<h4>2.2.2 主要线程</h4>

<p><strong>TriggerMonitor:</strong> 周期性检查raid-policy配置，根据最新的配置来进行对相应的数据raid化。raid化的调度周期主要收两个配置的影响，raid.config.reload.interval （重新加载raid-policy配置的周期，默认10s）和raid.policy.rescan.interval（重新扫描需要raid化的src的间隔，默认1小时）。简单讲，当新增了一个policy时，默认10s内该policy会被加载执行。而在一个已经raid化的目录中新增了一个文件时，该文件将在1个小时内被raid话。</p>

<p><strong>BlockIntegrityMonitor:</strong> 负责通过DFS的fsck来对DRFS中已经raid化的数据进行检查，检查内容主要包括corrupt（损坏）和decomssion（丢失）的文件。一旦检测到这类文件的存在，BlocIntegrityMonitor会通过其维护的CorruptMonitor和DecomissionMonitor的两个线程来进行数据的修复。BlockIntegrityMonitor对应local和dist两种模式有两个实现，分别为LocalBlockIntegrityMonitor和DistBlockIntegrityMonitor。（可通过raid.blockfix.classname配置项设置，默认为dist）。区别主要在获取的corruptionMonitor和DecomissionMonitor的实现不同。</p>

<p><strong>LocalBlockIntegrityMonitor:</strong> 提供了CorruptMonitor实现会循环通过fsck检查corrupt文件，通过BlockReconstructor.CorruptBlockReconstructor重建这些文件。但该实现不提供Decomissioning文件的监控处理。local模式下corrput文件的重建是在RaidNode上进行的，对大量数据的重建，会对RaidNode有较大的压力。</p>

<p><strong>DistBlockIntegrityMonitor:</strong> Dist模式提供的CorruptionMonitor和DecomissionMonitor是通过DFSck获取corrupt和decomissed的文件列表，计算优先级后，通过向集群提交job来完成重建，Job的输入是一个包含所有文件path的sequence file，Mapper实现是通过Reconstructor来重建每个文件。</p>

<p><strong>BlockFixer(CorruptionMonitor):</strong> BlockIntegrityMonitor构建的用于修复corrupt文件的worker线程。</p>

<p><strong>BlockCopier(DecomissionMonitor):</strong> BlockIntegrityMonitor构建用于修复decomission文件的worker线程。</p>

<p><strong>PlacementMonitor:</strong> PlacementMonitor主要是通过blockMover完成为DRFS中的根据placement策略提供在Datanode之间move block的工具线程。BlockMover通过一个ClusterInfo线程周期性（默认1min）获取集群中live节点的最新topo结构。对于parity block过于集中的节点，需要将其分散开。分散的过程主要是：为每个的block构建一个BlockMoveAction线程，该线程在所有datanode中除当前block所在的节点外随机选取一个datanode，并选取一个proxysource datanode，proxysource datanode是用于将block复制到datanode的源节点，选取规则是优先选取当前block副本所在dn中与目标datanode所属同一rack的节点，如果没有，则从副本列表中随机选取一个作为源节点。</p>

<p><strong>PurgeThread:</strong> PurgeThread封装了PurgeMonitor，它会定期扫描Parity文件中是否有孤儿Parity文件(即拥有该Parity文件的source文件已经不存在了)，如果有则需要将其删除，如果没有，会对Parity文件和对应的source文件进行placement检查。</p>

<p><strong>HarThread:</strong> 为了减少RAID后Parity文件对Namenode的负担，HarThread封装了HarMonitor，它定期对超期的Parity文件进行归档处理(HAR)，超期时间由raid.parity.har.threshold.days指定，默认是3天。</p>

<h2>三、 raid和unraid流程详解</h2>

<h3>3.1 数据raid化</h3>

<p>文件数据的raid化有两种场景，一种是通过raidShell之行 raidFile命令触发</p>

<pre><code>hadoop raidshell -raidFile /path/to/file
</code></pre>

<p>另一种是TiggerMonitor线程周期行扫描policy，根据新的配置信息进行相应的raid化。</p>

<h4>3.1.1 raidShell执行raidFile</h4>

<p>当前client端执行raidfile请求时，大致的处理流程如下：</p>

<p><img src="/images/hdfs/Raid-RaidFile.png" alt="raidFile" /></p>

<ol>
<li>首先检查请求的delay时间，还未到delay时间则不执行</li>
<li>参数处理，包括path路径校验，codec设置等</li>
<li>查询path路径状态，如果是文件或者当前模式是local模式，则执行doLocalRaid，通过RaidNode.doRaid()对path下所有文件进行raid。</li>
<li>如果是目录且当前配置的raid模式是dist，则通过raidNode.submitRaid() rpc请求向RaidNode提交raid请求。</li>
<li>RaidNode接收到client提交的请求后，根据提交的额参数构造一个raid-policy，并添加到configMangaer中。等待RaidNode上TiggerMonitor守护线程下次运行是处理该policy。</li>
</ol>


<h4>3.1.2 triggerMonitor线程处理流程</h4>

<p>triggerMonitor作为RaidNode上的守护线程，周期性从configManager中获取policy列表，对每个policy进行如下处理：</p>

<ol>
<li>查询该policy的状态，如果未执行过，则立即处理，获取path中文件列表。如果该policy已经处理过，过滤其path中尚未处理的file。</li>
<li>如果是local模式，对列表中的file执行RaidNode.doRaid()</li>
<li>如果是dist模式，通过DistRaid构建一个raid job，该job的输入文件是所有待raid文件path构成的sequence file。mapper主要是调用RaidNode.doRaid()对输入中的file path进行raid。</li>
</ol>


<h5>RaidNode.doRaid()流程</h5>

<p>上述表明，hdfs raid中对文件的raid最终都是由RaidNode.doRaid()来完成，不通场景下的区别主要是raid过程的执行地点不同：</p>

<ol>
<li>raidshell执行的local模式或者单个文件，raid过程是在client上完成</li>
<li>local模式下tiggermonitor触发的raid， raid过程是在RaidNode上完成</li>
<li>raidshell执行的dist模式且是目录时进行的raid，或者dist模式下triggermonitor触发的raid，是通过job的方式提交到集群上由每个task节点完成。</li>
</ol>


<p>RaidNode.doRaid()的主要流程如下：</p>

<p><img src="/images/hdfs/Raid-RaidNodeDoRaid.png" alt="RaidNodeDoRaid" /></p>

<ol>
<li>获取文件的block信息，如果block数小于3，则不进行raid。</li>
<li>对于为打到delay时间的也不进行raid</li>
<li>如果已经到达delay时间且block数>2 时进行生成parity文件</li>
<li>生成parity文件过程如图右半部所示：首先获取src文件path，生成parity文件的path，parity文件path的生成规则是 $parity_dir+src_path（codec中配置的是parity_dir是/raid， src文件path是data/file1.log， 那么该文件的parity文件path就是/raid/data/file1.log）</li>
<li>检查相应的parity文件是否已经存在，如果存在，检查parity文件的mtime（更新时间）是否与源文件mtime一致，如果是，则认为该源文件已经raid且是最新。不需要再进行raid。</li>
<li>如果parity文件不存在或不是最新，则重新通过Encoder来生成parity文件</li>
<li>设置parity文件的mtime为源文件的mtime。</li>
<li>检查parity文件的最终状态，主要是mtime是否与源文件一致。通过则raid完成</li>
</ol>


<h4>3.1.3 Encoder.encode过程</h4>

<p>raid过程中最终的编码生成parity的工作有Encoder完成。编码过程主要如下：</p>

<p><img src="/images/hdfs/Raid-EncodeFile.png" alt="EncodeFile" /></p>

<ol>
<li>由于编码过程会比较长，所以先生成™p文件。™p文件的目录可以通过™p_parity_dir配置，默认是™p/$parity_dir</li>
<li>构建™p文件path，™p文件的path为™p目录下parity文件path加上一个随机long值构成，$™p_parity_dir/$parity_file+randomlong。</li>
<li>通过Erasued Code来进行编码到™p文件</li>
<li>删除原有的parity文件</li>
<li>将™p文件重命名为parity文件。</li>
<li>删除™p文件。</li>
</ol>


<p>对于Erasured Code的生成过程大至流程如下：
从源文件中block列表中选取一些（数量有stripe_length指定，默认是10）block，构成一个strip（条？）。通过ParallelStreamReader工具构建一个并行读取10个block的的数据，每个block每次读取1个buff的数据(buffer大小有raid.encoder.bufsize指定，默认是1m)，一次读取构成一个二维byte数组byte[stripe_length][buff_size],这个二维数组做为Erasure Code的输入数据，进行编码生成erasued code。输出也是一个byte二维数组byte[parity_length][buffer_size]。</p>

<p><strong>XOR算法中</strong>:parity_length为1， 即根据10位输入byte生成1位的奇偶校验码。</p>

<p><strong>RS算法中</strong>: parity_length默认为4， 及根据10为输入生成4为的RS code，这四位分别写入4个™p文件中，在一个buffer全部编码完成后，将4个parity文件进行合并。生成一个™p文件。</p>

<h3>3.2 损坏数据的恢复</h3>

<p>raid数据的修复同样也有多个触发场景：</p>

<ol>
<li>client端使用DRFS读取数据发生数据丢失或损坏延长</li>
<li>RaidNode上BlockIntegrityMonitor周期获取block数据发现数据异常时</li>
<li>通过raidshell执行 fixblock时</li>
</ol>


<h4>3.2.1 block读取时修复损坏数据流程</h4>

<p>在client通过DRFS读取raid话的数据是，DRFS首先通过其内部封装的DFS去读block数据，当DFS读取时跑出CorruptionException或DecomissionException时，会被DRFS捕获，并对出错的block在client进行修复。主要流程如下：</p>

<p><img src="" alt="" /></p>

<ol>
<li>在client配置了DRFS并使用DFS作为内置fs时，当通过FS.open获取文件InputStream时，返回一个ExtFSDataInputStream实例。</li>
<li>通过该inputStream读取数据时，首先通过内置DFS读取响应的block，正常情况下，返回需要的数据。</li>
<li>当内置的DFS读取block时跑出CorruptionException或DecomissionException时，会被ExtFSDataInputStream捕获。通过调用RaidNode.unRaidCorrputionBlock()来获取一个恢复的block，并从该block读取数据。</li>
</ol>


<p>RaidNode.unRaidCorruptionBlock()过程首先获取该block的parity文件信息，然后构建一个恢复文件的path路径(该路径位于hdfs.raid.local.recovery.location配置的目录下，默认是/tmp/raidrecovery，文件名为原文件名+"."+随机long+".recoveryd")，并通过Decoder.fixErasedBlock()来根据parity文件生成恢复文件。</p>

<p><strong>注意:</strong>对于恢复文件所在的文件系统是可以通过fs.raid.recoveryfs.uselocal来配置的，默认是false，即使用DFS，恢复文件将在储与分布式系统中，当配置成true是，使用LocalFileSystem，将恢复文件存储在client端本地。</p>

<h4>3.2.2 BlockIntegrityMonitor线程修复</h4>

<p>RaidNode上的BlockIntegrityMonitor线程会通过DFSck工具检查系统中corrupt或decomission的数据，通过BlockCopier和BlockFixer线程周期行对出错的数据进行修复。local模式下，修复过程在RaidNode上之行，Dist模式下修复过程通过提交Job的方式提交给集群完成。</p>

<p><strong>Local模式</strong>
LocalBlockIntegrity线程的核心是周期调用doFix方法修复corrupt文件，主要流程如下：</p>

<ol>
<li>通过DFSck获取currput文件信息（HTTP访问）</li>
<li>过滤掉不能恢复的corrupt文件（没有parity文件的）</li>
<li>将corrput的文件排序，排序规则如下

<ul>
<li>parity文件优先，source文件在后</li>
<li>parity文件中codec.priority高的在先（codec.priority通过JSON中coder_priority配置）</li>
</ul>
</li>
<li>对排序号的corrupt文件列表依次通过BlockRecontsturer来恢复。</li>
</ol>


<p><strong>Dist模式</strong>
DistBlockIntegrity中的有两个worker线程blockCopier和blockFixer，分别对应修复decomssion和corrput的文件。实际上两个线程的处理流程基本一致，大体如下：</p>

<ol>
<li>检查当前正在运行的修复job数，如果当前job已经大于job上限，则等待之前的job运行完（该上线可以通过raid.blockfix.maxpendingjobs来配置，默认是100L）</li>
<li>通过DFSck获取损坏的文件信息，blockfixer线程获取corrupt文件信息，blockCopier获取decomission文件信息</li>
<li><p>计算获得的损坏文件的优先级：</p>

<p> corrput文件的优先级如下(R为文件副本数，C为该文件corrput的block数)：</p>

<ul>
<li>默认为LOW</li>
<li>R>1 &amp;&amp; C>0时： HIGH</li>
<li>R==1 &amp;&amp; C>1时： HIGH</li>
<li>parityfile corrput &amp;&amp; C>0时： HIGH</li>
</ul>


<p> decomission优先级计算规则如下（D为decomission的block数):</p>

<ul>
<li>默认为LOW</li>
<li>D>4时： HIGH</li>
</ul>
</li>
<li><p>将计算好优先级的文件列表按优先级排序，作为参数构建修复Job。</p></li>
<li>Job的输入是所有需要修复的文件path的sequence file。会根据raid.blockfix.filespertask配置的值进行sync，即在job的split阶段会按照该值设置的进行split，默认是20</li>
<li>Job的Mapper主要是通过Reconstruter在task机上完成响应文件的恢复。</li>
</ol>


<p><strong>注意：</strong>对于修复Job还有一个参数限制，及每次job最多进行的task数，该值为固定值50，这意味着一个Job一次最多能修复的文件数是100个（raid.blockfix.filespertask*50）</p>

<h4>3.2.3 RaidShell之行fixblock</h4>

<p>通过raidshell执行 fixblock时, raidShell会通过BlockReconstructor来完成文件的修复。</p>

<h4>3.2.4 BlockReconstructor文件修复过程</h4>

<p>BlockIntegrityMonitor和RaidShell对文件的修复最终都通过BlockReconstructor来完成。
BlockReconstructor修复文件过程主要分为三类：Har parity文件，parity文件和源数据文件。</p>

<p><strong>Har parity文件</strong></p>

<ol>
<li>获取har文件的基本信息及index</li>
<li>获取har文件中的lost block，对每个block进行如下处理：</li>
<li>在本地文件系统创建该block的临时文件，</li>
<li>对该block涉及的所有parity文件，获取对应的source文件，通过Encoder重新encode，在本地生成parity数。</li>
<li>将本地生成的block数据发送到一个datanode上，datanode的选取规则是从集群中除原block所属节点外随机选取一个。发送过程同时生成block的meta文件。</li>
</ol>


<p><strong>parity文件</strong></p>

<p>parity文件的修复处理相对简单：</p>

<ol>
<li>在本次创建lost block的临时文件</li>
<li>获取parity文件的源文件，通过Encoder重新encode，在本地生成parity文件的block</li>
<li>选取一个dn（选取规则和har parity文件修复一致），将block数据发送到该dn上，并同时生成meta文件</li>
</ol>


<p><strong>源数据文件</strong></p>

<p>源文件的恢复与parity文件的修复相反，是一个decode过程：</p>

<ol>
<li>对于file中丢失的每个block执行修复操作</li>
<li>在本地创建block的临时文件</li>
<li>通过Decoder恢复block数据</li>
<li>选取一个target dn，将block数据发送给target dn，并同时生成meta文件。</li>
</ol>


<h4>3.2.5 Decoder的修复过程</h4>

<p>Decoder的修复过程即一个parity文件的decode过程：</p>

<p><img src="/images/hdfs/Raid-decode.png" alt="Decode" /></p>

<ol>
<li>根据文件中出错的位置，计算出错的block，该block所在的stripe，以及在stripe中的位置，计算parity文件相应block的位置。</li>
<li>通过ParallelStreamReader读取源block数据和parity数据，读取方式与编码时类似</li>
<li>通过Erasured Code将源block和parity数据的进行解码，生成丢失的block数据。</li>
</ol>


<h2>四、参考资料</h2>

<ol>
<li><a href="http://hadoopblog.blogspot.com/2009/08/hdfs-and-erasure-codes-hdfs-raid.html">HDFS and Erasure Codes (HDFS-RAID)</a></li>
<li><a href="http://wiki.apache.org/hadoop/HDFS-RAID">HDFS-RAID wiki</a></li>
<li><a href="http://en.wikipedia.org/wiki/Erasure_code">Erasure Code</a></li>
<li><a href="https://github.com/facebook/hadoop-20/tree/production/src/contrib/raid">Facebook hadoop-20</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS-2246:使用BlockReaderLocal优化本地block读取]]></title>
    <link href="http://jiangbo.me/blog/2012/12/10/hdfs-blockreaderlocal/"/>
    <updated>2012-12-10T16:09:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/12/10/hdfs-blockreaderlocal</id>
    <content type="html"><![CDATA[<h2>一、背景</h2>

<p>HDFS中对Block的读取使用DataXceiver通过Socket发送Packet数据进行，client端通过一个BlockReader来接收Socket中的Block数据。详见<a href="">HDFS数据读取流程</a>。大致流程如下：</p>

<ol>
<li>client端调用FileSystem.open()获取一个DFSInputStream</li>
<li>client端调用DFSInputStream.read(byte[] buffer, int off, int len)读取数据</li>
<li>client端通过DFSInputStream.blockSeekTo()向NameNode发起请求，定位到需要读取的block所在的Datanode，构建一个BlockReader用于读去对应DataNode上的block数据，返回该DataNodeInfo</li>
<li>BlockReader主要负责同DataNode间建立一个Socket链接用于读取数据，BlockReader首先向DataNode发送请求头（包括操作类型，blockId，block时间戳，起始偏移量，读取数据的长度，client名）</li>
<li>DataNode接收到DataXceiverServer接收到client端请求后，构建一个DataXceiver处理该请求。</li>
<li>DataXceiver首先解析请求头，获取请求操作类型，当发现是READ_BLOCK操作后，调用相应的readBlock()方法处理</li>
<li>readBlock方法解析请求中需要的blockId，构建一个BlockSender用于读取磁盘上的block文件数据并发送给client</li>
<li>BlockSender读取磁盘上的block文件，将数据按照chunk通过socket发送给client的blockReader，同时，BlockSender在发送chunk后需要从meta文件中读取该chunk的checksum数据，同样发送给client，用于该chunk的checksum校验。</li>
<li>client端通过BlockReader.readChunks()接收BlockSender发送的chunk数据，并进行checksum校验，校验成功后向DataNode发送checksumOk。</li>
<li>循环6-10，直至当前block的数据全部被读取完成。</li>
<li>循环执行3-11, 直至需要读取的文件数据都被读取完。</li>
</ol>


<p>这个过程是在集群环境想，client读取datanode上数据的一个正常流程，但事实上当client和datanode位于同一个物理节点上时（如Hadoop集群中，task运行在datanode上），这个过程显的有些多余，client可以直接通过本地文件系统api读取文件，而不需要走繁杂的socket流程。</p>

<h2>二、设计实现</h2>

<p>HDFS-2246中提供了一个BlockReaderLocal的实现，当client发现从NameNode返回的Block所属的datanode和client位于同一节点上时，构建一个BlockReaderLocal用于读取本地文件。
上述3-10的流程将简化为：</p>

<ol>
<li>client端向NameNode发起请求获取block所属的datanode信息后，判断该datanode是否和client位于同一节点，是且开启了本地读取功能，则构建一个BlockReaderLocal读取本地文件，否则构建一个BlockReader按照原流程进行。</li>
<li>BlockReaderLocal通过DataNode.getBlockLocaPathInfo()从DataNode获取block的本地文件路径信息。</li>
<li>BlockReaderLocal构建InputStream读取block文件和meta文件信息</li>
<li>对于需要checksum的场景（默认），通过blockReaderLocal.readChunks()按chunk读取本地文件，同时读取meta文件中该chunk的checksum数据，进行校验</li>
<li>对于跳过checksum的场景，直接通过InputStream.read()读取block数据。</li>
</ol>


<h3>扩展DataNode协议接口</h3>

<p>client端需要能够从DataNode获取block文件的本地文件路径信息。因此扩展ClientDataNodeProtocol，增加一个</p>

<pre><code>BlockLocalPathInfo getBlockLocalPathInfo(Block block) throws IOException;
</code></pre>

<p>接口用于获取block的本地路径信息</p>

<h3>本地文件读取</h3>

<p>BlockReaderLocal共过BufferedInputStream直接读取本地文件，注意此处HDFS-2246的patch中使用的是FileInputStream，实际测试过程中发现，FileInputStream对本地文件的读取性能较差， 替换为使用BufferedInputStream</p>

<h3>checksum较验</h3>

<p>为了完成checksum校验，BlockReaderLocal同时需要读取block的meta文件，每当block文件读取一个chunk时需要从meta文件读取一个checksum数据，进行checksum校验，通过校验后进行下一个chunk的读取和校验。由于BlockReaderLocal读取的是本地文件，避免的网络传输对数据的影响，因此可以配置跳过checksum检查，以提高读取性能。默认是需要做checksum的。</p>

<h3>本机判断</h3>

<p>当前patch中的实现主要用IP来判断是否block所在的datanode与client是否位于同一节点上。</p>

<h2>测试</h2>

<p>通过TestDFSIO工具测试一个单节点的集群，2个文件，每个文件1000M
./bin/hadoop jar hadoop-0.19.1-dc-test.jar TestDFSIO -read -nrFiles 2 -fileSize 1000
测试结果对比:
socket读取：</p>

<pre><code>---
12/12/07 13:52:57 INFO mapred.FileInputFormat: ----- TestDFSIO ----- : read
12/12/07 13:52:57 INFO mapred.FileInputFormat:            Date &amp; time: Fri Dec 07 13:52:57 CST 2012
12/12/07 13:52:57 INFO mapred.FileInputFormat:        Number of files: 2
12/12/07 13:52:57 INFO mapred.FileInputFormat: Total MBytes processed: 2000
12/12/07 13:52:57 INFO mapred.FileInputFormat:      Throughput mb/sec: 283.5270768358378
12/12/07 13:52:57 INFO mapred.FileInputFormat: Average IO rate mb/sec: 283.5281982421875
12/12/07 13:52:57 INFO mapred.FileInputFormat:  IO rate std deviation: 0.5685961122141402
</code></pre>

<p>本地读取</p>

<pre><code>---
12/12/07 13:48:59 INFO mapred.FileInputFormat: ----- TestDFSIO ----- : read
12/12/07 13:48:59 INFO mapred.FileInputFormat:            Date &amp; time: Fri Dec 07 13:48:59 CST 2012
12/12/07 13:48:59 INFO mapred.FileInputFormat:        Number of files: 2
12/12/07 13:48:59 INFO mapred.FileInputFormat: Total MBytes processed: 2000
12/12/07 13:48:59 INFO mapred.FileInputFormat:      Throughput mb/sec: 369.61744594344856
12/12/07 13:48:59 INFO mapred.FileInputFormat: Average IO rate mb/sec: 369.6180725097656
12/12/07 13:48:59 INFO mapred.FileInputFormat:  IO rate std deviation: 0.4800772
</code></pre>

<p>另外通过Patch中提供的TestShortCircuitLocalRead工具，测试结果如下：</p>

<p>本地读取并进行checksum校验</p>

<pre><code>true no 1 32000000
---
Iteration 20 took 115453
Iteration 20 took 115803
Iteration 20 took 115748
</code></pre>

<p>socket读取并进行checksum校验（默认）</p>

<pre><code>no no 1 32000000
---
Iteration 20 took 128820
Iteration 20 took 135305
Iteration 20 took 129145
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用FUSE-DFS mount HDFS]]></title>
    <link href="http://jiangbo.me/blog/2012/10/23/mount-hdfs-with-fuse-dfs/"/>
    <updated>2012-10-23T10:24:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/23/mount-hdfs-with-fuse-dfs</id>
    <content type="html"><![CDATA[<h2>介绍</h2>

<p>Hadooop源码中自带了contrib/fuse-dfs模块，用于实现通过libhdfs和fuse将HDFS mount到*inux的本地。</p>

<h2>编译</h2>

<h3>环境</h3>

<ol>
<li>Linux: 2.6.18-164.el5 x86_64</li>
<li>JDK: 1.6.0_23 64bit</li>
<li>Hadoop: 0.19.1 下面假设源码目录为$HADOOP_SRC_HOME</li>
<li>Ant: 1.8.4</li>
<li>GCC: 4.1.2(系统默认)</li>
</ol>


<h3>编译libhdfs</h3>

<h4>修改configure执行权限</h4>

<pre><code>$chmod +x $HADOOP_SRC_HOME/src/c++/pipes/configure
$chmod +x $HADOOP_SRC_HOME/src/c++/utils/configure
</code></pre>

<h4>修改Makefile，调整编译模式</h4>

<p>64位机中，需要修改libhdfs的Makefile，将GCC编译的输出模式由32(-m32)位改为64(-m64)位</p>

<pre><code>CC = gcc
LD = gcc
CFLAGS =  -g -Wall -O2 -fPIC
LDFLAGS = -L$(JAVA_HOME)/jre/lib/$(OS_ARCH)/server -ljvm -shared -m64(这里) -Wl,-x
PLATFORM = $(shell echo $$OS_NAME | tr [A-Z] [a-z])
CPPFLAGS = -m64(还有这里) -I$(JAVA_HOME)/include -I$(JAVA_HOME)/include/$(PLATFORM)
</code></pre>

<h4>编译</h4>

<p>在$HADOOP_HOME目录下执行</p>

<pre><code>$ ant compile -Dcompile.c++=true -Dlibhdfs=true
</code></pre>

<p>编译结果将生成libhdfs库，位于$HADOOP_SRC_HOME/build/libhdfs目录下</p>

<h3>编译fuse-dfs</h3>

<h4>安装fuse库</h4>

<p>fuse-dfs依赖fuse库，可通过</p>

<pre><code>sudo lsmod|grep fuse
</code></pre>

<p>检查是否已经安装，如没有，可通过：</p>

<pre><code>yum -y install fuse fuse-devel fuse-libs
</code></pre>

<p>安装相关依赖库。</p>

<h4>设置编译库路径</h4>

<p>设置编译库路径，将libhdfs的库加入到编译路径中</p>

<pre><code>export LD_LIBRARY_PATH=/usr/lib:/usr/local/lib:$HADOOP_SRC_HOME/build/c++/Linux-amd64-64/lib:$JAVA_HOME/jre/lib/amd64/server
</code></pre>

<h4>编译</h4>

<p>编译contrib/fuse-dfs模块：</p>

<pre><code>ant compile-contrib -Dlibhdfs=1 -Dfusedfs=1
</code></pre>

<p>编译完成将会生成$HADOOP_HOME/build/contrib/fuse-dfs/目录，内有：</p>

<pre><code>fuse-dfs]$ ls
fuse_dfs  fuse_dfs_wrapper.sh  test
</code></pre>

<p>其中fuse_dfs是可执行程序，fuse_dfs_wrapper.sh是包含一些环境变量设置的脚本，不过其中大部分需要修改:(</p>

<h4>修改fuse_dfs_warpper.sh</h4>

<pre><code>#Hadoop安装目录
export HADOOP_HOME=/home/bo.jiangb/yunti-trunk/build/hadoop-0.19.1-dc
#将fuse_dfs加入到PATH
export PATH=$HADOOP_HOME/contrib/fuse_dfs:$PATH
#将hadoop的jar加入到CLASSPATH
for f in ls $HADOOP_HOME/lib/*.jar $HADOOP_HOME/*.jar ; do
export  CLASSPATH=$CLASSPATH:$f
done
#设置机器模式
export OS_ARCH=amd64
#设置JAVA_HOME
export  JAVA_HOME=/home/admin/tools/jdk1.6
#将libhdfs加入到链接库路径中
export LD_LIBRARY_PATH=$JAVA_HOME/jre/lib/$OS_ARCH/server:/home/bo.jiangb/yunti-trunk/build/libhdfs:/usr/local/lib
./fuse_dfs $@
</code></pre>

<h2>使用</h2>

<h3>mount</h3>

<ol>
<li><p>新建一个空目录</p>

<p> $mkdir /tmp/dfs</p></li>
<li><p>挂载dfs
$./fuse_dfs_wrapper.sh dfs://master_node(namenode地址):port /tmp/dfs -d
-d表示debug模式，如果正常，可以将-d参数去掉。</p></li>
</ol>


<h3>unmount</h3>

<p>卸载可通过：</p>

<pre><code>fusermount -u /tmp/dfs
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS源码学习（15）——DataXceiverServer]]></title>
    <link href="http://jiangbo.me/blog/2012/10/18/hdfs-dataxceiver/"/>
    <updated>2012-10-18T22:00:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/18/hdfs-dataxceiver</id>
    <content type="html"><![CDATA[<p>HDFS中有两种类型的通信机制，一种是进行消息传递的Hadoop IPC机制，一种是用于处理数据传输的DataXceiver机制。前者包括client<->namenode之间的通信，以及datanode<->namenode间通信，后者包括client<->datanode, datanode<->datanode间的数据传输。</p>

<h2>DataXceiverServer</h2>

<p>DataNode在启动时会通过DataXceiverServer开启一个Socket端口，负责block数据的读写。DataXceiverServer本身作为一个守护线程，监听dfs.datanode.address配置的数据读写服务端口。当有请求来时，新建一个DataXceiver线程处理请求。</p>

<h2>DataXceiver</h2>

<p>DataXceiver线程用于处理一个读/写数据流请求，其run方法入下主要是根据请求中不同的请求类型，调用响应的处理方法。</p>

<p>请求操作类型定义在DataTransferProtocol中，主要有：</p>

<ol>
<li>OP_WRITE_BLOCK： 写入Block数据，对应writeBlock()方法</li>
<li>OP_READ_BLOCK： 读取Block数据，对应readBlock()方法</li>
<li>OP_READ_METADATA： 读取Block元数据，对应readMetadata()方法</li>
<li>OP_REPLACE_BLOCK： 替换Block，将block发送到目标datanode上，用于IO负载均衡；对应replaceBlock()方法。</li>
<li>OP_COPY_BLOCK：复制Block，将block发送到proxy source上，用于IO负载均衡；对应copyBlock()方法。</li>
<li>OP_BLOCK_CHECKSUM：获取Block的checksum；对应getBlockChecksum()方法。</li>
</ol>


<p>请处理返回的状态也定义在该类中：</p>

<ol>
<li>OP_STATUS_SUCCESS： 成功</li>
<li>OP_STATUS_ERROR： 请求出错</li>
<li>OP_STATUS_ERROR_CHECKSUM： checksum校验出错</li>
<li>OP_STATUS_ERROR_INVALID： 读取无效block</li>
<li>OP_STATUS_ERROR_EXISTS：block不存在</li>
<li>OP_STATUS_CHECKSUM_OK： checksum校验正常</li>
</ol>


<h3>1.读取block——readBlock()</h3>

<p>OP_READ_BLOCK的请求数据格式如下：</p>

<p><img src="/images/hdfs/ReadBlock.png" alt="READ_BLOCK" /></p>

<p>返回数据格式如下：</p>

<p><img src="/images/hdfs/ReadResponse.png" alt="READ_Response" /></p>

<p>readBlock()主要从disk读取block数据，构建一个DataOutputStream数据流，并新建一个BlockSender将这个数据流发送出去（datanode或者client）。</p>

<p>BlockSender.sendBlock()发送的Block的流程大体如下：</p>

<ol>
<li>读取block的meta信息，获得checksum并发送</li>
<li>发送数据读取的偏移量</li>
<li>将block数据切分为packet，发送给client</li>
<li>所有packet发送完之后，关闭checksum文件和block文件</li>
</ol>


<h3>2.写入block——writeBlock()</h3>

<p>OP_WRITE_BLOCK的请求数据格式如下：</p>

<p><img src="/images/hdfs/WriteRequest.png" alt="WRITE_BLOCK" /></p>

<p>writeBlock()解析请求信息，构建一个BlockReceiver处理数据接收和写入，在client（或上一datanode节点）-当前datanode节点-下一datanode节点之间建立一个如下连接。</p>

<p><img src="/images/hdfs/WriteBlock.png" alt="WRITE_BLOCK" /></p>

<ol>
<li>BlockReceiver从上按packet一节点读取数据，写入到本地disk</li>
<li>如有下一备份节点，将该packet转发给下一节点</li>
<li>将该packet加入到ackqueue队列中等待ack消息</li>
<li>当下一节点完成该packet写入后会返回该packet对应的ack信息</li>
<li>PakcetResponder接收到ack信息后，将ackqueue中该packet删除，并向前置节点发送ack信息</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS源码学习（14）——Client代码结构]]></title>
    <link href="http://jiangbo.me/blog/2012/10/18/hdfs-client-code/"/>
    <updated>2012-10-18T21:59:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/18/hdfs-client-code</id>
    <content type="html"><![CDATA[<p>Client核心代码有DistributedFileSystem和DFSClient。</p>

<p><img src="/images/hdfs/Client.png" alt="Client" /></p>

<p>DistributedFileSystem扩展子FileSystem，在为客户端提供一个文件系统接口实现。其内部使用DFSClient完成各类文件操作。</p>

<p>DFSClient使用ClientProtocol与NameNode通信，完成文件元信息操作。并通过Socket连接完成与DataNode间的block读写操作。</p>

<p>DFSClient代码结构如下：</p>

<p><img src="/images/hdfs/DFSClient.png" alt="DFSClient" /></p>

<ol>
<li>LeaseChecker主要用于lease检查和续约。</li>
<li>DFSOutputStream用于提供带buffer的字节流写入功能。client在写入数据时先将数据缓存在本地。并将数据切分成多个packet（默认每个packet为64K）。每个packet又被拆分成多个chunk（默认512Byte），每个chunk都有一个checksum。client写满一个packet后会将该packet加入到一个dataqueue中。由DataStreamer线程负责将每个packet发送给datanode pipeline。发送完一个pakcet，streamer会将其从dataqueue移至ackqueue中。ResponseProcessor负责接收datanode发回的ack信息，每成功接收一个packet的ack信息，ResponseProcessor会将ackqueue中该packet删除。</li>
<li>DFSInputStream用于提供字节流的读取，其内部封装了与NN和DN的交互</li>
<li>DataStreamer: 负责向datanode pipeline发送packet。其本身是一个Daemon线程，从namenode获取blockId和block存放位置，将packet发送给pipeline中的datanode，每个packet都有一个seqId，每个packet发送完时都会收到datanode的ack信息。当收到所有packet的ack信息后（表示该block已发送完），streamer关闭该block。</li>
<li>ResponseProcessor:用于接收datanode返回ack信息，并将响应ackqueue中的packet删除</li>
</ol>


<h2>创建文件</h2>

<ol>
<li>client向NameNode发起创建文件请求</li>
<li>NameNode.create（）处理创建文件请求，检查是否有重名，当前是否处于Safe-mode，是否有权限创建文件， 校验通过后创建一个INode记录。</li>
<li>NameNode将创建文件的事件记录到EditLog中</li>
<li>INode被创建后，NameNode发放给Client一个lease，Client可以使用这个lease通过ClientProtocol访问，进行只读操作。（写操作需要等文件close）</li>
</ol>


<h2>写入流程</h2>

<p>client写入流程如下图所示：</p>

<p><img src="/images/hdfs/ClientWrite.png" alt="ClientWrite" /></p>

<ol>
<li>Client向NameNode发起创建文件的RPC请求</li>
<li>NameNode检查文件是否已经存在，是否有权创建等，成功则创建一个文件记录，并发放给Client一个lease</li>
<li>Client获得lease之后开始进行数据写入，写入的数据首先被缓存本地，并被拆分为多个packet，放置到dataqueue队列中</li>
<li>DataStreamer线程负责检查dataqueue队列，发现有数据时且没有可用block时，向NameNode发送addBlock()请求，申请一个分配一个block空间。NameNode返回给DataStreamer一个blockId和用于存放block的datanode list</li>
<li>DataStreamer将每个packet数据发送给datanode pipeline，并将该packet移至ackqueue</li>
<li>datanode pipeline中第一个datanode收到packet之后存储到本地block中并穿行备份至后续datanode中</li>
<li>pipeline中datanode存储好packet之后会逆序返回ack信息，并最终返回给client.</li>
<li>Client端ResponseProcessor捕获到每个packet的ack信息时会将响应ackqueue中的packet删除</li>
<li>当所有数据都写入完成后，client会向NameNode发起一个complete RPC请求，告知文件最新的时间戳和已经发送给datanode的block长度。NameNode检查所有block的副本信息，只有所有block的副本数均满足最低要求时，complete会返回成功。</li>
<li>最后，NameNode将收回client持有的lease。</li>
</ol>


<p>NameNode处理addBlock()请求的流程大致如下：</p>

<ol>
<li>校验client是否有该文件的lease</li>
<li>清理上一次写入记录，包括：a.提交上一次写入，b.更新lease有效期；c.将完成的写入记录到EditLog中</li>
<li>清理完毕之后，使用BlockManager分配指定副本数个block及其对应的datanode信息，返回给client</li>
</ol>


<p>DataNode处理block写入的流程大致如下：</p>

<ol>
<li>将block复制到本地磁盘</li>
<li>发送block received消息给NameNode告知写入了一个新的block</li>
<li>将block数据发送给datanode pipeline中下一个datanode，进行备份</li>
<li>返回一个ack消息给前一个调用者</li>
</ol>


<p>后续的datanode收到上一个datanode的备份block请求是做类似的操作。</p>

<h2>读取流程</h2>

<p>读取流程相对简单写，如下所示 ：</p>

<p><img src="/images/hdfs/ClientRead.png" alt="ClientWrite" /></p>

<ol>
<li>client向NameNode发起RPC请求，获取文件的blockLocation信息</li>
<li>NameNode返回一定长度（10*defaultBlockSize）block的datanode位置信息</li>
<li>Client根据返回的blockLocation信息选取距自己最近（同一节点&lt;通一机架&lt;同一机房）的datanode读取数据，读完一个block会对该block进行checksum校验。如果校验正确则关闭与该datanode连接，去读下一个block；如果校验失败，则通知NameNode该block在当前datanode上的副本损坏了，并继续从datanode列表中获取一个datanode，重新读取该block。</li>
<li>当本次获取的blockLocation中的block全部读完，且该文件还有block时，重复1，2，3过程，直至所有blcok全部读完。</li>
</ol>


<h2>关闭文件（complete）</h2>

<ol>
<li>当Client完成文件写入之后，会调用complete()通知NameNode文件写入完成了，该请求会提交文件写入的最后一个block信息并且告知NameNode写入的block总数以及最新时间戳。</li>
<li>NameNode收到请求后会检查是否所有的block的事物都已经提交了，并且每个block的副本数都达到了最小值。如果是则返回true，否则返回false。</li>
<li>Client收到返回值后如果失败则重试几次。</li>
</ol>

]]></content>
  </entry>
  
</feed>
