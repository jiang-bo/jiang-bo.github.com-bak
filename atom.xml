<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[非纯种程序猿]]></title>
  <link href="http://jiangbo.me/atom.xml" rel="self"/>
  <link href="http://jiangbo.me/"/>
  <updated>2012-10-23T13:32:23+08:00</updated>
  <id>http://jiangbo.me/</id>
  <author>
    <name><![CDATA[jiang-bo]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[使用FUSE-DFS mount HDFS]]></title>
    <link href="http://jiangbo.me/blog/2012/10/23/mount-hdfs-with-fuse-dfs/"/>
    <updated>2012-10-23T10:24:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/23/mount-hdfs-with-fuse-dfs</id>
    <content type="html"><![CDATA[<h2>介绍</h2>

<p>Hadooop源码中自带了contrib/fuse-dfs模块，用于实现通过libhdfs和fuse将HDFS mount到*inux的本地。</p>

<h2>编译</h2>

<h3>环境</h3>

<ol>
<li>Linux: 2.6.18-164.el5 x86_64</li>
<li>JDK: 1.6.0_23 64bit</li>
<li>Hadoop: 0.19.1 下面假设源码目录为$HADOOP_SRC_HOME</li>
<li>Ant: 1.8.4</li>
<li>GCC: 4.1.2(系统默认)</li>
</ol>


<h3>编译libhdfs</h3>

<h4>修改configure执行权限</h4>

<pre><code>$chmod +x $HADOOP_SRC_HOME/src/c++/pipes/configure
$chmod +x $HADOOP_SRC_HOME/src/c++/utils/configure
</code></pre>

<h4>修改Makefile，调整编译模式</h4>

<p>64位机中，需要修改libhdfs的Makefile，将GCC编译的输出模式由32(-m32)位改为64(-m64)位</p>

<pre><code>CC = gcc
LD = gcc
CFLAGS =  -g -Wall -O2 -fPIC
LDFLAGS = -L$(JAVA_HOME)/jre/lib/$(OS_ARCH)/server -ljvm -shared -m64(这里) -Wl,-x
PLATFORM = $(shell echo $$OS_NAME | tr [A-Z] [a-z])
CPPFLAGS = -m64(还有这里) -I$(JAVA_HOME)/include -I$(JAVA_HOME)/include/$(PLATFORM)
</code></pre>

<h4>编译</h4>

<p>在$HADOOP_HOME目录下执行</p>

<pre><code>$ ant compile -Dcompile.c++=true -Dlibhdfs=true
</code></pre>

<p>编译结果将生成libhdfs库，位于$HADOOP_SRC_HOME/build/libhdfs目录下</p>

<h3>编译fuse-dfs</h3>

<h4>安装fuse库</h4>

<p>fuse-dfs依赖fuse库，可通过</p>

<pre><code>sudo lsmod|grep fuse
</code></pre>

<p>检查是否已经安装，如没有，可通过：</p>

<pre><code>yum -y install fuse fuse-devel fuse-libs
</code></pre>

<p>安装相关依赖库。</p>

<h4>设置编译库路径</h4>

<p>设置编译库路径，将libhdfs的库加入到编译路径中</p>

<pre><code>export LD_LIBRARY_PATH=/usr/lib:/usr/local/lib:$HADOOP_SRC_HOME/build/c++/Linux-amd64-64/lib:$JAVA_HOME/jre/lib/amd64/server
</code></pre>

<h4>编译</h4>

<p>编译contrib/fuse-dfs模块：</p>

<pre><code>ant compile-contrib -Dlibhdfs=1 -Dfusedfs=1
</code></pre>

<p>编译完成将会生成$HADOOP_HOME/build/contrib/fuse-dfs/目录，内有：</p>

<pre><code>fuse-dfs]$ ls
fuse_dfs  fuse_dfs_wrapper.sh  test
</code></pre>

<p>其中fuse_dfs是可执行程序，fuse_dfs_wrapper.sh是包含一些环境变量设置的脚本，不过其中大部分需要修改:(</p>

<h4>修改fuse_dfs_warpper.sh</h4>

<pre><code>#Hadoop安装目录
export HADOOP_HOME=/home/bo.jiangb/yunti-trunk/build/hadoop-0.19.1-dc
#将fuse_dfs加入到PATH
export PATH=$HADOOP_HOME/contrib/fuse_dfs:$PATH
#将hadoop的jar加入到CLASSPATH
for f in ls $HADOOP_HOME/lib/*.jar $HADOOP_HOME/*.jar ; do
export  CLASSPATH=$CLASSPATH:$f
done
#设置机器模式
export OS_ARCH=amd64
#设置JAVA_HOME
export  JAVA_HOME=/home/admin/tools/jdk1.6
#将libhdfs加入到链接库路径中
export LD_LIBRARY_PATH=$JAVA_HOME/jre/lib/$OS_ARCH/server:/home/bo.jiangb/yunti-trunk/build/libhdfs:/usr/local/lib
./fuse_dfs $@
</code></pre>

<h2>使用</h2>

<h3>mount</h3>

<ol>
<li><p>新建一个空目录</p>

<p> $mkdir /tmp/dfs</p></li>
<li><p>挂载dfs
$./fuse_dfs_wrapper.sh dfs://master_node(namenode地址):port /tmp/dfs -d
-d表示debug模式，如果正常，可以将-d参数去掉。</p></li>
</ol>


<h3>unmount</h3>

<p>卸载可通过：</p>

<pre><code>fusermount -u /tmp/dfs
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS源码学习（15）——DataXceiverServer]]></title>
    <link href="http://jiangbo.me/blog/2012/10/18/hdfs-dataxceiver/"/>
    <updated>2012-10-18T22:00:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/18/hdfs-dataxceiver</id>
    <content type="html"><![CDATA[<p>HDFS中有两种类型的通信机制，一种是进行消息传递的Hadoop IPC机制，一种是用于处理数据传输的DataXceiver机制。前者包括client<->namenode之间的通信，以及datanode<->namenode间通信，后者包括client<->datanode, datanode<->datanode间的数据传输。</p>

<h2>DataXceiverServer</h2>

<p>DataNode在启动时会通过DataXceiverServer开启一个Socket端口，负责block数据的读写。DataXceiverServer本身作为一个守护线程，监听dfs.datanode.address配置的数据读写服务端口。当有请求来时，新建一个DataXceiver线程处理请求。</p>

<h2>DataXceiver</h2>

<p>DataXceiver线程用于处理一个读/写数据流请求，其run方法入下主要是根据请求中不同的请求类型，调用响应的处理方法。</p>

<p>请求操作类型定义在DataTransferProtocol中，主要有：</p>

<ol>
<li>OP_WRITE_BLOCK： 写入Block数据，对应writeBlock()方法</li>
<li>OP_READ_BLOCK： 读取Block数据，对应readBlock()方法</li>
<li>OP_READ_METADATA： 读取Block元数据，对应readMetadata()方法</li>
<li>OP_REPLACE_BLOCK： 替换Block，将block发送到目标datanode上，用于IO负载均衡；对应replaceBlock()方法。</li>
<li>OP_COPY_BLOCK：复制Block，将block发送到proxy source上，用于IO负载均衡；对应copyBlock()方法。</li>
<li>OP_BLOCK_CHECKSUM：获取Block的checksum；对应getBlockChecksum()方法。</li>
</ol>


<p>请处理返回的状态也定义在该类中：</p>

<ol>
<li>OP_STATUS_SUCCESS： 成功</li>
<li>OP_STATUS_ERROR： 请求出错</li>
<li>OP_STATUS_ERROR_CHECKSUM： checksum校验出错</li>
<li>OP_STATUS_ERROR_INVALID： 读取无效block</li>
<li>OP_STATUS_ERROR_EXISTS：block不存在</li>
<li>OP_STATUS_CHECKSUM_OK： checksum校验正常</li>
</ol>


<h3>1.读取block——readBlock()</h3>

<p>OP_READ_BLOCK的请求数据格式如下：</p>

<p><img src="http://jiangbo.me/images/hdfs/ReadBlock.png" alt="READ_BLOCK" /></p>

<p>返回数据格式如下：</p>

<p><img src="http://jiangbo.me/images/hdfs/ReadResponse.png" alt="READ_Response" /></p>

<p>readBlock()主要从disk读取block数据，构建一个DataOutputStream数据流，并新建一个BlockSender将这个数据流发送出去（datanode或者client）。</p>

<p>BlockSender.sendBlock()发送的Block的流程大体如下：</p>

<ol>
<li>读取block的meta信息，获得checksum并发送</li>
<li>发送数据读取的偏移量</li>
<li>将block数据切分为packet，发送给client</li>
<li>所有packet发送完之后，关闭checksum文件和block文件</li>
</ol>


<h3>2.写入block——writeBlock()</h3>

<p>OP_WRITE_BLOCK的请求数据格式如下：</p>

<p><img src="http://jiangbo.me/images/hdfs/WriteRequest.png" alt="WRITE_BLOCK" /></p>

<p>writeBlock()解析请求信息，构建一个BlockReceiver处理数据接收和写入，在client（或上一datanode节点）-当前datanode节点-下一datanode节点之间建立一个如下连接。</p>

<p><img src="http://jiangbo.me/images/hdfs/WriteBlock.png" alt="WRITE_BLOCK" /></p>

<ol>
<li>BlockReceiver从上按packet一节点读取数据，写入到本地disk</li>
<li>如有下一备份节点，将该packet转发给下一节点</li>
<li>将该packet加入到ackqueue队列中等待ack消息</li>
<li>当下一节点完成该packet写入后会返回该packet对应的ack信息</li>
<li>PakcetResponder接收到ack信息后，将ackqueue中该packet删除，并向前置节点发送ack信息</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS源码学习（14）——Client代码结构]]></title>
    <link href="http://jiangbo.me/blog/2012/10/18/hdfs-client-code/"/>
    <updated>2012-10-18T21:59:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/18/hdfs-client-code</id>
    <content type="html"><![CDATA[<p>Client核心代码有DistributedFileSystem和DFSClient。</p>

<p><img src="http://jiangbo.me/images/hdfs/Client.png" alt="Client" /></p>

<p>DistributedFileSystem扩展子FileSystem，在为客户端提供一个文件系统接口实现。其内部使用DFSClient完成各类文件操作。</p>

<p>DFSClient使用ClientProtocol与NameNode通信，完成文件元信息操作。并通过Socket连接完成与DataNode间的block读写操作。</p>

<p>DFSClient代码结构如下：</p>

<p><img src="http://jiangbo.me/images/hdfs/DFSClient.png" alt="DFSClient" /></p>

<ol>
<li>LeaseChecker主要用于lease检查和续约。</li>
<li>DFSOutputStream用于提供带buffer的字节流写入功能。client在写入数据时先将数据缓存在本地。并将数据切分成多个packet（默认每个packet为64K）。每个packet又被拆分成多个chunk（默认512Byte），每个chunk都有一个checksum。client写满一个packet后会将该packet加入到一个dataqueue中。由DataStreamer线程负责将每个packet发送给datanode pipeline。发送完一个pakcet，streamer会将其从dataqueue移至ackqueue中。ResponseProcessor负责接收datanode发回的ack信息，每成功接收一个packet的ack信息，ResponseProcessor会将ackqueue中该packet删除。</li>
<li>DFSInputStream用于提供字节流的读取，其内部封装了与NN和DN的交互</li>
<li>DataStreamer: 负责向datanode pipeline发送packet。其本身是一个Daemon线程，从namenode获取blockId和block存放位置，将packet发送给pipeline中的datanode，每个packet都有一个seqId，每个packet发送完时都会收到datanode的ack信息。当收到所有packet的ack信息后（表示该block已发送完），streamer关闭该block。</li>
<li>ResponseProcessor:用于接收datanode返回ack信息，并将响应ackqueue中的packet删除</li>
</ol>


<h2>创建文件</h2>

<ol>
<li>client向NameNode发起创建文件请求</li>
<li>NameNode.create（）处理创建文件请求，检查是否有重名，当前是否处于Safe-mode，是否有权限创建文件， 校验通过后创建一个INode记录。</li>
<li>NameNode将创建文件的事件记录到EditLog中</li>
<li>INode被创建后，NameNode发放给Client一个lease，Client可以使用这个lease通过ClientProtocol访问，进行只读操作。（写操作需要等文件close）</li>
</ol>


<h2>写入流程</h2>

<p>client写入流程如下图所示：</p>

<p><img src="http://jiangbo.me/images/hdfs/ClientWrite.png" alt="ClientWrite" /></p>

<ol>
<li>Client向NameNode发起创建文件的RPC请求</li>
<li>NameNode检查文件是否已经存在，是否有权创建等，成功则创建一个文件记录，并发放给Client一个lease</li>
<li>Client获得lease之后开始进行数据写入，写入的数据首先被缓存本地，并被拆分为多个packet，放置到dataqueue队列中</li>
<li>DataStreamer线程负责检查dataqueue队列，发现有数据时且没有可用block时，向NameNode发送addBlock()请求，申请一个分配一个block空间。NameNode返回给DataStreamer一个blockId和用于存放block的datanode list</li>
<li>DataStreamer将每个packet数据发送给datanode pipeline，并将该packet移至ackqueue</li>
<li>datanode pipeline中第一个datanode收到packet之后存储到本地block中并穿行备份至后续datanode中</li>
<li>pipeline中datanode存储好packet之后会逆序返回ack信息，并最终返回给client.</li>
<li>Client端ResponseProcessor捕获到每个packet的ack信息时会将响应ackqueue中的packet删除</li>
<li>当所有数据都写入完成后，client会向NameNode发起一个complete RPC请求，告知文件最新的时间戳和已经发送给datanode的block长度。NameNode检查所有block的副本信息，只有所有block的副本数均满足最低要求时，complete会返回成功。</li>
<li>最后，NameNode将收回client持有的lease。</li>
</ol>


<p>NameNode处理addBlock()请求的流程大致如下：</p>

<ol>
<li>校验client是否有该文件的lease</li>
<li>清理上一次写入记录，包括：a.提交上一次写入，b.更新lease有效期；c.将完成的写入记录到EditLog中</li>
<li>清理完毕之后，使用BlockManager分配指定副本数个block及其对应的datanode信息，返回给client</li>
</ol>


<p>DataNode处理block写入的流程大致如下：</p>

<ol>
<li>将block复制到本地磁盘</li>
<li>发送block received消息给NameNode告知写入了一个新的block</li>
<li>将block数据发送给datanode pipeline中下一个datanode，进行备份</li>
<li>返回一个ack消息给前一个调用者</li>
</ol>


<p>后续的datanode收到上一个datanode的备份block请求是做类似的操作。</p>

<h2>读取流程</h2>

<p>读取流程相对简单写，如下所示 ：</p>

<p><img src="http://jiangbo.me/images/hdfs/ClientRead.png" alt="ClientWrite" /></p>

<ol>
<li>client向NameNode发起RPC请求，获取文件的blockLocation信息</li>
<li>NameNode返回一定长度（10*defaultBlockSize）block的datanode位置信息</li>
<li>Client根据返回的blockLocation信息选取距自己最近（同一节点&lt;通一机架&lt;同一机房）的datanode读取数据，读完一个block会对该block进行checksum校验。如果校验正确则关闭与该datanode连接，去读下一个block；如果校验失败，则通知NameNode该block在当前datanode上的副本损坏了，并继续从datanode列表中获取一个datanode，重新读取该block。</li>
<li>当本次获取的blockLocation中的block全部读完，且该文件还有block时，重复1，2，3过程，直至所有blcok全部读完。</li>
</ol>


<h2>关闭文件（complete）</h2>

<ol>
<li>当Client完成文件写入之后，会调用complete()通知NameNode文件写入完成了，该请求会提交文件写入的最后一个block信息并且告知NameNode写入的block总数以及最新时间戳。</li>
<li>NameNode收到请求后会检查是否所有的block的事物都已经提交了，并且每个block的副本数都达到了最小值。如果是则返回true，否则返回false。</li>
<li>Client收到返回值后如果失败则重试几次。</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS源码学习（13）——DataNode启动过程]]></title>
    <link href="http://jiangbo.me/blog/2012/10/18/hdfs-datanode-startup/"/>
    <updated>2012-10-18T21:57:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/18/hdfs-datanode-startup</id>
    <content type="html"><![CDATA[<h2>main()</h2>

<pre><code>  public static void main(String args[]) {
    try {
      StringUtils.startupShutdownMessage(DataNode.class, args, LOG);
      DataNode datanode = createDataNode(args, null);
      if (datanode != null)
        datanode.join();
    } catch (Throwable e) {
      LOG.error(StringUtils.stringifyException(e));
      System.exit(-1);
    }
  }
</code></pre>

<h2>createDataNode()</h2>

<pre><code>  public static DataNode createDataNode(String args[],
                                 Configuration conf) throws IOException {
    // 初始化datanode
    DataNode dn = instantiateDataNode(args, conf);
    // 启动datanode后台线程
    runDatanodeDaemon(dn);
    return dn;
  }
</code></pre>

<h3>1.instantiateDataNode（）</h3>

<pre><code>  public static DataNode instantiateDataNode(String args[],
                                      Configuration conf) throws IOException {
    // 处理配置
    if (conf == null)
      conf = new Configuration();
    if (!parseArguments(args, conf)) {
      printUsage();
      return null;
    }
    if (conf.get("dfs.network.script") != null) {
      LOG.error("This configuration for rack identification is not supported" +
          " anymore. RackID resolution is handled by the NameNode.");
      System.exit(-1);
    }
    // 获取data目录配置
    String[] dataDirs = conf.getStrings("dfs.data.dir");
    dnThreadName = "DataNode: [" +
                        StringUtils.arrayToString(dataDirs) + "]";
    //创建datanode实例
    return makeInstance(dataDirs, conf);
  }
</code></pre>

<h4>1.1. makeInfstance()</h4>

<p>该方法主要用于检查给定的data目录中至少有一个可以创建，并实例化DataNode</p>

<pre><code>  public static DataNode makeInstance(String[] dataDirs, Configuration conf)
    throws IOException {
    ArrayList&lt;File&gt; dirs = new ArrayList&lt;File&gt;();
    for (int i = 0; i &lt; dataDirs.length; i++) {
      File data = new File(dataDirs[i]);
      try {
        DiskChecker.checkDir(data);
        dirs.add(data);
      } catch(DiskErrorException e) {
        LOG.warn("Invalid directory in dfs.data.dir: " + e.getMessage());
      }
    }
    if (dirs.size() &gt; 0) 
      return new DataNode(conf, dirs);
    LOG.error("All directories in dfs.data.dir are invalid.");
    return null;
  }
</code></pre>

<h4>1.2 new DataNode()</h4>

<pre><code>  DataNode(Configuration conf, 
           AbstractList&lt;File&gt; dataDirs) throws IOException {
    // 设置配置信息
    super(conf);
    datanodeObject = this;
    supportAppends = conf.getBoolean("dfs.support.append", false);
    this.conf = conf;
    try {
      // 启动DataNode
      startDataNode(conf, dataDirs);
    } catch (IOException ie) {
      shutdown();
      throw ie;
    }
  }
</code></pre>

<h5>1.2.1 startDataNode()</h5>

<p>代码较长，仅列出主要步骤：</p>

<ol>
<li>设置配置信息</li>
<li>向NameNode发起RPC请求，获取版本和StorageID信息</li>
<li>获取启动配置</li>
<li>初始化存储信息，构建FSDataSet</li>
<li>获取可用的端口号</li>
<li>调整注册信息中的机器名，加上端口号</li>
<li>初始化DataXceiverServer</li>
<li>设置blockReport和heartbeat各自的时间间隔</li>
<li>初始化blockScanner</li>
<li>初始胡并启动servlet info server，提供内容查询的http服务</li>
<li>初始化ipc server，该ipc server主要用于完成DataNode间的block recover。</li>
</ol>


<h3>runDatanodeDaemon()</h3>

<pre><code>  public static void runDatanodeDaemon(DataNode dn) throws IOException {
    if (dn != null) {
      //register datanode
      dn.register();
      dn.dataNodeThread = new Thread(dn, dnThreadName);
      dn.dataNodeThread.setDaemon(true); // needed for JUnit testing
      dn.dataNodeThread.start();
    }
  }
</code></pre>

<h4>2.1 向NameNode注册 —— dn.register();</h4>

<pre><code>  private void register() throws IOException {
    if (dnRegistration.getStorageID().equals("")) {
      setNewStorageID(dnRegistration);
    }
    while(shouldRun) {
      try {
        // reset name to machineName. Mainly for web interface.
        dnRegistration.name = machineName + ":" + dnRegistration.getPort();
        // 通过NameProtocal向NameNode注册
        dnRegistration = namenode.register(dnRegistration);
        break;
      } catch(SocketTimeoutException e) {  // namenode is busy
        LOG.info("Problem connecting to server: " + getNameNodeAddr());
        try {
          Thread.sleep(1000);
        } catch (InterruptedException ie) {}
      }
    }
    assert ("".equals(storage.getStorageID()) 
            &amp;&amp; !"".equals(dnRegistration.getStorageID()))
            || storage.getStorageID().equals(dnRegistration.getStorageID()) :
            "New storageID can be assigned only if data-node is not formatted";
    if (storage.getStorageID().equals("")) {
      storage.setStorageID(dnRegistration.getStorageID());
      storage.writeAll();
      LOG.info("New storage id " + dnRegistration.getStorageID()
          + " is assigned to data-node " + dnRegistration.getName());
    }
    if(! storage.getStorageID().equals(dnRegistration.getStorageID())) {
      throw new IOException("Inconsistent storage IDs. Name-node returned "
          + dnRegistration.getStorageID() 
          + ". Expecting " + storage.getStorageID());
    }

    if (supportAppends) {
      Block[] bbwReport = data.getBlocksBeingWrittenReport();
      long[] blocksBeingWritten = BlockListAsLongs.convertToArrayLongs(bbwReport);
      //如果支持append，则报告正在写入的block信息
      namenode.blocksBeingWrittenReport(dnRegistration, blocksBeingWritten);
    }
    // 调整下一次的BR时间，使其在下次heartbeat时进行
    scheduleBlockReport(initialBlockReportDelay);
  }
</code></pre>

<h4>2.2 启动datanode线程 —— dn.dataNodeThread.start();</h4>

<p>datanode线程本身非常简单，不停调用offerSevice提供服务：</p>

<pre><code>  public void run() {
    LOG.info(dnRegistration + "In DataNode.run, data = " + data);

    // start dataXceiveServer
    dataXceiverServer.start();
    new Thread(new CrashVolumeChecker()).start();//added by wukong

    while (shouldRun) {
      try {
        startDistributedUpgradeIfNeeded();
        offerService();
      } catch (Exception ex) {
        LOG.error("Exception: " + StringUtils.stringifyException(ex));
        if (shouldRun) {
          try {
            Thread.sleep(5000);
          } catch (InterruptedException ie) {
          }
        }
      }
    }

    LOG.info(dnRegistration + ":Finishing DataNode in: "+data);
    shutdown();
  }
</code></pre>

<h5>2.2.1 offerService()</h5>

<p>offerService的核心是周期性进行heartbeat和blockReport，主要流程如下：</p>

<p><img src="http://jiangbo.me/images/hdfs/offerService.png" alt="offerService" /></p>

<pre><code>  public void offerService() throws Exception {

    LOG.info("using BLOCKREPORT_INTERVAL of " + blockReportInterval + "msec" + 
       " Initial delay: " + initialBlockReportDelay + "msec");
    LOG.info("using DELETEREPORT_INTERVAL of " + deletedReportInterval + "msec");
    LOG.info("using HEARTBEAT_INTERVAL of " + heartBeatInterval + "msec");
    LOG.info("using HEARTBEAT_EXPIRE_INTERVAL of " + heartbeatExpireInterval + "msec");

    //
    // Now loop for a long time....
    //

    while (shouldRun) {
      try {
        long startTime = now();

        //
        // Every so often, send heartbeat or block-report
        //

        if (startTime - lastHeartbeat &gt; heartBeatInterval /* 3 secs*/) {
          //
          // All heartbeat messages include following info:
          // -- Datanode name
          // -- data transfer port
          // -- Total capacity
          // -- Bytes remaining
          //
          lastHeartbeat = startTime;
          DatanodeCommand[] cmds = namenode.sendHeartbeat(dnRegistration,
                                                       data.getCapacity(),
                                                       data.getDfsUsed(),
                                                       data.getRemaining(),
                                                       xmitsInProgress.get(),
                                                       getXceiverCount());
          myMetrics.heartbeats.inc(now() - startTime);
          //LOG.info("Just sent heartbeat, with name " + localName);
          if (!processCommand(cmds))
            continue;
        }

        reportReceivedBlocks();

        DatanodeCommand cmd = blockReport();
        processCommand(cmd);

        // start block scanner
        if (blockScanner != null &amp;&amp; blockScannerThread == null &amp;&amp;
            upgradeManager.isUpgradeCompleted()) {
          LOG.info("Starting Periodic block scanner.");
          blockScannerThread = new Daemon(blockScanner);
          blockScannerThread.start();
        }

        //
        // There is no work to do;  sleep until hearbeat timer elapses, 
        // or work arrives, and then iterate again.
        //
        long waitTime = heartBeatInterval - (System.currentTimeMillis() - lastHeartbeat);
        synchronized(receivedAndDeletedBlockList) {
          if (waitTime &gt; 0 &amp;&amp; receivedAndDeletedBlockList.size() == 0) {
            try {
              receivedAndDeletedBlockList.wait(waitTime);
            } catch (InterruptedException ie) {
            }
            delayBeforeBlockReceived();
          }
        } // synchronized

      } catch(RemoteException re) {
        String reClass = re.getClassName();
        if (UnregisteredDatanodeException.class.getName().equals(reClass) ||
            DisallowedDatanodeException.class.getName().equals(reClass) ||
            IncorrectVersionException.class.getName().equals(reClass)) {
          LOG.warn("DataNode is shutting down: " + 
                   StringUtils.stringifyException(re));
          shutdown();
          return;
        }
        LOG.warn(StringUtils.stringifyException(re));
      } catch (IOException e) {
        LOG.warn(StringUtils.stringifyException(e));
      }
    } // while (shouldRun)
  } // offerService
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS源码学习（12）——DataNode主要数据结构]]></title>
    <link href="http://jiangbo.me/blog/2012/10/18/hdfs-datanode-structure/"/>
    <updated>2012-10-18T21:56:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/18/hdfs-datanode-structure</id>
    <content type="html"><![CDATA[<p>HDFS中DataNode主要负责维护block->stream bytes的映射关系，即实际block数据的存储。
一个datanode的磁盘上存储目录下实际的文件部署结构如下：</p>

<pre><code>data/
├── blocksBeingWritten
├── current
│   ├── VERSION
│   ├── blk_-1148021215131449924
│   ├── blk_-1148021215131449924_1001.meta
│   ├── blk_-8598609183581346893
│   ├── blk_-8598609183581346893_1002.meta
│   ├── blk_6693595845022390257
│   ├── blk_6693595845022390257_1003.meta
│   └── dncp_block_verification.log.curr
├── detach
├── storage
└── tmp
</code></pre>

<p>data目录的路径是hdfs-site.xml中配置的dfs.data.dir的路径，表示每个datanode上数据存储的目录</p>

<p>1) blocksBeingWritten：当前正在写入的block，写完之后会将block移至current目录</p>

<p>2) current：当前已经写入的block文件目录</p>

<p>2.1) VERSION为存储的VERSION文件，包括namespaceId，存储Id，存储版本，存储类型，创建时间戳等信息</p>

<p>2.2）blk-*:文件为实际的block数据文件</p>

<p>2.3）blk-*_xxx.meta: block的元信息文件</p>

<p>2.4）dncp_*.log.curr: 当前copy文件</p>

<p>3) detach：copy-on-write使用的目录</p>

<p>4) tmp： 临时目录，DataNode启动时会检查 tmp的数据并删除。</p>

<h2>Storage相关</h2>

<p>Storage用于描述存储的类型，状态，目录等信息。
其主要结构如下：
<img src="http://jiangbo.me/images/hdfs/Storage.png" alt="Storage" /></p>

<h3>StorageInfo</h3>

<p>StorageInfo表示一个存储的通用信息，包括：</p>

<ol>
<li>layoutVersion： 存储文件中的版本号</li>
<li>namespaceId： 存储所属的命名空间ID</li>
<li>ctime： 该存储创建的时间戳</li>
</ol>


<h3>Storage</h3>

<p>存储信息的抽象类，管理一个server（NameNode或DataNode）上的存储目录。</p>

<p>Storage有两个关键属性：</p>

<ol>
<li>storageType: 表示该存储所属的节点类型（NameNode或是DataNode）</li>
<li>storageDirs: 该存储上存储目录的列表(ArrayList<StorageDirectory>),StorageDirectory表示一个存储目录。</li>
</ol>


<h4>StorageDirectory</h4>

<p>表示一个存储目录，有三个属性：</p>

<ol>
<li>root：根目录</li>
<li>lock：当前目录的文件锁</li>
<li>dirType：目录类型</li>
</ol>


<h4>StorageSate</h4>

<p>表示存储的状态：</p>

<ol>
<li>NON_EXISTENT: 目录不存在</li>
<li>NOT_FORMATTED: 目录未格式化</li>
<li>COMPLETE_UPGRADE: 升级完成</li>
<li>RECOVER_UPGRADE: 撤销升级</li>
<li>COMPLETE_FINALIZE: 提交完成</li>
<li>COMPLETE_ROLLBACK: 回滚完成</li>
<li>RECOVER_ROLLBACK: 撤销回滚</li>
<li>COMPLETE_CHECKPOINT: checkpoint完成</li>
<li>RECOVER_CHECKPOINT: 撤销checkpoint</li>
<li>NORMAL: 正常</li>
</ol>


<h3>DataStorage</h3>

<p>DataStorage是DataNode上使用的存储类，指定了datanode上各类存储文件的前缀：</p>

<ol>
<li>subdir：子目录前缀</li>
<li>blk_：块文件前缀</li>
<li>dncp_：拷贝文件前缀</li>
</ol>


<h2>DatanodeBlockInfo</h2>

<p>DataNode使用DatanodeBlockInfo管理block和其元数据之间的映射关系，结构如下：</p>

<p><img src="http://jiangbo.me/images/hdfs/DatanodeBlockInfo.png" alt="DatanodeBlockInfo" /></p>

<ol>
<li>volmun：block所属的卷</li>
<li>file：block文件</li>
<li>detached：是否完成copy-on-write</li>
</ol>


<h2>FSDataSet相关</h2>

<p>DataNode通过FSDataSet来完成数据的存储。FSDataset类结构如下：
<img src="http://jiangbo.me/images/hdfs/FSDataset.png" alt="FSDataset" /></p>

<h3>FSVolume</h3>

<p>FSVolumne用于进行block文件所属的卷管理，统计存储目录额使用情况，其中：</p>

<ol>
<li>currentDir： 当前数据目录, 对应data/current目录</li>
<li>dataDir： 数据目录</li>
<li>tmpDir： 临时目录, 对应data/tmp目录</li>
<li>dtacheDir: 用于实现写时复制的文件，对应data/detach目录</li>
<li>usage: 目录使用的空间</li>
<li>dfsusage: dfs使用的空间</li>
<li>reseved: 空余空间</li>
<li>blocksBeingWritten: 正在写入的block，对应data/blocksBeingWritten目录</li>
</ol>


<h3>FSVolumeSet</h3>

<p>FSVolumeSet是FSVolume的集合，提供了所有容量，剩余空间等方法。其中getNextVolume中提供了round-robin策略选取下一个volume，从而实现简单的IO负载均衡，提高IO处理能力。</p>

<h3>FSDataSet</h3>

<p>FSDataSet是在FSVolumeSet之上进行封装实现FSDatasetInterface借口，向外提供块查询和操作方法。</p>

<p>其中有几个主要属性:</p>

<ol>
<li>volumes: 卷集合（FSVolumeSet）</li>
<li>ongoingCreates: 当前活动的文件</li>
<li>maxBlocksPerDir: 每个目录下最多能存放发block数，可通过dfs.datanode.numblocks配置</li>
<li>volumeMap：块与块文件的映射信息(HashMap&lt;Block, DatanodeBlockInfo>)，当前集合中所有的块信息均维护在该map中</li>
</ol>


<h3>FSDir</h3>

<p>用于构建block块在datanode磁盘上的层次结构，默认情况下每个目录下最多64个子目录，最多能存储64个块。目录初始化时会递归扫描目录下的所有子目录和文件，构建一个树形结构。</p>

<p>addBlock时，首先尝试在当前目录新加块，如果当前目录没有空闲空间，则尝试在子目录中添加，如果没有子目录，则新建一个子目录。</p>

<h3>BlockAndFile</h3>

<p>Block与其文件名的封装</p>

<h3>ActiveFile</h3>

<p>表示一个当前活动中的文件</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS源码学习（11）——SecondaryNameNode]]></title>
    <link href="http://jiangbo.me/blog/2012/10/18/hdfs-secondary-namenode/"/>
    <updated>2012-10-18T21:55:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/18/hdfs-secondary-namenode</id>
    <content type="html"><![CDATA[<h2>概述</h2>

<p>SecondaryNameNode在HDFS中的主要作用是帮助master NameNode周期性执行checkpoint操作。</p>

<p>NameNode将运行过程中对文件的修改记录保存在EditLog中。当NameNode重新启动时会从FSImage中加载命名空间镜像，并将EditLog中的内容合并到FSImage中，将合并后的FSImage写入到磁盘，同时清空EditLog，共后续使用。但如果NameNode长时间不重启，随时间增长，EditLog将会越来越大（每次文件操作都要记录），大量占用NameNode磁盘空间，且会导致下一次重启花费大量时间在合并Editlog上。</p>

<p>为了解决这个问题，SecondaryNameNode会定期从NameNode下载最新的FSImage和EditLog，合并editLog日志到FSImage，将合并后的FSImage上传到NameNode，并清空NameNode上的Editlog，将EditLog日志大小控制在一定限度下。</p>

<h2>代码解析</h2>

<p>SecondaryNameNode代码结构如下：</p>

<p><img src="http://jiangbo.me/images/hdfs/SecondaryNameNode.png" alt="SecondaryNameNode" /></p>

<p>SecondaryNameNode本身就是实现了Runnable接口，即一个可执行线程。</p>

<p>checkpointImage表示当前SecondNameNode上的FsImage镜像，该类CheckpointStorage扩展自FSImage。</p>

<p>其中有两个主要的可配置属性：</p>

<ol>
<li>checkpointPeriod: 两次检查点的间隔时间，可通过fs.checkpoint.period配置</li>
<li>checkpointSize: EditLog文件的最大值，当EditLog超过这个最大值时会强制之行checkpoint，可通过fs.checkpoint.size配置，默认是64M</li>
</ol>


<h3>run()</h3>

<p>run方法代码如下：</p>

<pre><code>  public void run() {

    //
    // Poll the Namenode (once every 5 minutes) to find the size of the
    // pending edit log.
    //
    long period = 5 * 60;              // 5 minutes
    long lastCheckpointTime = 0;
    if (checkpointPeriod &lt; period) {
      period = checkpointPeriod;
    }

    while (shouldRun) {
      try {
        Thread.sleep(1000 * period);
      } catch (InterruptedException ie) {
        // do nothing
      }
      if (!shouldRun) {
        break;
      }
      try {
        long now = System.currentTimeMillis();

        long size = namenode.getEditLogSize();
        if (size &gt;= checkpointSize || 
            now &gt;= lastCheckpointTime + 1000 * checkpointPeriod) {
          doCheckpoint();
          lastCheckpointTime = now;
        }
      } catch (IOException e) {
        LOG.error("Exception in doCheckpoint: ");
        LOG.error(StringUtils.stringifyException(e));
        e.printStackTrace();
      } catch (Throwable e) {
        LOG.error("Throwable Exception in doCheckpoint: ");
        LOG.error(StringUtils.stringifyException(e));
        e.printStackTrace();
        Runtime.getRuntime().exit(-1);
      }
    }
  }
</code></pre>

<p>其核心就是周期性（默认每个5分钟)调用doCheckpoint().</p>

<h3>doCheckpoint()</h3>

<pre><code>  void doCheckpoint() throws IOException {

    // 准备合并所需的空间
    startCheckpoint();

    // 通知NameNode将修改信息记录到新的editlog中，并获取一个用于上传合并后的fsimage的token
    CheckpointSignature sig = (CheckpointSignature)namenode.rollEditLog();

    // error simulation code for junit test
    if (ErrorSimulator.getErrorSimulation(0)) {
      throw new IOException("Simulating error0 " +
                            "after creating edits.new");
    }
    //从NameNode获取fsimage和editslog
    downloadCheckpointFiles(sig);   // Fetch fsimage and edits
    //合并editlog到fsimage
    doMerge(sig);                   // Do the merge

    //上传合并后的fsimage到NameNode
    putFSImage(sig);

    // error simulation code for junit test
    if (ErrorSimulator.getErrorSimulation(1)) {
      throw new IOException("Simulating error1 " +
                            "after uploading new image to NameNode");
    }
    // 通知NameNode使用该fsimage作为最新的镜像
    namenode.rollFsImage();
    checkpointImage.endCheckpoint();

    LOG.warn("Checkpoint done. New Image Size: " 
              + checkpointImage.getFsImageName().length());
  }
</code></pre>

<h4>1. startCheckpoint()</h4>

<p>该方法主要用于准备合并所需的磁盘空间，代码如下：</p>

<pre><code>  private void startCheckpoint() throws IOException {
    checkpointImage.unlockAll();
    // 关闭当前Editlog
    checkpointImage.getEditLog().close();
    // 检查当前checkpoints目录，如果不存在则创建一个新目录，如果目录中存在异常，则尝试恢复该目录
    checkpointImage.recoverCreate(checkpointDirs, checkpointEditsDirs);
    // 为新的checkpoint准备目录空间，将当前的目录空间更名为lastcheckpoint.™p，新建一个current目录 
    checkpointImage.startCheckpoint();
  }
</code></pre>

<h4>2. namenode.rollEditLog();</h4>

<p>namenode.rollEditLog()实际通过NameNodeProtocol调用NameNode.rollEditLog()方法，并最终调用FSImage.rollEditLog()，该方法主要完成：</p>

<ol>
<li>调用FSEditLog.rollEditLog()关闭当前editLog，新建一个editLog：edits.new</li>
<li>返回一个CheckpointSignature做为上传合并后镜像的token</li>
</ol>


<h5>2.1. FSEditLog.rollEditLog()</h5>

<p>该方法主要完成：</p>

<ol>
<li>关闭当前editlog， 打开一个新的editlog： edit.new</li>
<li>返回editlog的最新更新时间</li>
</ol>


<p>代码结构如下</p>

<pre><code>  synchronized void rollEditLog() throws IOException {
    //检查edit.new是否已经存在，如果存在，检查是否所有目录都存在，如果是则认为edits.new已经建好了，直接返回
    //
    if (existsNew()) {
      for (Iterator&lt;StorageDirectory&gt; it = 
               fsimage.dirIterator(NameNodeDirType.EDITS); it.hasNext();) {
        File editsNew = getEditNewFile(it.next());
     if (!editsNew.exists()) { 
          throw new IOException("Inconsistent existance of edits.new " +
                                editsNew);
        }
      }
      return; // nothing to do, edits.new exists!
    }

    //关闭当前的editLog
    close();                     // close existing edit log

    //
    // 新建一个editLog： edits.new
    //
    for (Iterator&lt;StorageDirectory&gt; it = 
           fsimage.dirIterator(NameNodeDirType.EDITS); it.hasNext();) {
      StorageDirectory sd = it.next();
      try {
        EditLogFileOutputStream eStream = 
             new EditLogFileOutputStream(getEditNewFile(sd));
        eStream.create();
        editStreams.add(eStream);
      } catch (IOException e) {
        // remove stream and this storage directory from list
        processIOError(sd);
       it.remove();
      }
    }
  }
</code></pre>

<h4>3. downloadCheckpointFiles()</h4>

<p>该方法用于从NameNode下载FSImage和FSEditLog，代码结构如下：</p>

<pre><code>  private void downloadCheckpointFiles(CheckpointSignature sig
                                      ) throws IOException {

    checkpointImage.cTime = sig.cTime;
    checkpointImage.checkpointTime = sig.checkpointTime;

    // 获取fsimage
    String fileid = "getimage=1";
    File[] srcNames = checkpointImage.getImageFiles();
    assert srcNames.length &gt; 0 : "No checkpoint targets.";
    TransferFsImage.getFileClient(fsName, fileid, srcNames);
    LOG.info("Downloaded file " + srcNames[0].getName() + " size " +
             srcNames[0].length() + " bytes.");

    // 获取editlog
    fileid = "getedit=1";
    srcNames = checkpointImage.getEditsFiles();
    assert srcNames.length &gt; 0 : "No checkpoint targets.";
    TransferFsImage.getFileClient(fsName, fileid, srcNames);
    LOG.info("Downloaded file " + srcNames[0].getName() + " size " +
        srcNames[0].length() + " bytes.");

    // 标示checkpoint所需文件已经准备完成
    checkpointImage.checkpointUploadDone();
  }
</code></pre>

<h4>4. doMerge()</h4>

<p>doMerge主要完成editLog与fsimage的合并，实际调用的checkpointImage.doMerge(sig);</p>

<pre><code>private void doMerge(CheckpointSignature sig) throws IOException {
  getEditLog().open();
  StorageDirectory sdName = null;
  StorageDirectory sdEdits = null;
  Iterator&lt;StorageDirectory&gt; it = null;
  it = dirIterator(NameNodeDirType.IMAGE);
  if (it.hasNext())
    sdName = it.next();
  it = dirIterator(NameNodeDirType.EDITS);
  if (it.hasNext())
    sdEdits = it.next();
  if ((sdName == null) || (sdEdits == null))
    throw new IOException("Could not locate checkpoint directories");
  // 加载fsimage
  loadFSImage(FSImage.getImageFile(sdName, NameNodeFile.IMAGE));
  // 加载editlog，并合并到fsimage

  loadFSEdits(sdEdits);
  // 校验新fsimage的一致性，主要包括版本，更新时间，namespaceId
  sig.validateStorageInfo(this);
  // 将fsImage存储到本地，并创建新edits
  saveFSImage();
}
</code></pre>

<p>该方法与NameNode启动时类似:</p>

<ol>
<li>加载fsiamge</li>
<li>加载editslog，并作用到fsimage中</li>
<li>校验合并后的fsimage</li>
<li>将新fsimage存储到本地，并新建空的edit是目录</li>
</ol>


<h4>5. putFSImage(sig);</h4>

<p>该方法比较简单，主要通过TransferFsImage工具类将合并后的fsimage上传到NameNode</p>

<pre><code>  private void putFSImage(CheckpointSignature sig) throws IOException {
    String fileid = "putimage=1&amp;port=" + infoPort +
      "&amp;machine=" +
      InetAddress.getLocalHost().getHostAddress() +
      "&amp;token=" + sig.toString();
    LOG.info("Posted URL " + fsName + fileid);
    TransferFsImage.getFileClient(fsName, fileid, (File[])null);
  }
</code></pre>

<h4>6. namenode.rollFsImage();</h4>

<p>上传完新fsiamge之后，SecondaryNameNode通过namenode.rollFsImage()通知NameNode使用新的fsimage.ckpt作为最新镜像，并清空editslog。该请求最终由NameNode上的FsImage.rollFsImage()处理，代码如下：</p>

<pre><code> void rollFSImage() throws IOException {
    if (ckptState != CheckpointStates.UPLOAD_DONE) {
      throw new IOException("Cannot roll fsImage before rolling edits log.");
    }
    //
    // 校验fsimage.ckpt和edits.new是否存在于所有目录
    if (!editLog.existsNew()) {
      throw new IOException("New Edits file does not exist");
    }
    for (Iterator&lt;StorageDirectory&gt; it = 
                       dirIterator(NameNodeDirType.IMAGE); it.hasNext();) {
      StorageDirectory sd = it.next();
      File ckpt = getImageFile(sd, NameNodeFile.IMAGE_NEW);
      if (!ckpt.exists()) {
        throw new IOException("Checkpoint file " + ckpt +
                              " does not exist");
      }
    }
    //删除旧的edits，并将edits.new重命名为edits
    editLog.purgeEditLog(); // renamed edits.new to edits

    //
    // 重命名fsimage
    for (Iterator&lt;StorageDirectory&gt; it = 
                       dirIterator(NameNodeDirType.IMAGE); it.hasNext();) {
      StorageDirectory sd = it.next();
      File ckpt = getImageFile(sd, NameNodeFile.IMAGE_NEW);
      File curFile = getImageFile(sd, NameNodeFile.IMAGE);
      // renameTo fails on Windows if the destination file 
      // already exists.
      if (!ckpt.renameTo(curFile)) {
        curFile.delete();
        if (!ckpt.renameTo(curFile)) {
          // Close edit stream, if this directory is also used for edits
          if (sd.getStorageDirType().isOfType(NameNodeDirType.EDITS))
            editLog.processIOError(sd);
        // add storage to the removed list
          removedStorageDirs.add(sd);
          it.remove();
        }
      }
    }

    //
    // 更新所有目录的fstime
    //
    this.layoutVersion = FSConstants.LAYOUT_VERSION;
    this.checkpointTime = FSNamesystem.now();
    for (Iterator&lt;StorageDirectory&gt; it = 
                           dirIterator(); it.hasNext();) {
      StorageDirectory sd = it.next();
      // delete old edits if sd is the image only the directory
      if (!sd.getStorageDirType().isOfType(NameNodeDirType.EDITS)) {
        File editsFile = getImageFile(sd, NameNodeFile.EDITS);
        editsFile.delete();
      }
      // delete old fsimage if sd is the edits only the directory
      if (!sd.getStorageDirType().isOfType(NameNodeDirType.IMAGE)) {
        File imageFile = getImageFile(sd, NameNodeFile.IMAGE);
        imageFile.delete();
      }
      try {
        sd.write();
      } catch (IOException e) {
        LOG.error("Cannot write file " + sd.getRoot(), e);
        // Close edit stream, if this directory is also used for edits
        if (sd.getStorageDirType().isOfType(NameNodeDirType.EDITS))
          editLog.processIOError(sd);
      //add storage to the removed list
        removedStorageDirs.add(sd);
        it.remove();
      }
    }
    ckptState = FSImage.CheckpointStates.START;
  }
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS源码学习（10）——NameNode与DataNode间的通信]]></title>
    <link href="http://jiangbo.me/blog/2012/10/18/hdfs-namenode-and-datanode-communication/"/>
    <updated>2012-10-18T21:54:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/18/hdfs-namenode-and-datanode-communication</id>
    <content type="html"><![CDATA[<p>NameNode和DataNode间的通信分为四种场景：</p>

<ol>
<li>初始时DataNode注册：</li>
<li>周期性心跳检测：</li>
<li>周期性blockreport：</li>
<li>完成一个副本的写入：</li>
</ol>


<h2>一、初始时DataNode注册</h2>

<p>DataNode在启动时会向NameNode注册，注册时需要提交的信息有DatanodeRegistration表示。结构如下：</p>

<p><img src="http://jiangbo.me/images/hdfs/DatanodeRegistration.png" alt="DatanodeRegistration" /></p>

<p>主要包括：</p>

<ol>
<li>name：机器名（主机名+服务端口号）</li>
<li>infoPort: 状态信息服务端口好</li>
<li>ipcPort： 提供ipc服务的端口号</li>
</ol>


<p>此外，该类中的storageID是该datanode在集群中的唯一id，在注册时有NameNode分配</p>

<p>注册的主要流程如下：
<img src="http://jiangbo.me/images/hdfs/register.png" alt="DataNodeRegister" /></p>

<h2>二、心跳检测（heartbeat）</h2>

<p>DataNode通过周期性调用namenode.sendHeartbeat()来完成心跳检测.主要流程如下：</p>

<p><img src="http://jiangbo.me/images/hdfs/sendHeartbeat.png" alt="sendHeartbeat" /></p>

<h2>三、blockReport</h2>

<p>DataNode周期性向NameNode发送blockReport，告知自己最新的block信息：
<img src="http://jiangbo.me/images/hdfs/blockReport.png" alt="blockReport" /></p>

<h2>四、完成副本写入</h2>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS源码学习（9）——安全模式（SafeMode）]]></title>
    <link href="http://jiangbo.me/blog/2012/10/18/hdfs-namenode-safe-mode/"/>
    <updated>2012-10-18T21:52:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/18/hdfs-namenode-safe-mode</id>
    <content type="html"><![CDATA[<h2>一、SafeModeInfo</h2>

<p>SafeModeInfo维护了系统安全模式下的状态信息，每当系统进入安全模式时都会创建一个SafeModeInfo实例维护状态信息，离开时会销毁这个实例。
该类结构如下</p>

<p><img src="http://jiangbo.me/images/hdfs/SafeModeInfo.png" alt="SafeModeInfo" /></p>

<p>其中threshold和extension为可配置项：</p>

<ol>
<li>threshold表示离开安全模式时打到最低备份数的block的比例</li>
<li>extension表示进入安全模式的最低时长</li>
</ol>


<h2>二、SafeModeMonitor</h2>

<p>FSNameSystem中SafeModeMonitor代码结构如下：</p>

<pre><code>  class SafeModeMonitor implements Runnable {
    /** interval in msec for checking safe mode: {@value} */
    private static final long recheckInterval = 1000;

    /**
     */
    public void run() {
      while (fsRunning &amp;&amp; (safeMode != null &amp;&amp; !safeMode.canLeave())) {
        try {
          Thread.sleep(recheckInterval);
        } catch (InterruptedException ie) {
        }
      }
      // leave safe mode and stop the monitor
      try {
        leaveSafeMode(true);
      } catch(SafeModeException es) { // should never happen
        String msg = "SafeModeMonitor may not run during distributed upgrade.";
        assert false : msg;
        throw new RuntimeException(msg, es);
      }
      smmthread = null;
    }
  }
</code></pre>

<p>其核心就是每个1秒检测一次是否能够离开模式（safeMode.canLeave()），如果可以，则尝试离开并停止SafeModeMonitor线程（leaveSafeMode(true)）</p>

<h3>1.1. 是否能离开 —— safeMode.canLeave()</h3>

<p>能够离开安全模式的标准是：
1. 已进入安全模式的时长大于等于 extension
2. 安全的block数比例打到门槛值</p>

<pre><code>synchronized boolean canLeave() {
  if (reached == 0)
    return false;
  if (now() - reached &lt; extension) {
    reportStatus("STATE* Safe mode ON.", false);
    return false;
  }
  return !needEnter();
}

/** 
 * There is no need to enter safe mode 
 * if DFS is empty or {@link #threshold} == 0
 */
boolean needEnter() {
  return getSafeBlockRatio() &lt; threshold;
}
</code></pre>

<h3>1.2. 离开安全模式 —— leaveSafeMode(true);</h3>

<pre><code>  public void leaveSafeMode(boolean checkForUpgrades) throws SafeModeException {
    writeLock();
    try {
    if (!isInSafeMode()) {
      NameNode.stateChangeLog.info("STATE* Safe mode is already OFF."); 
      return;
    }
    //获取升级状态，如在升级中，不能离开安全模式
    if(getDistributedUpgradeState())
      throw new SafeModeException("Distributed upgrade is in progress",
                                  safeMode);
    //调用SafeModeInfo.leave()离开安全模式
    safeMode.leave(checkForUpgrades);
    } finally {
      writeUnlock();
    }
  }
</code></pre>

<h3>1.2.1 SafeModeInfo.leave()</h3>

<pre><code>    synchronized void leave(boolean checkForUpgrades) {
      if(checkForUpgrades) {
        // 验证是否需要升级
        boolean needUpgrade = false;
        try {
          needUpgrade = startDistributedUpgradeIfNeeded();
        } catch(IOException e) {
          FSNamesystem.LOG.error(StringUtils.stringifyException(e));
        }
        if(needUpgrade) {
          //如果需要升级，进入手动安全模式
          safeMode = new SafeModeInfo();
          return;
        }
      }
      // 如果备份队列未初始化完，继续初始化该队列
      if (!isPopulatingReplQueues()) {
        initializeReplQueues();
      }
      long timeInSafemode = now() - systemStart;
      NameNode.stateChangeLog.info("STATE* Leaving safe mode after " 
                                    + timeInSafemode/1000 + " secs.");
      NameNode.getNameNodeMetrics().safeModeTime.set((int) timeInSafemode);

      if (reached &gt;= 0) {
        NameNode.stateChangeLog.info("STATE* Safe mode is OFF."); 
      }
      reached = -1;
      safeMode = null;
      NameNode.stateChangeLog.info("STATE* Network topology has "
                                   +clusterMap.getNumOfRacks()+" racks and "
                                   +clusterMap.getNumOfLeaves()+ " datanodes");
      NameNode.stateChangeLog.info("STATE* UnderReplicatedBlocks has "
                                   +neededReplications.size()+" blocks");
    }
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS源码学习（8）——Backup Mode]]></title>
    <link href="http://jiangbo.me/blog/2012/10/18/hdfs-namenode-backup-mode/"/>
    <updated>2012-10-18T21:51:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/18/hdfs-namenode-backup-mode</id>
    <content type="html"><![CDATA[<h2>元数据的持久化</h2>

<p>HDFS通过主要通过FSImage和FSEditLog来完成文件元数据的持久化。对文件系统的任何修改，NameNode都会通过Editlog记录下来，持久化到本地。同时整个系统的命名空间，所有的文件元信息均保存在FSImage中，包括block->File的映射，文件的属性等等。NameNode启动时会从本地磁盘加载FSImage和FSEditLog，并将EditLog中的日志信息合并到FSImage中进行之持久化（该合并过程称为一个检查点：checkpoint），并构建文件系统的元信息。</p>

<p>但是持久化的数据中不包括block<->datanode的映射信息，该信息由每个datanode向NameNode发起blockReport()请求时报告其所拥有的block信息。</p>

<h3>Editlog记录修改日志</h3>

<p>EditLog的持久化文件是一个二进制文件，大体结构如下：</p>

<p><img src="http://jiangbo.me/images/hdfs/EditLogFile.png" alt="EditLogFile" /></p>

<p>EditLog文件开始是一个日志版本号，0.19版本的hdfs中该version为-18。
随后是每一条操作的事务日志，每条日志的起始为一个操作类型位，随后是该操作的详细信息，不同的操作类型所带的详细信息也不同。加载EditLog是根据layoutVersion和edit_op位采取不同的方式解析后面的详细信息。
EditLog能够记录如下17中操作：</p>

<p><img src="http://jiangbo.me/images/hdfs/EditLogOp.png" alt="EditLogOp" /></p>

<p>EditLog为每种操作都提供了相应的log方法，当系统中发生文件修改时，会调用相应的log方法记录日志</p>

<h2>元数据加载与恢复</h2>

<p>NameNode启动过程中最终会通过FSImage.loadFSImage()来从fsimage目录中加载最新的fsimage镜像和editslog，并合并构建命名空间。
代码结构如下：</p>

<pre><code>  boolean loadFSImage() throws IOException {
    // 根据checkpointtime查找最新的fsimage
    long latestNameCheckpointTime = Long.MIN_VALUE;
    long latestEditsCheckpointTime = Long.MIN_VALUE;
    StorageDirectory latestNameSD = null;
    StorageDirectory latestEditsSD = null;
    boolean needToSave = false;
    isUpgradeFinalized = true;
    Collection&lt;String&gt; imageDirs = new ArrayList&lt;String&gt;();
    Collection&lt;String&gt; editsDirs = new ArrayList&lt;String&gt;();
    for (Iterator&lt;StorageDirectory&gt; it = dirIterator(); it.hasNext();) {
      StorageDirectory sd = it.next();
      if (!sd.getVersionFile().exists()) {
        needToSave |= true;
        continue; // some of them might have just been formatted
      }
      boolean imageExists = false, editsExists = false;
      if (sd.getStorageDirType().isOfType(NameNodeDirType.IMAGE)) {
        imageExists = getImageFile(sd, NameNodeFile.IMAGE).exists();
        imageDirs.add(sd.getRoot().getCanonicalPath());
      }
      if (sd.getStorageDirType().isOfType(NameNodeDirType.EDITS)) {
        editsExists = getImageFile(sd, NameNodeFile.EDITS).exists();
        editsDirs.add(sd.getRoot().getCanonicalPath());
      }

      checkpointTime = readCheckpointTime(sd);
      if ((checkpointTime != Long.MIN_VALUE) &amp;&amp; 
          ((checkpointTime != latestNameCheckpointTime) || 
           (checkpointTime != latestEditsCheckpointTime))) {
        // Force saving of new image if checkpoint time
        // is not same in all of the storage directories.
        needToSave |= true;
      }
      if (sd.getStorageDirType().isOfType(NameNodeDirType.IMAGE) &amp;&amp; 
         (latestNameCheckpointTime &lt; checkpointTime) &amp;&amp; imageExists) {
        latestNameCheckpointTime = checkpointTime;
        latestNameSD = sd;
      }
      if (sd.getStorageDirType().isOfType(NameNodeDirType.EDITS) &amp;&amp; 
           (latestEditsCheckpointTime &lt; checkpointTime) &amp;&amp; editsExists) {
        latestEditsCheckpointTime = checkpointTime;
        latestEditsSD = sd;
      }
      if (checkpointTime &lt;= 0L)
        needToSave |= true;
      // set finalized flag
      isUpgradeFinalized = isUpgradeFinalized &amp;&amp; !sd.getPreviousDir().exists();
    }

    // 确保至少有一个fsimage和一个edits目录
    if (latestNameSD == null)
      throw new IOException("Image file is not found in " + imageDirs);
    if (latestEditsSD == null)
      throw new IOException("Edits file is not found in " + editsDirs);

    // 确保获得的fsimage和edits是同一个检查点
    if (latestNameCheckpointTime &gt; latestEditsCheckpointTime
        &amp;&amp; latestNameSD != latestEditsSD
        &amp;&amp; latestNameSD.getStorageDirType() == NameNodeDirType.IMAGE
        &amp;&amp; latestEditsSD.getStorageDirType() == NameNodeDirType.EDITS) {
      // This is a rare failure when NN has image-only and edits-only
      // storage directories, and fails right after saving images,
      // in some of the storage directories, but before purging edits.
      // See -NOTE- in saveNamespace().
      LOG.error("This is a rare failure scenario!!!");
      LOG.error("Image checkpoint time " + latestNameCheckpointTime +
          " &gt; edits checkpoint time " + latestEditsCheckpointTime);
      LOG.error("Name-node will treat the image as the latest state of " +
          "the namespace. Old edits will be discarded.");
    } else if (latestNameCheckpointTime != latestEditsCheckpointTime)
      throw new IOException("Inconsitent storage detected, " +
          "image and edits checkpoint times do not match. " +
          "image checkpoint time = " + latestNameCheckpointTime +
          "edits checkpoint time = " + latestEditsCheckpointTime);

    // 如果上次检查点中断了，则恢复该检查点
    needToSave |= recoverInterruptedCheckpoint(latestNameSD, latestEditsSD);

    long startTime = FSNamesystem.now();
    long imageSize = getImageFile(latestNameSD, NameNodeFile.IMAGE).length();

    //
    // 加载fsimage文件
    //
    latestNameSD.read();
    needToSave |= loadFSImage(getImageFile(latestNameSD, NameNodeFile.IMAGE));
    LOG.info("Image file of size " + imageSize + " loaded in " 
        + (FSNamesystem.now() - startTime)/1000 + " seconds.");

    // 加载最新的edits并作用于fsimage上
    if (latestNameCheckpointTime &gt; latestEditsCheckpointTime)
      // the image is already current, discard edits
      needToSave |= true;
    else // latestNameCheckpointTime == latestEditsCheckpointTime
      needToSave |= (loadFSEdits(latestEditsSD) &gt; 0);

    return needToSave;
  }
</code></pre>

<p>其中FSImage.loadFSImage(File curFile) 代码如下：</p>

<pre><code>  boolean loadFSImage(File curFile) throws IOException {
    assert this.getLayoutVersion() &lt; 0 : "Negative layout version is expected.";
    assert curFile != null : "curFile is null";

    FSNamesystem fsNamesys = FSNamesystem.getFSNamesystem();
    FSDirectory fsDir = fsNamesys.dir;

    //
    // Load in bits
    //
    boolean needToSave = true;
    DataInputStream in = new DataInputStream(new BufferedInputStream(
                              new FileInputStream(curFile)));
    try {
      /*
       * Note: Remove any checks for version earlier than 
       * Storage.LAST_UPGRADABLE_LAYOUT_VERSION since we should never get 
       * to here with older images.
       */

      /*
       * TODO we need to change format of the image file
       * it should not contain version and namespace fields
       */
      // 读取imageversion
      int imgVersion = in.readInt();
      // 读取namespaceid
      this.namespaceID = in.readInt();

      // 读取镜像中的文件数
      long numFiles;
      if (imgVersion &lt;= -16) {
        numFiles = in.readLong();
      } else {
        numFiles = in.readInt();
      }

      this.layoutVersion = imgVersion;
      // 读取镜像时间戳
      if (imgVersion &lt;= -12) {
        long genstamp = in.readLong();
        fsNamesys.setGenerationStamp(genstamp); 
      }

      needToSave = (imgVersion != FSConstants.LAYOUT_VERSION);

      // 读取每个文件的信息
      short replication = FSNamesystem.getFSNamesystem().getDefaultReplication();

      LOG.info("Number of files = " + numFiles);

      byte[][] pathComponents;
      byte[][] parentPath = ;
      INodeDirectory parentINode = fsDir.rootDir;
      for (long i = 0; i &lt; numFiles; i++) {
        long modificationTime = 0;
        long atime = 0;
        long blockSize = 0;
        //读取文件名(path)
        pathComponents = readPathComponents(in);
        //读取副本数
        replication = in.readShort();
        //调整副本数，使其不超过系统的最大和最小副本数限制
        replication = FSEditLog.adjustReplication(replication);
        //读取文件修改时间
        modificationTime = in.readLong();
        if (imgVersion &lt;= -17) {
        //读取最近访问时间
          atime = in.readLong();
        }
        if (imgVersion &lt;= -8) {
        //读取block块大小
          blockSize = in.readLong();
        }
        //读取block数
        int numBlocks = in.readInt();
        //构建blocks
        Block blocks[] = null;

        // 老版本hdfs中，numBlocks=0表示目录，新版本中numBlocks=-1表示目录
        if ((-9 &lt;= imgVersion &amp;&amp; numBlocks &gt; 0) ||
            (imgVersion &lt; -9 &amp;&amp; numBlocks &gt;= 0)) {
           //构建文件block信息
          blocks = new Block[numBlocks];
          for (int j = 0; j &lt; numBlocks; j++) {
            blocks[j] = new Block();
            if (-14 &lt; imgVersion) {
              blocks[j].set(in.readLong(), in.readLong(), 
                            Block.GRANDFATHER_GENERATION_STAMP);
            } else {
              // 读取block信息
              blocks[j].readFields(in);
            }
          }
        }
        // 老版本inode中不维护blocksize，如果存在多个block，blocksize选取第一个block的大小，如果只有一个block，则选该block大小和默认大小中较大的，如果没有block，则选用默认大小
        if (-8 &lt;= imgVersion &amp;&amp; blockSize == 0) {
          if (numBlocks &gt; 1) {
            blockSize = blocks[0].getNumBytes();
          } else {
            long first = ((numBlocks == 1) ? blocks[0].getNumBytes(): 0);
            blockSize = Math.max(fsNamesys.getDefaultBlockSize(), first);
          }
        }

        // 如果是目录（blocks=null），读取该目录的配额
        long nsQuota = -1L;
        if (imgVersion &lt;= -16 &amp;&amp; blocks == null) {
          nsQuota = in.readLong();
        }
        long dsQuota = -1L;
        if (imgVersion &lt;= -18 &amp;&amp; blocks == null) {
          dsQuota = in.readLong();
        }

        //获取权限信息
        PermissionStatus permissions = fsNamesys.getUpgradePermission();
        if (imgVersion &lt;= -11) {
          permissions = PermissionStatus.read(in);
        }

        //如果该path为root，且设置了配额信息，则更新根目录的配额
        if (isRoot(pathComponents)) { // it is the root
          // update the root's attributes
          if (nsQuota != -1 || dsQuota != -1) {
            fsDir.rootDir.setQuota(nsQuota, dsQuota);
          }
          fsDir.rootDir.setModificationTime(modificationTime);
          fsDir.rootDir.setPermissionStatus(permissions);
          continue;
        }
        //如果该inode的parent与当前路径不一至，获取新的parentPaht
        if(!isParent(pathComponents, parentPath)) {
          parentINode = null;
          parentPath = getParent(pathComponents);
        }
        // 将该inode添加到inode树中
        parentINode = fsDir.addToParent(pathComponents, parentINode, permissions,
                                        blocks, replication, modificationTime, 
                                        atime, nsQuota, dsQuota, blockSize);
      }

      // 加载datanode信息，imgVersion&lt;-12的版本事实上啥都不做，datanode信息已经不保存在fsimage中
      this.loadDatanodes(imgVersion, in);

      // 加载正在构建中的文件
      this.loadFilesUnderConstruction(imgVersion, in, fsNamesys);

    } finally {
      in.close();
    }

    return needToSave;
  }
</code></pre>

<p>其核心就是加载并解析fsimage文件，构建命名空间，fsimage文件的结构参见《NameNode中主要数据结构》</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS源码学习（7）——Block管理]]></title>
    <link href="http://jiangbo.me/blog/2012/10/18/hdfs-namenode-block-management/"/>
    <updated>2012-10-18T21:46:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/18/hdfs-namenode-block-management</id>
    <content type="html"><![CDATA[<p>HDFS通过一个BlockManager管理集群中所有的block信息</p>

<h2>主要数据结构</h2>

<h3>Block</h3>

<p>Block是HDFS读写的基本单元，集群中每个block通过一个long id来唯一标示。</p>

<h3>BlockInfo</h3>

<p>维护一个block的元信息，主要通过</p>

<h3>BlockMap</h3>

<p>通过一个GSet&lt;Block, BlockInfo>维护一个block与其元数据信息的映射关系，元信息包括其所属的BlockCollection和存储该block的datanode节点，每个BlockMap有个初始容量capacity</p>

<h3>BlockCollection</h3>

<h2>Block和副本管理</h2>

<h3>Block和副本状态</h3>

<p>Block有如下状态：</p>

<ol>
<li>committed：所有的副本已经被创建且更新至最新</li>
<li>Under construction: 需要创建一个或多个副本</li>
<li>To be deleted: 所有副本需要被删除。发生在文件被删除或者block被重写</li>
<li>Over-replicated: 过多的副本存在。此时副本中的一个需要设置为无效并删除。</li>
</ol>


<p>副本有如下状态：</p>

<ol>
<li>Current: 正常状态，该副本正确反应block内容</li>
<li>Conrrupt: 某个副本损坏。副本损坏是由client报告给namenode的。client通过checksum检查副本是否损坏，如果损坏了，通过BlockManager.invalidateBlock()处理</li>
<li>On a faild DataNode: DataNode Heartbeat发现有DataNode失效时，即将在改datanode上创建的副本将被删除</li>
<li>Out of Date: 当Datanode失效，且副本所属的block发生更新后，Datanode恢复正常。过期的block将通过blockreport报告给namenode，并将其删除</li>
<li>Under construction: 副本尚未被写入并在Datanode上被验证。在NameNode看来，只有当收到blockReport并且报告中timestamp正确时，猜人物副本写入正常。</li>
</ol>


<h2>Block分配</h2>

<h2>Block查询</h2>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS源码学习（6）——租约管理（lease management)]]></title>
    <link href="http://jiangbo.me/blog/2012/10/18/hdfs-namenode-lease-management/"/>
    <updated>2012-10-18T21:44:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/18/hdfs-namenode-lease-management</id>
    <content type="html"><![CDATA[<p>LeaseManagement是HDFS中的一个同步机制，用于保证同一时刻只有一个client对一个文件进行写或创建操作。如当新建一个文件f时，client向NameNode发起一个create请求，那么leaseManager会想该client分配一个f文件的lease。client凭借该lease完成文件的创建操作。此时其他client无法获得f的当client长时间（默认为超过1min）不进行操作时，发放的lease将被收回。</p>

<p>LeaseManager主要完成两部分工作：</p>

<ol>
<li>文件create，write，complete操作时，创建lease、更新时间戳、回收lease</li>
<li>一个后台线程定期检查是否有过期的lease</li>
</ol>


<p>LeaseManager的代码结构如下</p>

<p><img src="http://jiangbo.me/images/hdfs/LeaseManager.png" alt="LeaseManager" /></p>

<p>其中Lease表示一个租约，包括一个client(holder)所拥有的所有文件锁(paths)。</p>

<p>Monitor是检查是否有过期租约的线程。</p>

<p>LeaseManager中有几个主要数据结构：</p>

<ol>
<li>leases（TreeMap&lt;String, Lease>）：维护holder -> leased的映射集合</li>
<li>sortedLeases (TreeSet<Lease>): lease集合</li>
<li>sortedLeaseByPath(TreeMap&lt;String, Lease>): 维护paths->lease的映射集合</li>
</ol>


<h2>一、创建lease</h2>

<p>当client向NameNode发起create操作时，NameNode.create()调用FSNameSystem.startFile()->FSNameSystem.startFileInternal()，该方法最终会调用leaseManager.addLease(cons.clientName, src)来创建lease。</p>

<p>LeaseManager.addLease()方法如下：</p>

<pre><code>  synchronized Lease addLease(String holder, String src
      ) throws IOException {
    Lease lease = getLease(holder);
    if (lease == null) {
      lease = new Lease(holder);
      leases.put(holder, lease);
      sortedLeases.add(lease);
    } else {
      renewLease(lease);
    }
    sortedLeasesByPath.put(src, lease);
    lease.paths.add(src);
    return lease;
  }
</code></pre>

<p>代码结构简单：判断该client是否有lease，没有则新建一个lease，并将起加到leases集合中。否则更新lease。更新sortedLeasesByPath，将filepath加入到该lease的paths集合中</p>

<h2>二、更新时间戳</h2>

<p>针对已经存在的lease，通过LeasemManager.renewLease()来更新该lease的时间戳。代码如下：</p>

<pre><code>  synchronized void renewLease(Lease lease) {
    if (lease != null) {
      sortedLeases.remove(lease);
      lease.renew();
      sortedLeases.add(lease);
    }
  }
</code></pre>

<p>lease.renew()代码如下：</p>

<pre><code>/** Only LeaseManager object can renew a lease */
private void renew() {
  this.lastUpdate = FSNamesystem.now();
}
</code></pre>

<h2>三、compelete时回收lease</h2>

<p>当client调用NameNode.complete()方法时，最终会调用FSNameSystem.completeFileInternal()方法。其中执行finalizeINodeFileUnderConstruction()是调用leaseManager.removeLease()释放lease。</p>

<p>代码结构如下：</p>

<pre><code>  synchronized void removeLease(String holder, String src) {
    Lease lease = getLease(holder);
    if (lease != null) {
      removeLease(lease, src);
    }
  }
</code></pre>

<p> removeLease(lease, src);代码如下：</p>

<pre><code>  /**
   * Remove the specified lease and src.
   */
  synchronized void removeLease(Lease lease, String src) {
    sortedLeasesByPath.remove(src);
    if (!lease.removePath(src)) {
      LOG.error(src + " not found in lease.paths (=" + lease.paths + ")");
    }

    if (!lease.hasPath()) {
      leases.remove(lease.holder);
      if (!sortedLeases.remove(lease)) {
        LOG.error(lease + " not found in sortedLeases");
      }
    }
  }
</code></pre>

<h2>四、后台线程回收过期lease</h2>

<p>Monitor回收lease线程代码结构如下：</p>

<pre><code> class Monitor implements Runnable {
    final String name = getClass().getSimpleName();

    /** Check leases periodically. */
    public void run() {
      for(; fsnamesystem.isRunning(); ) {
        fsnamesystem.writeLock();
        try {
          checkLeases();
        } finally {
          fsnamesystem.writeUnlock();
        }

        try {
          Thread.sleep(2000);
        } catch(InterruptedException ie) {
          if (LOG.isDebugEnabled()) {
            LOG.debug(name + " is interrupted", ie);
          }
        }
      }
    }
  }
</code></pre>

<p>代码结构简单，每个2s周期性执行checkLeases()。</p>

<h3>4.1 checkLeases()</h3>

<pre><code>  /** Check the leases beginning from the oldest. */
  synchronized void checkLeases() {
    for(; sortedLeases.size() &gt; 0; ) {
      final Lease oldest = sortedLeases.first();
      if (!oldest.expiredHardLimit()) {
        return;
      }

      LOG.info("Lease " + oldest + " has expired hard limit");

      final List&lt;String&gt; removing = new ArrayList&lt;String&gt;();
      // need to create a copy of the oldest lease paths, becuase 
      // internalReleaseLease() removes paths corresponding to empty files,
      // i.e. it needs to modify the collection being iterated over
      // causing ConcurrentModificationException
      String[] leasePaths = new String[oldest.getPaths().size()];
      oldest.getPaths().toArray(leasePaths);
      for(String p : leasePaths) {
        try {
          fsnamesystem.internalReleaseLeaseOne(oldest, p);
        } catch (IOException e) {
          LOG.error("Cannot release the path "+p+" in the lease "+oldest, e);
          removing.add(p);
        }
      }

      for(String p : removing) {
        removeLease(oldest, p);
      }
    }
  }
</code></pre>

<h2>Lease Recovery ——租约回收</h2>

<h3>lease recovery时机</h3>

<p>lease发放之后，在不用时会被回收，回收的产经除上述Monitor线程检测lease过期是回收外，还有：</p>

<ol>
<li>NameNode收到DataNode的Sync block command时</li>
<li>DFSClient主动关闭一个流时</li>
<li>创建文件时，如果该DFSClient的lease超过soft limit时</li>
</ol>


<h3>lease recovery 算法</h3>

<p>1) NameNode查找lease信息</p>

<p>2) 对于lease中的每个文件f，令b为f的最后一个block，作如下操作：</p>

<p>2.1) 获取b所在的datanode列表</p>

<p>2.2) 令其中一个datanode作为primary datanode p</p>

<p>2.3) p 从NameNode获取最新的时间戳</p>

<p>2.4) p 从每个DataNode获取block信息</p>

<p>2.5) p 计算最小的block长度</p>

<p>2.6) p 用最小的block长度和最新的时间戳来更新具有有效时间戳的datanode</p>

<p>2.7) p 通知NameNode更新结果</p>

<p>2.8) NameNode更新BlockInfo</p>

<p>2.9) NameNode从lease中删除f，如果此时该lease中所有文件都已被删除，将删除该lease</p>

<p>2.10) Name提交修改的EditLog</p>

<h2>Client续约 —— DFSClient.LeaseChecker</h2>

<p>在NameNode上的LeaseManager.Monitor线程负责检查过期的lease，那么client为了防止尚在使用的lease过期，需要定期想NameNode发起续约请求。该任务有DFSClient中的LeaseChecker完成。</p>

<p>LeaseChecker结构如下：</p>

<p><img src="http://jiangbo.me/images/hdfs/LeaseChecker.png" alt="LeaseChecker" /></p>

<p>其中pendingCreates是一个TreeMap&lt;String, OutputStream>用来维护src->当前正在写入的文件的DFSOutputStream的映射。</p>

<p>其核心是周期性（每个1s）调用run()方法来对租约过半的lease进行续约</p>

<pre><code>public void run() {
  long lastRenewed = 0;
  while (clientRunning &amp;&amp; !Thread.interrupted()) {
    //当租约周期过半时需要进行续约
    if (System.currentTimeMillis() - lastRenewed &gt; (LEASE_SOFTLIMIT_PERIOD / 2)) {
      try {
        renew();
        lastRenewed = System.currentTimeMillis();
      } catch (IOException ie) {
        LOG.warn("Problem renewing lease for " + clientName, ie);
      }
    }

    try {
      Thread.sleep(1000);
    } catch (InterruptedException ie) {
      if (LOG.isDebugEnabled()) {
        LOG.debug(this + " is interrupted.", ie);
      }
      return;
    }
  }
}
</code></pre>

<p>其中renew()方法如下：</p>

<pre><code>    private void renew() throws IOException {
      synchronized(this) {
        //如果当前创建中的文件列表为空，则不需要续约
        if (pendingCreates.isEmpty()) {
          return;
        }
      }
      //向NameNode发起续约请求
      namenode.renewLease(clientName);
    }
</code></pre>

<p>NameNode接收到renewLease请求后，调用FSNameSystem.renewLease()并最终调用LeaseManager.renewLease()完成续约。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS源码学习（5）——副本管理（Replica Management)]]></title>
    <link href="http://jiangbo.me/blog/2012/10/18/hdfs-namenode-replica-management/"/>
    <updated>2012-10-18T21:43:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/18/hdfs-namenode-replica-management</id>
    <content type="html"><![CDATA[<p>HDFS中的副本管理通过FSNameSystem.java中的ReplicationMonitor线程来完成。该线程代码结构较为简单。</p>

<pre><code>  class ReplicationMonitor implements Runnable {
    static final int INVALIDATE_WORK_PCT_PER_ITERATION = 32;
    static final float REPLICATION_WORK_MULTIPLIER_PER_ITERATION = 2;
    public void run() {
      while (fsRunning) {
        try {
          computeDatanodeWork();
          processPendingReplications();
          Thread.sleep(replicationRecheckInterval);
        } catch (InterruptedException ie) {
          LOG.warn("ReplicationMonitor thread received InterruptedException.", ie);
          break;
        } catch (IOException ie) {
          LOG.warn("ReplicationMonitor thread received exception. " + ie +  " " +
              StringUtils.stringifyException(ie));
        } catch (Throwable t) {
          LOG.fatal("ReplicationMonitor thread received Runtime exception. " + t + " " +
              StringUtils.stringifyException(t));
          Runtime.getRuntime().exit(-1);
        }
      }
    }
  }
</code></pre>

<p>该线程只是周期性调用computeDatanodeWork()和processPendingReplications()。</p>

<h2>一、computeDatanodeWork()</h2>

<pre><code>  public int computeDatanodeWork() throws IOException {
    int workFound = 0;
    int blocksToProcess = 0;
    int nodesToProcess = 0;
    // blocks should not be replicated or removed if safe mode is on
    if (isInSafeMode())
      return workFound;
    //计算需要备份的block数和节点数
    synchronized(heartbeats) {
      blocksToProcess = (int)(heartbeats.size() 
          * ReplicationMonitor.REPLICATION_WORK_MULTIPLIER_PER_ITERATION);
      nodesToProcess = (int)Math.ceil((double)heartbeats.size() 
          * ReplicationMonitor.INVALIDATE_WORK_PCT_PER_ITERATION / 100);
    }
    //执行备份
    workFound = computeReplicationWork(blocksToProcess); 

    // Update FSNamesystemMetrics counters
    pendingReplicationBlocksCount = pendingReplications.size();
    underReplicatedBlocksCount = neededReplications.size();
    scheduledReplicationBlocksCount = workFound;
    corruptReplicaBlocksCount = corruptReplicas.size();

    //HADOOP-5549 : Fix bug of schedule both replication and deletion work in one iteration
    workFound += computeInvalidateWork(nodesToProcess);
    return workFound;
  }
</code></pre>

<h3>1.1. computeReplicationWork()</h3>

<pre><code>  private int computeReplicationWork(
                                  int blocksToProcess) throws IOException {
    // stall only useful for unit tests (see TestFileAppend4.java)
    if (stallReplicationWork)  {
      return 0;
    }

    // 选取需要备份的block
    List&lt;List&lt;Block&gt;&gt; blocksToReplicate =
      chooseUnderReplicatedBlocks(blocksToProcess);

    // 执行备份
    return computeReplicationWorkForBlocks(blocksToReplicate);
  }
</code></pre>

<h3>1.1.1 选取需要备份的block —— chooseUnderReplicatedBlocks()</h3>

<pre><code>  List&lt;List&lt;Block&gt;&gt; chooseUnderReplicatedBlocks(int blocksToProcess) {
    // 初始化返回值数据结构，返回值是一个二维优先级列表
    List&lt;List&lt;Block&gt;&gt; blocksToReplicate =
      new ArrayList&lt;List&lt;Block&gt;&gt;(UnderReplicatedBlocks.LEVEL);
    for (int i = 0; i &lt; UnderReplicatedBlocks.LEVEL; i++) {
      blocksToReplicate.add(new ArrayList&lt;Block&gt;());
    }

    writeLock();
    try {
      synchronized (neededReplications) {
        if (neededReplications.size() == 0) {
          return blocksToReplicate;
        }

        for (int priority = 0; priority&lt;UnderReplicatedBlocks.LEVEL; priority++) {
        //遍历所有需要备份的block列表（UnderReplicatedBlocks结构）
        BlockIterator neededReplicationsIterator = neededReplications.iterator(priority);
        int numBlocks = neededReplications.size(priority);
        //检查该优先级列表中是否已经开始备份（relIndex数组中保存的是当前每个优先级列表中已备份的block索引）
        if (replIndex[priority] &gt; numBlocks) {
          replIndex[priority] = 0;
        }
        // skip to the first unprocessed block, which is at replIndex
        for (int i = 0; i &lt; replIndex[priority] &amp;&amp; neededReplicationsIterator.hasNext(); i++) {
          neededReplicationsIterator.next();
        }
        // 计算该优先级下需要备份的block数，低优先级的block备份数不超过总配额的20%
        int blocksToProcessIter = getQuotaForThisPriority(blocksToProcess,
            numBlocks, neededReplications.getSize(priority+1));
        blocksToProcess -= blocksToProcessIter;

        //便利改优先级列表将该优先级下的block添加到返回值中
        for (int blkCnt = 0; blkCnt &lt; blocksToProcessIter; blkCnt++, replIndex[priority]++) {
          if (!neededReplicationsIterator.hasNext()) {
            // start from the beginning
            replIndex[priority] = 0;
            neededReplicationsIterator = neededReplications.iterator(priority);
            assert neededReplicationsIterator.hasNext() :
              "neededReplications should not be empty.";
          }

          Block block = neededReplicationsIterator.next();
          blocksToReplicate.get(priority).add(block);
        } // end for
        }
      } // end try
      return blocksToReplicate;
    } finally {
      writeUnlock();
    }
  }
</code></pre>

<h3>1.1.2. 备份blocks —— computeReplicationWorkForBlocks（）</h3>

<pre><code>  int computeReplicationWorkForBlocks(List&lt;List&lt;Block&gt;&gt; blocksToReplicate) {
    int requiredReplication, numEffectiveReplicas, priority;
    List&lt;DatanodeDescriptor&gt; containingNodes;
    DatanodeDescriptor srcNode;
    INodeFile fileINode = null;

    int scheduledWork = 0;
    List&lt;ReplicationWork&gt; work = new LinkedList&lt;ReplicationWork&gt;();

    writeLock();
    try {
      synchronized (neededReplications) {
        for (priority = 0; priority &lt; blocksToReplicate.size(); priority++) {
          for (Block block : blocksToReplicate.get(priority)) {
            // block should belong to a file
            //获取该block所属的INode
            fileINode = blocksMap.getINode(block);
            // abandoned block not belong to a file
            if (fileINode == null ) {
              neededReplications.remove(block, priority); // remove from neededReplications
              replIndex[priority]--;
              continue;
            }
            //获取该文件需要的副本数
            requiredReplication = fileINode.getReplication();

            // 获取一个源datanode节点
            containingNodes = new ArrayList&lt;DatanodeDescriptor&gt;();
            NumberReplicas numReplicas = new NumberReplicas();
            srcNode = chooseSourceDatanode(block, containingNodes, numReplicas);
            if (srcNode == null) // block can not be replicated from any node
            {
              continue;
            }

          // 检查正在备份中的副本数是否满足备份需要，满足则不需要再备份
            numEffectiveReplicas = numReplicas.liveReplicas() +
              pendingReplications.getNumReplicas(block);
            if (numEffectiveReplicas &gt;= requiredReplication) {
              neededReplications.remove(block, priority); // remove from neededReplications
              replIndex[priority]--;
              continue;
            }
            //添加到待备份列表中
            work.add(new ReplicationWork(block, fileINode, requiredReplication
                - numEffectiveReplicas, srcNode, containingNodes, priority));
          }
        }
      }
    } finally {
      writeUnlock();
    }

    // 选取一个备份目标datanode
    for(ReplicationWork rw : work){
      DatanodeDescriptor targets[] = chooseTarget(rw);
      rw.targets = targets;
    }

    writeLock();
    try {
      for(ReplicationWork rw : work){
        DatanodeDescriptor[] targets = rw.targets;
        if(targets == null || targets.length == 0){
          rw.targets = null;
          continue;
        }
        synchronized (neededReplications) {
          Block block = rw.block;
          priority = rw.priority;
          // 重新检查INode和备份数，因为全局锁已经释放
          // block should belong to a file
          fileINode = blocksMap.getINode(block);
          // abandoned block not belong to a file
          if (fileINode == null ) {
            neededReplications.remove(block, priority); // remove from neededReplications
            rw.targets = null;
            replIndex[priority]--;
            continue;
          }
          requiredReplication = fileINode.getReplication();


          NumberReplicas numReplicas = countNodes(block);
          numEffectiveReplicas = numReplicas.liveReplicas() +
            pendingReplications.getNumReplicas(block);
          if (numEffectiveReplicas &gt;= requiredReplication) {
            neededReplications.remove(block, priority); // remove from neededReplications
            replIndex[priority]--;
            rw.targets = null;
            continue;
          }

          // 将block添加到datanode的需要备份的block列表中
          rw.srcNode.addBlockToBeReplicated(block, targets);

          scheduledWork++;

          //设置namenode的block调度计数器
          for (DatanodeDescriptor dn : targets) {
            dn.incBlocksScheduled();
          }

          // Move the block-replication into a "pending" state.
          // The reason we use 'pending' is so we can retry
          // replications that fail after an appropriate amount of time.
          //将该block移至pendingReplications（PendingReplicationBlocks）中，表示该block的状态为'pending'(正在备份中)。'pending'表示如果失败了还可以重试
          pendingReplications.add(block, targets.length);
          NameNode.stateChangeLog.debug(
            "BLOCK* block " + block
              + " is moved from neededReplications to pendingReplications");

          // remove from neededReplications
          //从 neededReplication列表中移除该block
          if (numEffectiveReplicas + targets.length &gt;= requiredReplication) {
            neededReplications.remove(block, priority); // remove from neededReplications
            replIndex[priority]--;
          }
        }
      }
    } finally {
      writeUnlock();
    }

    // 更新 metrics
    updateReplicationMetrics(work);

    // 打印debug信息
    if(NameNode.stateChangeLog.isInfoEnabled()){
      // log which blocks have been scheduled for replication
      for(ReplicationWork rw : work){
        // report scheduled blocks
        DatanodeDescriptor[] targets = rw.targets;
        if (targets != null &amp;&amp; targets.length != 0) {
          StringBuffer targetList = new StringBuffer("datanode(s)");
          for (int k = 0; k &lt; targets.length; k++) {
            targetList.append(' ');
            targetList.append(targets[k].getName());
          }
          NameNode.stateChangeLog.info(
            "BLOCK* ask "
              + rw.srcNode.getName() + " to replicate "
              + rw.block + " to " + targetList);
        }
      }
    }

    // 记录一次备份操作
    if (NameNode.stateChangeLog.isDebugEnabled()) {
      NameNode.stateChangeLog.debug("BLOCK* neededReplications = "
          + neededReplications.size() + " pendingReplications = "
          + pendingReplications.size());
    }
    return scheduledWork;
  }
</code></pre>

<h4>1.1.2.1. 选取源datanode —— chooseSourceDatanode（）</h4>

<p>获取一个源datanode节点</p>

<pre><code>  private DatanodeDescriptor chooseSourceDatanode(
                                    Block block,
                                    List&lt;DatanodeDescriptor&gt; containingNodes,
                                    NumberReplicas numReplicas) {
    containingNodes.clear();
    DatanodeDescriptor srcNode = null;
    int live = 0;
    int decommissioned = 0;
    int corrupt = 0;
    int excess = 0;
    Iterator&lt;DatanodeDescriptor&gt; it = blocksMap.nodeIterator(block);
    Collection&lt;DatanodeDescriptor&gt; nodesCorrupt = corruptReplicas.getNodes(block);
    while(it.hasNext()) {
      DatanodeDescriptor node = it.next();
      Collection&lt;Block&gt; excessBlocks = 
        excessReplicateMap.get(node.getStorageID());
      if ((nodesCorrupt != null) &amp;&amp; (nodesCorrupt.contains(node)))
        corrupt++;
      else if (node.isDecommissionInProgress() || node.isDecommissioned())
        decommissioned++;
      else if (excessBlocks != null &amp;&amp; excessBlocks.contains(block)) {
        excess++;
      } else {
        live++;
      }
      containingNodes.add(node);
      // Check if this replica is corrupt
      // If so, do not select the node as src node
      if ((nodesCorrupt != null) &amp;&amp; nodesCorrupt.contains(node))
        continue;
      if(node.getNumberOfBlocksToBeReplicated() &gt;= maxReplicationStreams)
        continue; // already reached replication limit
      // the block must not be scheduled for removal on srcNode
      if(excessBlocks != null &amp;&amp; excessBlocks.contains(block))
        continue;
      // never use already decommissioned nodes
      if(node.isDecommissioned())
        continue;
      // we prefer nodes that are in DECOMMISSION_INPROGRESS state
      if(node.isDecommissionInProgress() || srcNode == null) {
        srcNode = node;
        continue;
      }
      if(srcNode.isDecommissionInProgress())
        continue;
      // switch to a different node randomly
      // this to prevent from deterministically selecting the same node even
      // if the node failed to replicate the block on previous iterations
      if(r.nextBoolean())
        srcNode = node;
    }
    if(numReplicas != null)
      numReplicas.initialize(live, decommissioned, corrupt, excess);
    return srcNode;
  }
</code></pre>

<h4>1.1.2.2. 选取目标datanode —— chooseTarget(rw);</h4>

<pre><code>  private DatanodeDescriptor[] chooseTarget(ReplicationWork work) {
    if (!neededReplications.contains(work.block)) {
      return null;
    }
    if (work.blockSize == BlockCommand.NO_ACK) {
      LOG.warn("Block " + work.block.getBlockId() + 
          " of the file " + work.fileINode.getFullPathName() + 
          " is invalidated and cannot be replicated.");
      return null;
    }
    if (work.blockSize == DFSUtil.DELETED) {
      LOG.warn("Block " + work.block.getBlockId() + 
          " of the file " + work.fileINode.getFullPathName() + 
          " is a deleted block and cannot be replicated.");
      return null;
    }
    //实际调用replicator(BlockPlacementPolicy)的chooseTarget方法选取target
    return replicator.chooseTarget(work.fileINode,
        work.numOfReplicas, work.srcNode,
        work.containingNodes, null, work.blockSize);
  }
</code></pre>

<h2>副本存放策略</h2>

<p><img src="img/BlockAllocation.png" alt="BlockAllocation" /></p>

<p>如图所示，HDFS默认的副本存放策略为：</p>

<ol>
<li>第一个副本存放在当前datanode的本地</li>
<li>第二个副本存放在与第一个副本所在datanode不在同一机架上的一个datanode上</li>
<li>第三个副本存放在与第二个副本同一机架但不同datanode上</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS源码学习(4)——DataNode心跳检测（HeartBeat）]]></title>
    <link href="http://jiangbo.me/blog/2012/10/18/hdfs-heartbeat/"/>
    <updated>2012-10-18T21:38:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/18/hdfs-heartbeat</id>
    <content type="html"><![CDATA[<p>HDFS中DataNode的心跳检测通过FSNameSystem中的HeartbeatMonitor完成。代码结构如下：</p>

<pre><code>  class HeartbeatMonitor implements Runnable {
    /**
     */
    public void run() {
      while (fsRunning) {
        try {
          heartbeatCheck();
        } catch (Exception e) {
          FSNamesystem.LOG.error(StringUtils.stringifyException(e));
        }
        try {
          Thread.sleep(heartbeatRecheckInterval);
        } catch (InterruptedException ie) {
        }
      }
    }
  }
</code></pre>

<p>代码很简单，心跳检测线程周期性调用heartbeatCheck()。</p>

<h2>一、心跳检查——heartbeatCheck()</h2>

<p>该方法主要用于检测是否有过期的心跳检测，如有，检测其上的block是否已经进行过重新备份。该线程每次只处理一个datanode。</p>

<pre><code>  void heartbeatCheck() {
    if (isInSafeMode()) {
      // 安全模式下不做心跳检测
      return;
    }
    boolean allAlive = false;
    while (!allAlive) {
      boolean foundDead = false;
      DatanodeID nodeID = null;

      // 获取第一个dead datanode
      synchronized(heartbeats) {
        for (Iterator&lt;DatanodeDescriptor&gt; it = heartbeats.iterator();
             it.hasNext();) {
          DatanodeDescriptor nodeInfo = it.next();
          if (isDatanodeDead(nodeInfo)) {
            foundDead = true;
            nodeID = nodeInfo;
            break;
          }
        }
      }

      // 申请fsnamesystem锁，删除dead datanode
      if (foundDead) {
        writeLock();
        try {
          synchronized(heartbeats) {
            synchronized (datanodeMap) {
              DatanodeDescriptor nodeInfo = null;
              try {
                nodeInfo = getDatanode(nodeID);
              } catch (IOException e) {
                nodeInfo = null;
              }
              if (nodeInfo != null &amp;&amp; isDatanodeDead(nodeInfo)) {
                NameNode.stateChangeLog.info("BLOCK* NameSystem.heartbeatCheck: "
                                             + "lost heartbeat from " + nodeInfo.getName());
                removeDatanode(nodeInfo);
              }
            }
          }
        } finally {
          writeUnlock();
        }
      }
      allAlive = !foundDead;
    }
  }
</code></pre>

<h3>1.1 判断是否已死 —— isDatanodeDead（）</h3>

<p>判断一个datanode是否已经dead的标准很简单，当前距该节点最后的更新时间差是否已经超过心跳检测的过期时间限制</p>

<pre><code>private boolean isDatanodeDead(DatanodeDescriptor node) {
    return (node.getLastUpdate() &lt;
            (now() - heartbeatExpireInterval));
  }
</code></pre>

<h3>1.2 删除datanode —— removeDatanode（）</h3>

<pre><code>  private void removeDatanode(DatanodeDescriptor nodeInfo) {
    synchronized (heartbeats) {
      if (nodeInfo.isAlive) {
        updateStats(nodeInfo, false);
        //从heartbeats中移除
        heartbeats.remove(nodeInfo);
        //更新datanode状态
        nodeInfo.isAlive = false;
      }
    }

    nodeInfo.hasInitialBlockReport = false;
    for (Iterator&lt;Block&gt; it = nodeInfo.getBlockIterator(); it.hasNext();) {
      //移除该节点上的block
      removeStoredBlock(it.next(), nodeInfo);
    }
    unprotectedRemoveDatanode(nodeInfo);
    clusterMap.remove(nodeInfo);
  }
</code></pre>

<h4>1.2.1 removeStoredBlock（）</h4>

<p>该方法更新block->datanode的映射(blocksMap)，如果block还有效，有可能导致block备份发生</p>

<pre><code>  void removeStoredBlock(Block block, DatanodeDescriptor node) {
    if (NameNode.stateChangeLog.isDebugEnabled()) {
      NameNode.stateChangeLog.debug("BLOCK* NameSystem.removeStoredBlock: "
                                    +block + " from "+node.getName());
    }
    assert (hasWriteLock());
    if (!blocksMap.removeNode(block, node)) {
      if (NameNode.stateChangeLog.isDebugEnabled()) {
        NameNode.stateChangeLog.debug("BLOCK* NameSystem.removeStoredBlock: "
                                      +block+" has already been removed from node "+node);
      }
      return;
    }

    //
    //检查是否需要备份删除的block
    INode fileINode = blocksMap.getINode(block);
    if (fileINode != null) {
      //减小当前系统中安全block（备份数满足最小值的block）数量
      decrementSafeBlockCount(block);
      //更新需要备份的block数量
      updateNeededReplications(block, -1, 0);
    }

    //
    // 从excessblocks中删除改block，并从excessReplicateMap删除改datanode
    Collection&lt;Block&gt; excessBlocks = excessReplicateMap.get(node.getStorageID());
    if (excessBlocks != null) {
      if (excessBlocks.remove(block)) {
        excessBlocksCount--;
        if (NameNode.stateChangeLog.isDebugEnabled()) {
          NameNode.stateChangeLog.debug("BLOCK* NameSystem.removeStoredBlock: "
              +block+" is removed from excessBlocks");
        }
        if (excessBlocks.size() == 0) {
          excessReplicateMap.remove(node.getStorageID());
        }
      }
    }

    // 从corruptReplicas中移除该block
    corruptReplicas.removeFromCorruptReplicasMap(block, node);
  }
</code></pre>

<h5>1.2.1.1  BlocksMap.removeNode（）</h5>

<p>其中BlocksMap.removeNode()方法如下：</p>

<pre><code>  boolean removeNode(Block b, DatanodeDescriptor node) {
    BlockInfo info = blocks.get(b);
    if (info == null)
      return false;

    // 从datanode 的blocklist中移除block，并从block的datalist中移除datanode
    boolean removed = node.removeBlock(info);

    if (info.getDatanode(0) == null     // no datanodes left
              &amp;&amp; info.inode == null) {  // does not belong to a file
      //从blocksmap中移除该block
      blocks.remove(b);  // remove block from the map
    }
    return removed;
  }
</code></pre>

<h5>1.2.1.2 减小当前安全的block数 —— decrementSafeBlockCount()</h5>

<p>减小当前副本数安全的的block数，此举有可能触发系统进入安全模式（safemode）</p>

<pre><code>  void decrementSafeBlockCount(Block b) {
    if (safeMode == null) // mostly true
      return;

    safeMode.decrementSafeBlockCount((short)countNodes(b).liveReplicas());
  }
</code></pre>

<p>其中safeMode.decrementSafeBlockCount()代码如下：</p>

<pre><code>synchronized void decrementSafeBlockCount(short replication) {

  if (replication == safeReplication-1) {
    //安全的block数减一
    this.blockSafe--;
    //检查是否需要进入到safemode
    checkMode();
  }
}
</code></pre>

<p>SafeModeInfo.checkMode()代码如下：</p>

<pre><code>    private void checkMode() {
      //当安全的block数比例降至安全值以下，进入安全模式
      if (needEnter()) {
        enter();
        // check if we are ready to initialize replication queues
        if (canInitializeReplQueues() &amp;&amp; !isPopulatingReplQueues()) {
          //初始化副本队列
          initializeReplQueues();
        }
        reportStatus("STATE* Safe mode ON.", false);
        return;
      }
      // 如果安全模式已经关闭或者门槛小于0，则跳出安全模式
      if (!isOn() ||                           // safe mode is off
          extension &lt;= 0 || threshold &lt;= 0) {  // don't need to wait
        this.leave(true); // leave safe mode
        return;
      }

      //之前已经进入安全模式，直接返回
      if (reached &gt; 0) {  // threshold has already been reached before
        reportStatus("STATE* Safe mode ON.", false);
        return;
      }
      // 启动SafeModeMonitor线程
      reached = now();
      smmthread = new Daemon(new SafeModeMonitor());
      smmthread.start();
      reportStatus("STATE* Safe mode extension entered.", true);
    }
</code></pre>

<h5>1.2.1.3 更新需要备份的列表 —— updateNeededReplications（）</h5>

<pre><code>  /* updates a block in under replication queue */
  void updateNeededReplications(Block block,
                        int curReplicasDelta, int expectedReplicasDelta) {
    writeLock();
    try {
    //计算当前副本数
    NumberReplicas repl = countNodes(block);
    //期望的副本数
    int curExpectedReplicas = getReplication(block);
    //将该block更新到需要备份的列表中（neededReplications）
    neededReplications.update(block, 
                              repl.liveReplicas(), 
                              repl.decommissionedReplicas(),
                              curExpectedReplicas,
                              curReplicasDelta, expectedReplicasDelta);
    } finally {
      writeUnlock();
    }
  }
</code></pre>

<h4>1.2.2 移除datanode —— unprotectedRemoveDatanode</h4>

<pre><code>  void unprotectedRemoveDatanode(DatanodeDescriptor nodeDescr) {
    //重置清空datanode中block信息
    nodeDescr.resetBlocks();
    //从invlidateSet中移除datanode
    removeFromInvalidates(nodeDescr.getStorageID());
    if (NameNode.stateChangeLog.isDebugEnabled()) {
      NameNode.stateChangeLog.debug(
                                    "BLOCK* NameSystem.unprotectedRemoveDatanode: "
                                    + nodeDescr.getName() + " is out of service now.");
    }
  }
</code></pre>

<h5>1.2.2.1 removeFromInvalidates()</h5>

<pre><code>  private void removeFromInvalidates(String storageID) {
    //从recentInvalidateSet中移除该datanode
    Collection&lt;Block&gt; blocks = recentInvalidateSets.remove(storageID);
    if (blocks != null) {
      //从正在删除的block总数中减去当前节点上的block总数
      pendingDeletionBlocksCount -= blocks.size();
    }
  }
</code></pre>

<h2>二、 处理心跳检测请求 —— handleHeartbeat()</h2>

<p>NameNode只负责创建一个HeartbeatMonitor来通过每个datanode的最新更新时间周期性检查是否有过期的datanode，而每个datanode是否的最新更新时间是由datanode主动向namenode报告的，namenode通过handleHeartbeat()处理心跳请求。</p>

<pre><code>  DatanodeCommand[] handleHeartbeat(DatanodeRegistration nodeReg,
      long capacity, long dfsUsed, long remaining,
      int xceiverCount, int xmitsInProgress) throws IOException {
    DatanodeCommand cmd = null;
    synchronized (heartbeats) {
      synchronized (datanodeMap) {
        DatanodeDescriptor nodeinfo = null;
        try {
          nodeinfo = getDatanode(nodeReg);
        } catch(UnregisteredDatanodeException e) {
          return new DatanodeCommand[]{DatanodeCommand.REGISTER};
        }

        // 检查该datanode是否需要被关闭，可以通过设置datanode的adminState为DECOMMISSIONED来关闭一个datanode
        if (nodeinfo != null &amp;&amp; shouldNodeShutdown(nodeinfo)) {
          setDatanodeDead(nodeinfo);
          throw new DisallowedDatanodeException(nodeinfo);
        }

        if (nodeinfo == null || !nodeinfo.isAlive) {
          return new DatanodeCommand[]{DatanodeCommand.REGISTER};
        }

        updateStats(nodeinfo, false);
        nodeinfo.updateHeartbeat(capacity, dfsUsed, remaining, xceiverCount);
        updateStats(nodeinfo, true);

        //检查租约恢复状态
        cmd = nodeinfo.getLeaseRecoveryCommand(Integer.MAX_VALUE);
        if (cmd != null) {
          return new DatanodeCommand[] {cmd};
        }

        ArrayList&lt;DatanodeCommand&gt; cmds = new ArrayList&lt;DatanodeCommand&gt;(2);
        //检查正在备份中的副本
        cmd = nodeinfo.getReplicationCommand(
              maxReplicationStreams - xmitsInProgress);
        if (cmd != null) {
          cmds.add(cmd);
        }
        //检查无效的block
        cmd = nodeinfo.getInvalidateBlocks(blockInvalidateLimit);
        if (cmd != null) {
          cmds.add(cmd);
        }
        if (!cmds.isEmpty()) {
          return cmds.toArray(new DatanodeCommand[cmds.size()]);
        }
      }
    }

    //检查是否需要升级系统
    cmd = getDistributedUpgradeCommand();
    if (cmd != null) {
      return new DatanodeCommand[] {cmd};
    }
    return null;
  }
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS源码学习（3）——NameNode中的线程]]></title>
    <link href="http://jiangbo.me/blog/2012/10/18/hdfs-namenode-thread/"/>
    <updated>2012-10-18T21:37:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/18/hdfs-namenode-thread</id>
    <content type="html"><![CDATA[<p>NameNode存在三种运行模式：</p>

<ol>
<li>Normal： NameNode正常服务的状态</li>
<li>Safe mode：NameNode重启时进入Safe mode，该模式下整个系统是只读的，以便于NameNode手机DataNode信息</li>
<li>Backup mode：备份NameNode处于Backup mode，被动的接收主NameNode的检查点信息</li>
</ol>


<p>在NameNode中存在如下几种线程：</p>

<ol>
<li>DataNode 健康检查管理线程</li>
<li>副本管理线程</li>
<li>租约管理（lease Management）</li>
<li>IPC Handler 线程</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS源码学习(2)——NameNode初始化]]></title>
    <link href="http://jiangbo.me/blog/2012/10/18/hdfs-namenode-startup/"/>
    <updated>2012-10-18T21:35:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/18/hdfs-namenode-startup</id>
    <content type="html"><![CDATA[<h2>main()</h2>

<pre><code>  public static void main(String argv[]) throws Exception {
    try {
      StringUtils.startupShutdownMessage(NameNode.class, argv, LOG);
      //创建nameNode
      NameNode namenode = createNameNode(argv, null);
      if (namenode != null)
        namenode.join();
    } catch (Throwable e) {
      LOG.error(StringUtils.stringifyException(e));
      System.exit(-1);
    }
  }
</code></pre>

<h3>createNameNode()</h3>

<pre><code>  public static NameNode createNameNode(String argv[], 
                                 Configuration conf) throws IOException {
    if (conf == null)
      conf = new Configuration();
      //从命令行参数中提取启动配置项数据
    StartupOption startOpt = parseArguments(argv);
    if (startOpt == null) {
      printUsage();
      return null;
    }
    //设置启动参数
    setStartupOption(conf, startOpt);

    switch (startOpt) {
      case FORMAT:
        boolean aborted = format(conf, true);
        System.exit(aborted ? 1 : 0);
      case FINALIZE:
        aborted = finalize(conf, true);
        System.exit(aborted ? 1 : 0);
      default:
    }

    //新建NameNode
    NameNode namenode = new NameNode(conf);
    return namenode;
  }
</code></pre>

<h3>NameNode()</h3>

<pre><code>  public NameNode(Configuration conf) throws IOException {
    super(conf);
    try {
    //初始化
      initialize(getConf());
    } catch (IOException e) {
      this.stop();
      throw e;
    }
  }
</code></pre>

<h3>initialize()</h3>

<pre><code>  private void initialize(Configuration conf) throws IOException {
    InetSocketAddress socAddr = NameNode.getAddress(conf);
    int handlerCount = conf.getInt("dfs.namenode.handler.count", 10);
    // 关键-&gt;创建一个RPC Server
    this.server = RPC.getServer(this, socAddr.getHostName(), socAddr.getPort(),
                                handlerCount, false, conf);

    // The rpc-server port can be ephemeral... ensure we have the correct info
    this.serverAddress = this.server.getListenerAddress(); 
    FileSystem.setDefaultUri(conf, getUri(serverAddress));
    LOG.info("Namenode up at: " + this.serverAddress);

    myMetrics = new NameNodeMetrics(conf, this);

    //关键-&gt;创建一个FSNameSystem
    this.namesystem = new FSNamesystem(this, conf);
    //启动HTTP Server
    startHttpServer(conf);
    //启动RPC Server
    this.server.start();  //start RPC server   

    startTrashEmptier(conf);
  }
</code></pre>

<h2>FSNameSystem()</h2>

<pre><code>  FSNamesystem(NameNode nn, Configuration conf) throws IOException {
    try {
      //初始化FSNameSystem
      initialize(nn, conf);
      userPasswordInformation = new UserPasswordInformation(conf);
      extendAccessControlList = new ExtendAccessControlList(conf);
    } catch(IOException e) {
      LOG.error(getClass().getSimpleName() + " initialization failed.", e);
      close();
      throw e;
    }
  }
</code></pre>

<h3>FSNameSystem.initialize()</h3>

<pre><code>  private void initialize(NameNode nn, Configuration conf) throws IOException {
    this.systemStart = now();
    this.fsLock = new ReentrantReadWriteLock(); // non-fair locking
    setConfigurationParameters(conf);

    this.nameNodeAddress = nn.getNameNodeAddress();
    this.registerMBean(conf); // register the MBean for the FSNamesystemStutus

    //创建FSDirectory
    this.dir = new FSDirectory(this, conf);
    StartupOption startOpt = NameNode.getStartupOption(conf);

    //加载FSImage
    this.dir.loadFSImage(getNamespaceDirs(conf),
                         getNamespaceEditsDirs(conf), startOpt);
    long timeTakenToLoadFSImage = now() - systemStart;
    LOG.info("Finished loading FSImage in " + timeTakenToLoadFSImage + " msecs");
    NameNode.getNameNodeMetrics().fsImageLoadTime.set(
                              (int) timeTakenToLoadFSImage);
    this.safeMode = new SafeModeInfo(conf);
    setBlockTotal();
    //创建PendingReplicationBlocks
    pendingReplications = new PendingReplicationBlocks(
                            conf.getInt("dfs.replication.pending.timeout.sec", 
                                        -1) * 1000L);
    //创建心跳检查线程                                          
    this.hbthread = new Daemon(new HeartbeatMonitor());
    //创建租约管理线程
    this.lmthread = new Daemon(leaseManager.new Monitor());
    //创建副本管理线程
    this.replthread = new Daemon(new ReplicationMonitor());
    hbthread.start();
    lmthread.start();
    replthread.start();

    // 副本超额block管理线程
    this.overreplthread = new Daemon(new OverReplicationMonitor());
    overreplthread.start();

    this.hostsReader = new HostsFileReader(conf.get("dfs.hosts",""),
                                           conf.get("dfs.hosts.exclude",""));
    //创建退役节点管理线程
    this.dnthread = new Daemon(new DecommissionManager(this).new Monitor(
        conf.getInt("dfs.namenode.decommission.interval", 30),
        conf.getInt("dfs.namenode.decommission.nodes.per.interval", 5)));
    dnthread.start();

    this.dnsToSwitchMapping = ReflectionUtils.newInstance(
        conf.getClass("topology.node.switch.mapping.impl", ScriptBasedMapping.class,
            DNSToSwitchMapping.class), conf);

    /* If the dns to swith mapping supports cache, resolve network 
     * locations of those hosts in the include list, 
     * and store the mapping in the cache; so future calls to resolve
     * will be fast.
     */
    if (dnsToSwitchMapping instanceof CachedDNSToSwitchMapping) {
      dnsToSwitchMapping.resolve(new ArrayList&lt;String&gt;(hostsReader.getHosts()));
    }
    //创建副本定位器用于定位副本存放位置
    this.replicator = BlockPlacementPolicy.getInstance(
        conf,
        this,
        this.clusterMap,
        this.hostsReader,
        this.dnsToSwitchMapping,
        this);
  }
</code></pre>

<h2>FSDirectory(this, conf)</h2>

<p>新建FSDirecotry</p>

<pre><code> FSDirectory(FSNamesystem ns, Configuration conf) {
    //创建一个FSImage，并实例化构建FSDirectory
    this(new FSImage(), ns, conf);
    fsImage.setCheckpointDirectories(FSImage.getCheckpointDirs(conf, null),
                                FSImage.getCheckpointEditsDirs(conf, null));
  }
</code></pre>

<h3>this(new FSImage(), ns, conf);</h3>

<pre><code>FSDirectory(FSImage fsImage, FSNamesystem ns, Configuration conf) {
    this.bLock = new ReentrantReadWriteLock(); // non-fair
    this.cond = bLock.writeLock().newCondition();
    //创建根目录
    rootDir = new INodeDirectoryWithQuota(INodeDirectory.ROOT_NAME,
        ns.createFsOwnerPermissions(new FsPermission((short)0755)),
        Integer.MAX_VALUE, -1);
    this.fsImage = fsImage;
    namesystem = ns;
    initialize(conf);
  }
</code></pre>

<h3>FSDirectory.initialize()</h3>

<pre><code>  private void initialize(Configuration conf) {
    MetricsContext metricsContext = MetricsUtil.getContext("dfs");
    directoryMetrics = MetricsUtil.createRecord(metricsContext, "FSDirectory");
    directoryMetrics.setTag("sessionId", conf.get("session.id"));
  }
</code></pre>

<h3>FSDirectory.loadFSImage()</h3>

<pre><code>  void loadFSImage(Collection&lt;File&gt; dataDirs,
                   Collection&lt;File&gt; editsDirs,
                   StartupOption startOpt) throws IOException {
    // format before starting up if requested
    if (startOpt == StartupOption.FORMAT) {
      fsImage.setStorageDirectories(dataDirs, editsDirs);
      fsImage.format();
      startOpt = StartupOption.REGULAR;
    }
    try {
      //从datadir和editdirs加载FSImage
      if (fsImage.recoverTransitionRead(dataDirs, editsDirs, startOpt)) {
        fsImage.saveNamespace(true);
      }
      //初始化Editlog
      FSEditLog editLog = fsImage.getEditLog();
      assert editLog != null : "editLog must be initialized";
      if (!editLog.isOpen())
        editLog.open();
      fsImage.setCheckpointDirectories(null, null);
    } catch(IOException e) {
      fsImage.close();
      throw e;
    }
    writeLock();
    try {
      this.ready = true;
      cond.signalAll();
    } finally {
      writeUnlock();
    }
  }
</code></pre>

<h3>FSImage.recoverTransitionRead（）</h3>

<pre><code>  boolean recoverTransitionRead(Collection&lt;File&gt; dataDirs,
                             Collection&lt;File&gt; editsDirs,
                                StartupOption startOpt
                                ) throws IOException {
    assert startOpt != StartupOption.FORMAT : 
      "NameNode formatting should be performed before reading the image";

    // none of the data dirs exist
    if (dataDirs.size() == 0 || editsDirs.size() == 0)  
      throw new IOException(
        "All specified directories are not accessible or do not exist.");

    if(startOpt == StartupOption.IMPORT 
        &amp;&amp; (checkpointDirs == null || checkpointDirs.isEmpty()))
      throw new IOException("Cannot import image from a checkpoint. "
                          + "\"fs.checkpoint.dir\" is not set." );

    if(startOpt == StartupOption.IMPORT 
        &amp;&amp; (checkpointEditsDirs == null || checkpointEditsDirs.isEmpty()))
      throw new IOException("Cannot import image from a checkpoint. "
                          + "\"fs.checkpoint.edits.dir\" is not set." );

    setStorageDirectories(dataDirs, editsDirs);
    // 1.检查所有目录的状态和一致性
    Map&lt;StorageDirectory, StorageState&gt; dataDirStates = 
             new HashMap&lt;StorageDirectory, StorageState&gt;();
    boolean isFormatted = false;
    for (Iterator&lt;StorageDirectory&gt; it = 
                      dirIterator(); it.hasNext();) {
      StorageDirectory sd = it.next();
      StorageState curState;
      try {
        curState = sd.analyzeStorage(startOpt);
        // sd is locked but not opened
        switch(curState) {
        case NON_EXISTENT:
          // name-node fails if any of the configured storage dirs are missing
          throw new InconsistentFSStateException(sd.getRoot(),
                                                 "storage directory does not exist or is not accessible.");
        case NOT_FORMATTED:
          break;
        case NORMAL:
          break;
        default:  // recovery is possible
          sd.doRecover(curState);      
        }
        if (curState != StorageState.NOT_FORMATTED 
            &amp;&amp; startOpt != StartupOption.ROLLBACK) {
          sd.read(); // read and verify consistency with other directories
          isFormatted = true;
        }
        if (startOpt == StartupOption.IMPORT &amp;&amp; isFormatted)
          // import of a checkpoint is allowed only into empty image directories
          throw new IOException("Cannot import image from a checkpoint. " 
              + " NameNode already contains an image in " + sd.getRoot());
      } catch (IOException ioe) {
        sd.unlock();
        throw ioe;
      }
      dataDirStates.put(sd,curState);
    }

    if (!isFormatted &amp;&amp; startOpt != StartupOption.ROLLBACK 
                     &amp;&amp; startOpt != StartupOption.IMPORT)
      throw new IOException("NameNode is not formatted.");
    if (layoutVersion &lt; LAST_PRE_UPGRADE_LAYOUT_VERSION) {
      checkVersionUpgradable(layoutVersion);
    }
    if (startOpt != StartupOption.UPGRADE
          &amp;&amp; layoutVersion &lt; LAST_PRE_UPGRADE_LAYOUT_VERSION
          &amp;&amp; layoutVersion != FSConstants.LAYOUT_VERSION)
        throw new IOException(
                          "\nFile system image contains an old layout version " + layoutVersion
                          + ".\nAn upgrade to version " + FSConstants.LAYOUT_VERSION
                          + " is required.\nPlease restart NameNode with -upgrade option.");
    // check whether distributed upgrade is reguired and/or should be continued
    verifyDistributedUpgradeProgress(startOpt);

    // 2. Format unformatted dirs.
    this.checkpointTime = 0L;
    for (Iterator&lt;StorageDirectory&gt; it = 
                     dirIterator(); it.hasNext();) {
      StorageDirectory sd = it.next();
      StorageState curState = dataDirStates.get(sd);
      switch(curState) {
      case NON_EXISTENT:
        assert false : StorageState.NON_EXISTENT + " state cannot be here";
      case NOT_FORMATTED:
        LOG.info("Storage directory " + sd.getRoot() + " is not formatted.");
        LOG.info("Formatting ...");
        sd.clearDirectory(); // create empty currrent dir
        break;
      default:
        break;
      }
    }
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS源码学习（1）——NameNode主要数据结构]]></title>
    <link href="http://jiangbo.me/blog/2012/10/18/hdfs-namenode-datastructure/"/>
    <updated>2012-10-18T11:04:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/18/hdfs-namenode-datastructure</id>
    <content type="html"><![CDATA[<h2>FSNameSystem</h2>

<p>FSNameSystem是HDFS文件系统实际执行的核心，提供各种增删改查文件操作接口。其内部维护多个数据结构之间的关系：</p>

<ol>
<li>fsname->block列表的映射</li>
<li>所有有效blocks集合</li>
<li>block与其所属的datanodes之间的映射（该映射是通过block reports动态构建的，维护在namenode的内存中。每个datanode在启动时向namenode报告其自身node上的block）</li>
<li>每个datanode与其上的blocklist的映射</li>
<li>采用心跳检测根据LRU算法更新的机器（datanode）列表</li>
</ol>


<h3>FSDirectory</h3>

<p>FSDirectory用于维护当前系统中的文件树。</p>

<p>其内部主要组成结构包括一个INodeDirectoryWithQuota作为根目录(rootDir)和一个FSImage来持久化文件树的修改操作。</p>

<h4>INode</h4>

<p>HDFS中文件树用类似VFS中INode的方式构建，整个HDFS中文件被表示为INodeFile，目录被表示为INodeDirectory。INodeDiretoryWithQuota是INodeDirectory的扩展类，即带配额的文件目录</p>

<p><img src="http://jiangbo.me/images/hdfs/INode.png" alt="INode" /></p>

<p>INodeFile表示INode书中的一个文件，扩展自INode，除了名字(name)，父节点(parent)等之外，一个主要元素是blocks，一个BlockInfo数组，表示该文件对应的block信息。</p>

<h3>BlocksMap</h3>

<p>BlocksMap用于维护Block -> { INode, datanodes, self ref } 的映射
<img src="http://jiangbo.me/images/hdfs/BlocksMap.png" alt="BlocksMap" />
BlocksMap结构比较简单，实际上就是一个Block到BlockInfo的映射。</p>

<h4>Block</h4>

<p>Block是HDFS中的基本读写单元，主要包括：</p>

<ol>
<li>blockId: 一个long类型的块id</li>
<li>numBytes: 块大小</li>
<li>generationStamp: 块更新的时间戳</li>
</ol>


<h4>BlockInfo</h4>

<p>BlockInfo扩展自Block，除基本信息外还包括一个inode引用，表示该block所属的文件；以及一个神奇的三元组数组Object[] triplets，用来表示保存该block的datanode信息，假设系统中的备份数量为3。那么这个数组结构如下：</p>

<p><img src="http://jiangbo.me/images/hdfs/triplets.png" alt="triplets" /></p>

<ol>
<li>DN1，DN2，DN3分别表示存有改block的三个datanode的引用(DataNodeDescriptor）</li>
<li>DN1-prev-blk表示在DN1上block列表中当前block的前置block引用</li>
<li>DN1-next-blk表示在DN1上block列表中当前block的后置block引用</li>
</ol>


<p>DN2,DN3的prev-blk和next-blk类似。
HDFS采用这种结构存放block->datanode list的信息主要是为了节省内存空间，block->datanodelist之间的映射关系需要占用大量内存，如果同样还要将datanode->blockslist的信息保存在内存中，同样要占用大量内存。采用三元组这种方式能够从其中一个block获得到改block所属的datanode上的所有block列表。</p>

<h4>FSImage</h4>

<p>FSImage用于持久化文件树的变更以及系统启动时加载持久化数据。
HDFS启动时通过FSImage来加载磁盘中原有的文件树，系统Standby之后，通过FSEditlog来保存在文件树上的修改，FSEditLog定期将保存的修改信息刷到FSImage中进行持久化存储。
FSImage中文件元信息的存储结构如下（参见FImage.saveFSImage()方法）</p>

<p><img src="http://jiangbo.me/images/hdfs/FSImage.png" alt="FSImage" /></p>

<h5>FSImage头部信息</h5>

<ol>
<li>layoutVersion(int):image layout版本号，0.19版本的hdfs中为-18</li>
<li>namespaceId(int): 命名空间ID，系统初始化时生成，在一个namenode生命周期内保持不变，datanode想namenode注册是返回改id作为registerId，以后每次datanode与namenode通信时都携带该id，不认识的id的请求将被拒绝。</li>
<li>numberItemOfTree(long): 系统中的文件总数</li>
<li>generationTimeStamp: 生成image的时间戳</li>
</ol>


<h5>INode信息</h5>

<p>FSImage头之后是numberItemOfTree个INode信息，INode信息分为文件(INodeFile)和文件目录(INodeDirectory)两类，两者大体一致，分为INode头，Blocks区（目录没有blocks）和文件权限。</p>

<p><strong>INode头</strong></p>

<ol>
<li>nameLen(short): 文件名长度</li>
<li>filename(String): 文件名</li>
<li>replication(short): 备份数量</li>
<li>modificationTime(long): 最近修改时间</li>
<li>accessTime(long): 最近访问时间</li>
<li>preferedBlockSize(long): 块大小（目录为0）</li>
<li>block num(int): 块数量（目录为-1）</li>
</ol>


<p><strong>Blocks区</strong></p>

<ol>
<li>blockId(long)</li>
<li>numBytes(long,block大小)</li>
<li>generationTimeStamp(long, 更新时间戳）</li>
</ol>


<p><strong>文件权限</strong></p>

<ol>
<li>username(String): 文件用户名</li>
<li>group(String): 所属组</li>
<li>fileperm(short): 文件权限</li>
</ol>


<h5>underconstructionFile区</h5>

<p>layoutverion&lt;-18版本的fsimage还包括正在构建的文件区。与普通Inode信息类似，均有inode头和blocks区以及文件权限，除此之外，underConstructionFile还包括：</p>

<p><strong>client信息</strong></p>

<ol>
<li>clientName：client明</li>
<li>clientMachine： client机器名</li>
</ol>


<p><strong>已分配的datanode信息</strong></p>

<ol>
<li>ipcport： 服务端口</li>
<li>capacity: 容量</li>
<li>dfsuse： 已使用的空间</li>
<li>remaining： 剩余空间</li>
<li>lastupdate： 最新更新时间</li>
<li>xceiverCount</li>
<li>location： datanode位置</li>
<li>hostName：主机名</li>
<li>state： admin管理状态</li>
</ol>


<h2>其他结构</h2>

<h3>CorruptReplicasMap</h3>

<p>CorruptReplicasMap通过一个TreeMap维护corrupt状态block的blocks&#8211;>datanodedescriptor(s)映射。一个block备份在多个datanode中，当其中的一个或多个datanode上的block损坏时，会将该datanode加到treeMap中该block对应的datanodeDescriptor集合中。FSNameSystem通过该Map来维护所有损坏的block与其对应datanode的关系。</p>

<h3>Map&lt;String, LightWeightHashSet<Block>> recentInvalidateSets</h3>

<p>维护最近失效的block集合，map中为storageId->ArrayList<Block>，当某个block的一个datanode上副本失效时会将改block和对应的datanode的storeageId添加到recentInvalidateSet中，当datanode想namenode进行heartbeat时，namenode会检查该datanode中是否有损坏的block，如有，则通知datanode删除改block。</p>

<h3>NavigableMap&lt;String, DatanodeDescriptor>  datanodeMap</h3>

<p>datanodeMap用于维护datanode->block的映射</p>

<h3>ArrayList<DatanodeDescriptor> heartbeats</h3>

<p>维护多有当前活着的节点</p>

<h3>UnderReplicatedBlocks neededReplications</h3>

<p>通过一个优先级队列来维护当前需要备份的block集合，副本数越少的block优先级越高，0为最高级，表示当前只有一个副本。</p>

<h3>PendingReplicationBlocks pendingReplications;</h3>

<p>维护当前正在备份的block集合，并且进行备份请求的时间统计，并通过一个后台线程（PendingReplicationMonitor）来周期性（默认为5分钟）的统计超时的备份请求，当发生超时时，会将这个block重新添加到neededReplications列表中。
<img src="http://jiangbo.me/images/hdfs/PendingReplicationBlocks.png" alt="PendingReplicationBlocks" /></p>

<h3>LightWeightLinkedSet<Block> overReplicatedBlocks</h3>

<p>当前需要检查是否备份过多的block集合</p>

<h3>Map&lt;String, Collection<Block>> excessReplicateMap</h3>

<p>维护系统中datanode与其上的超额备份block的集合，这些超额的备份将被删除。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[本地编译Hadoop小记]]></title>
    <link href="http://jiangbo.me/blog/2012/09/24/compile-hadoop/"/>
    <updated>2012-09-24T15:28:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/09/24/compile-hadoop</id>
    <content type="html"><![CDATA[<h2>Git源码</h2>

<pre><code>git clone git://git.apache.org/hadoop-common.git
</code></pre>

<p>视网速不通，略慢</p>

<h2>编译</h2>

<pre><code>cd hadoop-common
mvn install -DskipTests
</code></pre>

<p>抛异常：</p>

<pre><code>[ERROR] Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.6:run (compile-proto) on project hadoop-common: An Ant BuildException has occured: exec returned: 127 -&gt; [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.6:run (compile-proto) on project hadoop-common: An Ant BuildException has occured: exec returned: 127
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:217)
    at... 
    Caused by: /Users/Shared/Workspace/hadoop/hadoop-common/hadoop-common-project/hadoop-common/target/antrun/build-main.xml:23: exec returned: 127
    at org.apache.tools.ant.taskdefs.ExecTask.runExecute(ExecTask.java:650)
    at org.apache.tools.ant.taskdefs.ExecTask.runExec(ExecTask.java:676)
    at org.apache.tools.ant.taskdefs.ExecTask.execute(ExecTask.java:502)
    at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)
    at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)
    ... 21 more
</code></pre>

<p>原因是缺少protocol buffer， 找不到protoc命令。</p>

<h3>安装protocol buffer</h3>

<pre><code>wget https://protobuf.googlecode.com/files/protobuf-2.4.1.tar.bz2
tar -xvf protobuf-2.4.1.tar.bz2
cd protobuf-2.4.1
./configure &amp;&amp; make
make install
</code></pre>

<h3>导入Eclipse</h3>

<pre><code>mvn eclipse:eclipse -DdownloadSources=true -DdownloadJavadocs=true
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[关于Memcache内存管理模型的理解]]></title>
    <link href="http://jiangbo.me/blog/2012/08/31/something-about-memcache-internal/"/>
    <updated>2012-08-31T07:52:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/08/31/something-about-memcache-internal</id>
    <content type="html"><![CDATA[<script async class="speakerdeck-embed" data-id="504049be5ec53c000202daa6" data-ratio="1.299492385786802" src="http://jiangbo.me//speakerdeck.com/assets/embed.js"></script>


<h2>说在前面</h2>

<p>本文不包含为什么使用memcache，以及如何使用memcache等基础知识。相关知识请查阅各类手册。
另，为便于理解，最好手头准备一份memcache的源码，本文使用的是目前最新的1.4.4版本源码，可自行到github上clone。</p>

<h2>Item、Chunk、Page、Slab</h2>

<h3>Data Item</h3>

<pre><code>+---------------------------------------+
|  key-value | cas | suffix | item head |  
+---------------------------------------+
</code></pre>

<p>Item指实际存放到memcache中的数据对象结构，除key-value数据外，还包括memcache自身对数据对象的描述信息（Item=key+value+后缀长+32byte结构体）</p>

<h3>Chunk</h3>

<p>Chunk指Memcache用来存放Data Item的最小单元，同一个Slab中的chunk大小是固定的。</p>

<pre><code>+------------------------------+
|   data item    | empty space |
+------------------------------+
</code></pre>

<h2>Page</h2>

<pre><code>+-------------------------------------+
|  chunk1 | chunk2 | chunk3 | chunk4  |
+-------------------------------------+
</code></pre>

<p>每个Slab中按照Page来申请内存，Page的大小默认为1M，可以通过-l参数调整，最小1k，最大128m.</p>

<h3>Slab</h3>

<pre><code>+--------------------------------+
|  Page1 | Page2 | Page3 | Page4 |
+--------------------------------+
</code></pre>

<p>Memcache将分配给它的内存（-m 参数指定，默认64m）按照Chunk大小不同，划分为多个slab。</p>

<p>他们三者的关系如下图所示:</p>

<pre><code>                 Chunk
                   ^                                                         
+------------------|------------------------------------------------------------+
|   Memory         |                                                            | 
|  +---------------|---------------------------------------------------------+  |
|  |      +--------|---------------------+  +------------------------------+ |  |
|  |      |Page1 +-|---+ +-----+ +-----+ |  |Page2 +-----+ +-----+ +-----+ | |  |
|  | Slab |(1M)  | 96B | | 68B | | 72B | |  |(1M)  | 92B | | 76B | | 84B | | |  | 
|  |  1   |      +-----+ +-----+ +-----+ |  |      +-----+ +-----+ +-----+ | |  |
|  |      +------------------------------+  +------------------------------+ |  |
|  +-------------------------------------------------------------------------+  |
|                                                                               |
|  +-------------------------------------------------------------------------+  |
|  |      +------------------------------+  +------------------------------+ |  |
|  |      |Page1 +------+    +------+    |  |Page2 +------+    +-------+   | |  |
|  | Slab | (1M) | 128B |    | 120B |    |  |(1M)  | 128B |    | 97B   |   | |  |
|  |   2  |      +------+    +------+    |  |      +------+    +-------+   | |  |
|  |      +------------------------------+  +------------------------------+ |  |
|  +-------------------------------------------------------------------------+  |
+-------------------------------------------------------------------------------+
</code></pre>

<h2>Slab内存分配</h2>

<h3>slab初始化</h3>

<p>Memcache启动时会进行slab初始化（参见slabs.c中slabs_init()函数），默认最小的chunksize为80（查看源码会发现settings中chunk_size默认为48，但是实际还需要加上一个32bytes的item结构体），可以通过-n参数调整，按照然后按照factor（默认为1.25，可以通过-f参数调整）(<em>关于参数更多的memcache默认参数可以参考memcache.c中settings的设置</em>)比例递增，划分出多个不同chunk大小的slab空间，即slab1的chunk大小=80，slab2的chunk大小为80*1.25=100，slab3的chunk大小为80*1.25*1.25=125，但最大一个一个chunk不会大于一个Page的大小（默认1M）。</p>

<pre><code>一下代码节选自 slabs.c
 95 void slabs_init(const size_t limit, const double factor, const bool prealloc) {
 96     int i = POWER_SMALLEST - 1;
 97     unsigned int size = sizeof(item) + settings.chunk_size;
 98  
 99     mem_limit = limit;
100  
101     if (prealloc) {
102         /* Allocate everything in a big chunk with malloc */
103         mem_base = malloc(mem_limit);
104         if (mem_base != NULL) {
105             mem_current = mem_base;
106             mem_avail = mem_limit;
107         } else {
108             fprintf(stderr, "Warning: Failed to allocate requested memory in"
109                     " one large chunk.\nWill allocate in smaller chunks\n");
110         }
111     }
112  
113     memset(slabclass, 0, sizeof(slabclass));
114  
115     while (++i &lt; POWER_LARGEST &amp;&amp; size &lt;= settings.item_size_max / factor) {
116         /* Make sure items are always n-byte aligned */
117         if (size % CHUNK_ALIGN_BYTES)
118             size += CHUNK_ALIGN_BYTES - (size % CHUNK_ALIGN_BYTES);
119  
120         slabclass[i].size = size;
121         slabclass[i].perslab = settings.item_size_max / slabclass[i].size;
122         size *= factor;
123         if (settings.verbose &gt; 1) {
124             fprintf(stderr, "slab class %3d: chunk size %9u perslab %7u\n",
125                     i, slabclass[i].size, slabclass[i].perslab);
126         }
127     }
128  
129     power_largest = i;
130     slabclass[power_largest].size = settings.item_size_max;
131     slabclass[power_largest].perslab = 1;
132     if (settings.verbose &gt; 1) {
133         fprintf(stderr, "slab class %3d: chunk size %9u perslab %7u\n",
134                 i, slabclass[i].size, slabclass[i].perslab);
135     }
136  
137     /* for the test suite:  faking of how much we've already malloc'd */
138     {
139         char *t_initial_malloc = getenv("T_MEMD_INITIAL_MALLOC");
140         if (t_initial_malloc) {
141             mem_malloced = (size_t)atol(t_initial_malloc);
142         }
143  
144     }
145  
146     if (prealloc) {
147         slabs_preallocate(power_largest);
148     }
149 }                             
</code></pre>

<p>PS：prealloc指的是直接申请一个大的chunk存放所有数据，默认是不采用这种方式的。</p>

<h3>数据存储过程</h3>

<p>一个数据项的大致存储量过程可以理解为（完整代码较长，不在粘贴，具体可参见items.c中do_item_alloc()方法）：</p>

<ol>
<li>构造一个数据项结构体，计算数据项的大小，（假设默认配置下，数据项大小为102B）</li>
<li>根据数据项的大小，找到最合适的slab，（100&lt;102&lt;125，所以存储在slab3中）</li>
<li>检查该slab中是否有过期的数据，如有清理掉</li>
<li>如果没有过期的数据项，则从当前slab中申请空间，参见slabs.c中slab_alloc()方法。</li>
<li>如果当前slab中申请失败，则尝试根据LRU算法逐出一个数据项，默认memcache是允许逐出的，如果被设置为禁止逐出，那么这是会反生悲剧的oom了</li>
<li>获取到item空间后将数据存储到改空间中，并追加到该slab的item列表中</li>
</ol>


<p>一个slab的申请一个chunk空间的过程大致如下（以下代码节选自slabs.c）：</p>

<pre><code>195 static int do_slabs_newslab(const unsigned int id) { 
196     slabclass_t *p = &amp;slabclass[id];
197     int len = settings.slab_reassign ? settings.item_size_max
198         : p-&gt;size * p-&gt;perslab;
199     char *ptr;             
200            
201     if ((mem_limit &amp;&amp; mem_malloced + len &gt; mem_limit &amp;&amp; p-&gt;slabs &gt; 0) ||
202         (grow_slab_list(id) == 0) ||    
203         ((ptr = memory_allocate((size_t)len)) == 0)) {
204            
205         MEMCACHED_SLABS_SLABCLASS_ALLOCATE_FAILED(id);
206         return 0;          
207     }      
208            
209     memset(ptr, 0, (size_t)len);    
210     split_slab_page_into_freelist(ptr, id);
211            
212     p-&gt;slab_list[p-&gt;slabs++] = ptr; 
213     mem_malloced += len;   
214     MEMCACHED_SLABS_SLABCLASS_ALLOCATE(id);
215            
216     return 1;              
217 }
218  
219 /*@null@*/ 
220 static void *do_slabs_alloc(const size_t size, unsigned int id) {
221     slabclass_t *p;        
222     void *ret = NULL;      
223     item *it = NULL;       
224  
225     if (id &lt; POWER_SMALLEST || id &gt; power_largest) {
226         MEMCACHED_SLABS_ALLOCATE_FAILED(size, 0);
227         return NULL;
228     }
229  
230     p = &amp;slabclass[id];
231     assert(p-&gt;sl_curr == 0 || ((item *)p-&gt;slots)-&gt;slabs_clsid == 0);
232  
233     /* fail unless we have space at the end of a recently allocated page,
234        we have something on our freelist, or we could allocate a new page */
235     if (! (p-&gt;sl_curr != 0 || do_slabs_newslab(id) != 0)) {
236         /* We don't have more memory available */
237         ret = NULL;
238     } else if (p-&gt;sl_curr != 0) {
239         /* return off our freelist */
240         it = (item *)p-&gt;slots;
241         p-&gt;slots = it-&gt;next;
242         if (it-&gt;next) it-&gt;next-&gt;prev = 0;
243         p-&gt;sl_curr--;
244         ret = (void *)it;
245     }
246  
247     if (ret) {
248         p-&gt;requested += size;
249         MEMCACHED_SLABS_ALLOCATE(size, id, p-&gt;size, ret);
250     } else {
251         MEMCACHED_SLABS_ALLOCATE_FAILED(size, id);
252     }
253  
254     return ret;
255 }
</code></pre>

<p>slab优先从slots（空闲chunk空间列表）中申请空间，如果没有则尝试申请一个Page的新空间（do_slab_newslab()），申请新slab是会先判断是否进行slab_reasgin（重新分配slab空间，默认不开启）。</p>

<h2>内存浪费</h2>

<p>根据上述描述，Memcache使用Slab预分配的方式进行内存管理提升了性能（减少分配内存的消耗），但是带来了内存浪费，主要体现在：</p>

<ol>
<li><p>Data Item Size &lt;= Chunk Size，Chunk是存储数据项的最小单元，数据项的大小必须不大于其所在的Chunk大小。也就是说76B的数据对象存入96B的Chunk中，将带来96B-76B=20B的空间浪费。</p></li>
<li><p>Memcache是按照Page申请和使用内存的，当Page大小不是Chunk的整数倍时，余下的空间将被浪费。即如果PageSize=1M，ChunkSize=1000B,那么将有1024*1024%1000=576B的空间浪费。</p></li>
<li><p>Memcache默认是不开启slab reasign的，也就是说分配已经分配给一个slab的内存空间，即使该slab不用，默认也不会分配给其他slab的</p></li>
</ol>


<h2>案例分析：定长问题导致逐出</h2>

<p>memcache的chunk分布是均匀的，这是为了通用性考虑，但是现实中一些场景chunk的分布是不均运的，例如为了减小对数据库的压力，对数据进行了全量缓存，为标识数据库中不存在的记录，向缓存中放置了一个stupidObject。这个对象大小是固定的，且该数据的量很大，导致该数据类型所在的slab占用了大量缓存空间。再一次调整对象结构时，修改了这个StupidObject大小，使其分布在另一个slab中，但是这个原分配的slab空间不会回收，空闲空间不足，导致大量逐出。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用方法链和静态工厂构造流畅接口]]></title>
    <link href="http://jiangbo.me/blog/2012/04/08/build-fluent-interface-with-method-chain-and-static-factory/"/>
    <updated>2012-04-08T11:23:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/04/08/build-fluent-interface-with-method-chain-and-static-factory</id>
    <content type="html"><![CDATA[<h3>问题现象</h3>

<p>现有的VO，DO，Model等模型类中，均遵循JavaBean规范，为对属性的访问提供了getter和setter方法，并且在实际使用时通常为构造一个模型实例，需要调用大段的setter方法。下面以VasViewVO为例：</p>

<pre><code>public class VasViewVO {

    private String description;

    private Date gmtOpen;

    private Date gmtClose;

    private CreditVasType serviceType;

    private CreditVasMemberStatus creditVasMemberStatus;
    ...//此处省略getter和setter方法
}
</code></pre>

<p>通常按照如下方式构造一个VasViewVO实例：</p>

<pre><code>    VasViewVO vasViewVO = new VasViewVO();
    vasViewVO.setCreditVasMemberStatus(CreditVasMemberStatus.OPEN);
    vasViewVO.setGmtOpen(new Date());
    vasViewVO.setServiceType(CreditVasType.fastpay);
    vasViewVO.setDescription("open fastpay");
    //do something with vasViewVO
</code></pre>

<p>如此构造VasViewVO的实例并非有严重的逻辑错误或者其他的问题，只是不够简洁易懂（贴近自然语言）。
最近学习过jQuery的都知道，jQuery中可以按如下方式连续调用api</p>

<pre><code>    $("p").hide().fadeIn("slow").slideUp("slow").slideDown("slow");
</code></pre>

<p>如果Java程序中也可以使用类似的方式将会简洁许多。</p>

<h3>解决方法</h3>

<h4>使用方法链</h4>

<p>jQuery中之所以能够连续调用api，主要是因为每个方法返回的都是一个jQuery对象。Java本身也可这样做，即返回一个this(自身引用)，也就是常说的<a href="http://en.wikipedia.org/wiki/Method_chaining">方法链</a>。
方法链的实现非常简单，通常的setter方法返回void，方法链返回的是this引用，如下：</p>

<pre><code>    public VasViewVO description(String description) {
        this.description = description;
        return this;
    }

    public VasViewVO gmtOpen(Date gmtOpen) {
        this.gmtOpen = gmtOpen;
        return this;
    }

    public VasViewVO gmtClose(Date gmtClose) {
        this.gmtClose = gmtClose;
        return this;
    }

    public VasViewVO serviceType(CreditVasType serviceType) {
        this.serviceType = serviceType;
        return this;
    }
</code></pre>

<p>如此一来在调用时只需要新建一个对象，连续调用赋值方法即可：</p>

<pre><code>    VasViewVO vasView = new VasViewVO();
    vasView.creditVasMemberStatus(CreditVasMemberStatus.CLOSE)
            .description("Commoent cxxx").gmtClose(new Date())
            .gmtOpen(new Date()).serviceType(CreditVasType.fastpay);
</code></pre>

<p>如此相比连续调用5次setter方法简洁的多。</p>

<h4>使用静态工厂和static import优化</h4>

<p>使用方法链之后简化了setter调用，但是每次还必须要先new一个实例，略显繁琐。这个问题可以通过Effectvie Java的第一条静态工厂来解决，即在VasViewVO内部实现一个静态工厂方法。</p>

<pre><code>    public static VasViewVO with() {
        return new VasViewVO();
    }
</code></pre>

<p>这个with的方法名与一般的静态工厂所用的getInstance，newInstance不同，主要时为了更加贴近自然语言。
如此，构造一个VasViewVO实例就可简化为</p>

<pre><code>    VasViewVO vasViewVO = VasViewVO.with()
                .creditVasMemberStatus(CreditVasMemberStatus.CLOSE)
                .description("Commoent cxxx").gmtClose(new Date())
                .gmtOpen(new Date()).serviceType(CreditVasType.fastpay);
</code></pre>

<p>代码已经很简洁，不过每次都是用枚举类长长的类名，感觉很是不雅，可以通过static import解决，最终的到的简化后构造一个VasViewVO实例的代码如下：</p>

<pre><code>import static  com.alibaba.china.credit.common.constants.CreditVasType.*;
import static com.alibaba.china.credit.vas.dal.constant.CreditVasMemberStatus.*;
public class Test{
    public static void main(String[] args) {
        . . .

        VasViewVO vasViewVO = VasViewVO.with()
            .creditVasMemberStatus(CLOSE)
            .description("Commoent cxxx").gmtClose(new Date())
            .gmtOpen(new Date()).serviceType(fastpay);
        . . .
    }
}
</code></pre>

<h3>进阶总结</h3>

<p>程序设计要解决的是讲现实世界中的问题描述转化为计算机可识别的计算机语言（二进制码），因此编程语言必须为这一转化过程提供有效的抽像机制。现有编程语言提供的抽象机制侧重各有不同，在C，C++，JAVA等通用语言中更关注语言的基本语义，离实际的问题域描述较远；SQL，CSS等特定领域语言更关注特定问题域问题的抽象，更贴近实际的问题描述。</p>

<p>站在人的角度，语言越是贴近实际问题的描述越是容易理解，对问题的描述也更加准确。但现有的特定领域语言又不具备通用变成语言解决问题的的通用性，因此我们更希望能够在通用语言上进行更高层次的抽象，使其更加贴近具体的问题域。某种程度上讲语言提供的API或者方法库就是对基本问题的更高层次抽象，但离具体的问题域还是太远，有时我们更渴望更加贴近具体问题的语言来解决问题并兼具通用语言的通用性，因此诞生了通用语言的内部DSL。
<a href="http://www.martinfowler.com/bliki/FluentInterface.html">流畅接口（Fluent Interface）</a>是实现内部DSL的重要手段，<a href="http://en.wikipedia.org/wiki/Fluent_interface">wikipedia上如此描述</a>Fluent Interface：</p>

<pre><code>A fluent interface (as first coined by Eric Evans and Martin Fowler) is an implementation of an object oriented API that aims to provide for more readable code. A fluent interface is normally implemented by using method chaining to relay the instruction context of a subsequent call (but a fluent interface entails more than just method chaining)。
</code></pre>

<p>方法链（method chain）是实现Fluent Interface的重要手段（注意，method chain!= Fluent Interface!=DSL）。但仅仅有方法链是不足够构建有效的DSL的，除此之外还需要一些且他编程技巧，比如static factory和static import等，具体请参见<a href="http://www.infoq.com/articles/internal-dsls-java">《An Approach to Internal Domain－Specific Languages in Java》</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[java.lang.instrument笔记]]></title>
    <link href="http://jiangbo.me/blog/2012/02/21/java-lang-instrument/"/>
    <updated>2012-02-21T17:13:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/02/21/java-lang-instrument</id>
    <content type="html"><![CDATA[<h2>什么是Instrumentation？</h2>

<p>java Instrumentation指的是可以用独立于应用程序之外的代理（agent）程序来监测和协助运行在JVM上的应用程序。这种监测和协助包括但不限于获取JVM运行时状态，替换和修改类定义等。
Java SE5中使用JVM TI替代了JVM PI和JVM DI。提供一套代理机制，支持独立于JVM应用程序之外的程序以代理的方式连接和访问JVM。java.lang.instrument是在JVM TI的基础上提供的Java版本的实现。
Instrumentation提供的主要功能是修改jvm中类的行为。
Java SE6中由两种应用Instrumentation的方式，premain（命令行）和agentmain（运行时）</p>

<h2>premain方式</h2>

<p>在Java SE5时代，Instrument只提供了premain一种方式，即在真正的应用程序（包含main方法的程序）main方法启动前启动一个代理程序。例如使用如下命令：</p>

<pre><code>java -javaagent:agent_jar_path[=options] java_app_name
</code></pre>

<p>可以在启动名为java_app_name的应用之前启动一个agent_jar_path指定位置的agent jar。
实现这样一个agent jar包，必须满足两个条件：</p>

<ol>
<li>在这个jar包的manifest文件中包含Premain-Class属性，并且改属性的值为代理类全路径名。</li>
<li>代理类必须提供一个public static void premain(String args, Instrumentation inst)或 public static void premain(String args) 方法。</li>
</ol>


<p>当在命令行启动该代理jar时，VM会根据manifest中指定的代理类，使用于main类相同的系统类加载器（即ClassLoader.getSystemClassLoader()获得的加载器）加载代理类。在执行main方法前执行premain()方法。如果premain(String args, Instrumentation inst)和premain(String args)同时存在时，优先使用前者。其中方法参数args即命令中的options，类型为String（注意不是String[]），因此如果需要多个参数，需要在方法中自己处理（比如用&#8221;;&#8221;分割多个参数之类）；inst是运行时由VM自动传入的Instrumentation实例，可以用于获取VM信息。</p>

<h2>premain实例-打印所有的方法调用</h2>

<p>下面实现一个打印程序执行过程中所有方法调用的功能，这个功能可以通过AOP其他方式实现，这里只是尝试使用Instrumentation进行ClassFile的字节码转换实现：</p>

<h3>构造agent类</h3>

<p>premain方式的agent类必须提供premain方法，代码如下：</p>

<pre><code>package test;

import java.lang.instrument.Instrumentation;

public class Agent {

    public static void premain(String args, Instrumentation inst){
        System.out.println("Hi, I'm agent!");
        inst.addTransformer(new TestTransformer());
    }
}
</code></pre>

<p>premain有两个参数，args为自定义传入的代理类参数，inst为VM自动传入的Instrumentation实例。 premain方法的内容很简单，除了标准输出外，只有</p>

<pre><code>inst.addTransformer(new TestTransformer());
</code></pre>

<p>这行代码的意思是向inst中添加一个类的转换器。用于转换类的行为。</p>

<h3>构造Transformer</h3>

<p>下面来实现上述过程中的TestTransformer来完成打印调用方法的类定义转换。</p>

<pre><code>package test;

import java.lang.instrument.ClassFileTransformer;
import java.lang.instrument.IllegalClassFormatException;
import java.security.ProtectionDomain;

import org.objectweb.asm.ClassReader;
import org.objectweb.asm.ClassWriter;
import org.objectweb.asm.Opcodes;
import org.objectweb.asm.tree.ClassNode;
import org.objectweb.asm.tree.FieldInsnNode;
import org.objectweb.asm.tree.InsnList;
import org.objectweb.asm.tree.LdcInsnNode;
import org.objectweb.asm.tree.MethodInsnNode;
import org.objectweb.asm.tree.MethodNode;

public class TestTransformer implements ClassFileTransformer {

    @Override
    public byte[] transform(ClassLoader arg0, String arg1, Class&lt;?&gt; arg2,
            ProtectionDomain arg3, byte[] arg4)
            throws IllegalClassFormatException {
        ClassReader cr = new ClassReader(arg4);
        ClassNode cn = new ClassNode();
        cr.accept(cn, 0);
        for (Object obj : cn.methods) {
            MethodNode md = (MethodNode) obj;
            if ("&lt;init&gt;".endsWith(md.name) || "&lt;clinit&gt;".equals(md.name)) {
                continue;
            }
            InsnList insns = md.instructions;
            InsnList il = new InsnList();
            il.add(new FieldInsnNode(Opcodes.GETSTATIC, "java/lang/System",
                    "out", "Ljava/io/PrintStream;"));
            il.add(new LdcInsnNode("Enter method-&gt; " + cn.name+"."+md.name));
            il.add(new MethodInsnNode(Opcodes.INVOKEVIRTUAL,
                    "java/io/PrintStream", "println", "(Ljava/lang/String;)V"));
            insns.insert(il);
            md.maxStack += 3;

        }
        ClassWriter cw = new ClassWriter(0);
        cn.accept(cw);
        return cw.toByteArray();
    }

}
</code></pre>

<p>TestTransformer实现了ClassFileTransformer接口，该接口只有一个transform方法，参数传入包括该类的类加载器，类名，原字节码字节流等，返回被转换后的字节码字节流。
TestTransformer主要使用ASM实现在所有的类定义的方法中，在方法开始出添加了一段打印该类名和方法名的字节码。在转换完成后返回新的字节码字节流。详细的ASM使用请参考ASM手册。</p>

<h3>设置MANIFEST.MF</h3>

<p>设置MANIFEST.MF文件中的属性，文件内容如下：</p>

<pre><code>Manifest-Version: 1.0
Premain-Class: test.Agent
Created-By: 1.6.0_29
</code></pre>

<h3>测试</h3>

<p>代码编写完成后将代码编译打成agent.jar。
编写测试代码：</p>

<pre><code>public class TestAgent {

    public static void main(String[] args) {
        TestAgent ta = new TestAgent();
        ta.test();
    }

    public void test() {
        System.out.println("I'm TestAgent");
    }

}
</code></pre>

<p>从命令行执行该类，并设置agent.jar</p>

<pre><code>java -javaagent:agent.jar TestAgent
</code></pre>

<p>将打印出程序运行过程中实际执行过的所有方法名：</p>

<pre><code>Hi, I'm agent!
Enter method-&gt; test/TestAgent.main
Enter method-&gt; test/TestAgent.test
I'm TestAgent
Enter method-&gt; java/util/IdentityHashMap$KeySet.iterator
Enter method-&gt; java/util/IdentityHashMap$IdentityHashMapIterator.hasNext
Enter method-&gt; java/util/IdentityHashMap$KeyIterator.next
Enter method-&gt; java/util/IdentityHashMap$IdentityHashMapIterator.nextIndex
Enter method-&gt; java/util/IdentityHashMap$IdentityHashMapIterator.hasNext
Enter method-&gt; java/util/IdentityHashMap$KeySet.iterator
Enter method-&gt; java/util/IdentityHashMap$IdentityHashMapIterator.hasNext
Enter method-&gt; java/util/IdentityHashMap$KeyIterator.next
Enter method-&gt; java/util/IdentityHashMap$IdentityHashMapIterator.nextIndex
Enter method-&gt; com/apple/java/Usage$3.run
。。。
</code></pre>

<p>从输出中可以看出，程序首先执行的是代理类中的premain方法（不过代理类自身不会被自己转换，所以不能打印出代理类的方法名），然后是应用程序中的main方法。</p>

<h2>agentmain方式</h2>

<p>premain时Java SE5开始就提供的代理方式，给了开发者诸多惊喜，不过也有些须不变，由于其必须在命令行指定代理jar，并且代理类必须在main方法前启动。因此，要求开发者在应用前就必须确认代理的处理逻辑和参数内容等等，在有些场合下，这是比较苦难的。比如正常的生产环境下，一般不会开启代理功能，但是在发生问题时，我们不希望停止应用就能够动态的去修改一些类的行为，以帮助排查问题，这在应用启动前是无法确定的。
为解决运行时启动代理类的问题，Java SE6开始，提供了在应用程序的VM启动后在动态添加代理的方式，即agentmain方式。
与Permain类似，agent方式同样需要提供一个agent jar，并且这个jar需要满足：</p>

<ol>
<li>在manifest中指定Agent-Class属性，值为代理类全路径</li>
<li>代理类需要提供public static void agentmain(String args, Instrumentation inst)或public static void agentmain(String args)方法。并且再二者同时存在时以前者优先。args和inst和premain中的一致。</li>
</ol>


<p>不过如此设计的再运行时进行代理有个问题——如何在应用程序启动之后再开启代理程序呢？
JDK6中提供了Java Tools API，其中Attach API可以满足这个需求。</p>

<p>Attach API中的VirtualMachine代表一个运行中的VM。其提供了loadAgent()方法，可以在运行时动态加载一个代理jar。具体需要参考<a href="">《Attach API》</a></p>

<h2>agentmain实例-打印当前已加载的类</h2>

<h3>构造agent类</h3>

<p>agentmain方式的代理类必须提供agentmain方法：</p>

<pre><code>package loaded;

import java.lang.instrument.Instrumentation;

public class LoadedAgent {
    @SuppressWarnings("rawtypes")
    public static void agentmain(String args, Instrumentation inst){
        Class[] classes = inst.getAllLoadedClasses();
        for(Class cls :classes){
            System.out.println(cls.getName());
        }
    }
}
</code></pre>

<p>agentmain方法通过传入的Instrumentation实例获取当前系统中已加载的类。</p>

<h3>设置MANNIFEST.MF</h3>

<p>设置MANIFEST.MF文件，指定Agent-Class:</p>

<pre><code>Manifest-Version: 1.0
Agent-Class: loaded.LoadedAgent
Created-By: 1.6.0_29
</code></pre>

<h3>绑定到目标VM</h3>

<p>将agent类和MANIFEST.MF文件编译打成loadagent.jar后，由于agent main方式无法向pre main方式那样在命令行指定代理jar，因此需要借助Attach Tools API。</p>

<pre><code>package attach;

import java.io.IOException;

import com.sun.tools.attach.AgentInitializationException;
import com.sun.tools.attach.AgentLoadException;
import com.sun.tools.attach.AttachNotSupportedException;
import com.sun.tools.attach.VirtualMachine;

public class Test {
    public static void main(String[] args) throws AttachNotSupportedException,
            IOException, AgentLoadException, AgentInitializationException {
        VirtualMachine vm = VirtualMachine.attach(args[0]);
        vm.loadAgent("/Users/jiangbo/Workspace/code/java/javaagent/loadagent.jar");

    }

}
</code></pre>

<p>该程序接受一个参数为目标应用程序的进程id，通过Attach Tools API的VirtualMachine.attach方法绑定到目标VM，并向其中加载代理jar。</p>

<h3>构造目标测试程序</h3>

<p>构造一个测试用的目标应用程序：</p>

<pre><code>package attach;

public class TargetVM {
    public static void main(String[] args) throws InterruptedException{
        while(true){
            Thread.sleep(1000);
        }
    }
}
</code></pre>

<p>这个测试程序什么都不做，只是不停的sleep。:)
运行该程序，获得进程ID=33902。
运行上面绑定到VM的Test程序，将进程id作为参数传入：</p>

<pre><code>java attach.Test 33902
</code></pre>

<p>观察输出，会打印出系统当前所有已经加载类名</p>

<pre><code>java.lang.NoClassDefFoundError
java.lang.StrictMath
java.security.SignatureSpi
java.lang.Runtime
java.util.Hashtable$EmptyEnumerator
sun.security.pkcs.PKCS7
java.lang.InterruptedException
java.io.FileDescriptor$1
java.nio.HeapByteBuffer
java.lang.ThreadGroup
[Ljava.lang.ThreadGroup;
java.io.FileSystem
。。。
</code></pre>

<h2>参考文档</h2>

<ul>
<li><a href="http://docs.oracle.com/javase/6/docs/api/java/lang/instrument/package-summary.html">java.lang.instrument API docs</a></li>
<li><a href="https://blogs.oracle.com/CoreJavaTechTips/entry/the_attach_api">The Attach API</a></li>
<li><a href="http://www.ibm.com/developerworks/cn/java/j-lo-jse61/index.html">Java SE6新特性：Instrumentation新功能</a></li>
</ul>


<h2>附：agent jar中manifest的属性</h2>

<ul>
<li>Premain-Class: 当在VM启动时，在命令行中指定代理jar时，必须在manifest中设置Premain-Class属性，值为代理类全类名，并且该代理类必须提供premain方法。否则JVM会异常终止。</li>
<li>Agent-Class: 当在VM启动之后，动态添加代理jar包时，代理jar包中manifest必须设置Agent-Class属性，值为代理类全类名，并且该代理类必须提供agentmain方法，否则无法启动该代理。</li>
<li>Boot-Class-Path: Bootstrap class loader加载类时的搜索路径，可选。</li>
<li>Can-Redefine-Classes: true/false；标示代理类是否能够重定义类。可选。</li>
<li>Can-Retransform-Classes: true/false；标示代理类是否能够转换类定义。可选。</li>
<li>Can-Set-Native-Prefix::true/false；标示代理类是否需要本地方法前缀，可选。</li>
</ul>


<p><strong> 当一个代理jar包中的manifest文件中既有Premain-Class又有Agent-Class时，如果以命令行方式在VM启动前指定代理jar，则使用Premain-Class；反之如果在VM启动后，动态添加代理jar，则使用Agent-Class </strong></p>
]]></content>
  </entry>
  
</feed>
