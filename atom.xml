<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[非纯种程序猿]]></title>
  <link href="http://jiangbo.me/atom.xml" rel="self"/>
  <link href="http://jiangbo.me/"/>
  <updated>2012-12-21T17:16:56+08:00</updated>
  <id>http://jiangbo.me/</id>
  <author>
    <name><![CDATA[jiang-bo]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[HDFS RAID]]></title>
    <link href="http://jiangbo.me/blog/2012/12/21/hdfs-raid/"/>
    <updated>2012-12-21T11:19:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/12/21/hdfs-raid</id>
    <content type="html"><![CDATA[<h2>一、背景</h2>

<p>HDFS是构建在普通机器上的分布式文件系统，而这类系统需要解决的一个首要问题就是容错，允许部分节点失效。而为了解决数据的可靠性，HDFS采用了副本策略。默认会为所有的block存放三个副本（具体参见HDFS设计文档）。
副本机制能够有效解决部分节点失效导致数据丢失的问题，但对于大规模的HDFS集群，副本机制会带来大量的存储资源消耗。例如为了存储1PB的数据，默认需要保留3个副本，这意味着实际存储所有副本需要至少3PB的空间。存储空间浪费达到200%。减小浪费的方式主要是减少副本数，而当副本数降低到小于3时，数据丢失的风险会非常高。而HDFS RAID的出现主要是解决降低副本数之后，通过RAID机制中的Erasured Code来确保数据的可用性。</p>

<script async class="speakerdeck-embed" data-id="3f1abed02d630130814722000a9d03e5" data-ratio="1.2994923857868" src="http://jiangbo.me//speakerdeck.com/assets/embed.js"></script>


<h2>二、整体结构</h2>

<p>HDFS RAID的实现（Facebook的实现）主要是在现有的HDFS之上增加了一个包装contrib。之所以不再HDFS上直接修改，原设计者的解释是“HDFS的核心代码已经够复杂了，不想让它更复杂”。</p>

<p><img src="http://jiangbo.me/images/hdfs/Raid-Overview.png" alt="Overview" /></p>

<h3>2.1 使用的角度看HDFS RAID（Client端）</h3>

<p>HDFS RAID的使用场景主要有两个：raid数据管理和raid数据读取。</p>

<h4>2.1.1 Raid数据的管理</h4>

<p>对于DRFS的管理，包括DFS中那些文件需要进行raid化，查询raid文件的状态等，主要通过HDFS-RAID提供的RaidShell工具来完成。本质上RaidShell作为一个client工具，通过RPC与集群中的RaidNode通信，完成各种管理操作。</p>

<h4>2.1.2 Raid数据读写</h4>

<p>使用HDFS RAID的client端需要配置fs.hdfs.impl为DistributedRaidFileSytem，DRFS包装了DFS的读（只是读）请求，当block读取时发生block丢失（抛出MissingBlockException)或损坏(CorruptionException)时，DRFS会捕获这两个异常，并向RaidNode发送RPC对失效的数据进行恢复。</p>

<h3>2.2  RaidNode结构(Server端）</h3>

<p>RaidNode是HDFS-RAID中除NameNode和JobTracker之外的第三个master node，主要是接收client端的RPC请求和调度各守护线程完成数据的raid化和数据修复，parity文件删除等操作。</p>

<h4>2.2.1 两种实现</h4>

<p><strong>LocalRaidNode:</strong> 在RaidNode本地进行parity计算，parity文件的生成是一个计算密集型任务，而本地计算能力有限，因此该方式的扩展性有限。</p>

<p><strong>DistributedRaidNode:</strong> 通过提交mapreduce job来进行parity计算</p>

<h4>2.2.2 主要线程</h4>

<p><strong>TriggerMonitor:</strong> 周期性检查raid-policy配置，根据最新的配置来进行对相应的数据raid化。raid化的调度周期主要收两个配置的影响，raid.config.reload.interval （重新加载raid-policy配置的周期，默认10s）和raid.policy.rescan.interval（重新扫描需要raid化的src的间隔，默认1小时）。简单讲，当新增了一个policy时，默认10s内该policy会被加载执行。而在一个已经raid化的目录中新增了一个文件时，该文件将在1个小时内被raid话。</p>

<p><strong>BlockIntegrityMonitor:</strong> 负责通过DFS的fsck来对DRFS中已经raid化的数据进行检查，检查内容主要包括corrupt（损坏）和decomssion（丢失）的文件。一旦检测到这类文件的存在，BlocIntegrityMonitor会通过其维护的CorruptMonitor和DecomissionMonitor的两个线程来进行数据的修复。BlockIntegrityMonitor对应local和dist两种模式有两个实现，分别为LocalBlockIntegrityMonitor和DistBlockIntegrityMonitor。（可通过raid.blockfix.classname配置项设置，默认为dist）。区别主要在获取的corruptionMonitor和DecomissionMonitor的实现不同。</p>

<p><strong>LocalBlockIntegrityMonitor:</strong> 提供了CorruptMonitor实现会循环通过fsck检查corrupt文件，通过BlockReconstructor.CorruptBlockReconstructor重建这些文件。但该实现不提供Decomissioning文件的监控处理。local模式下corrput文件的重建是在RaidNode上进行的，对大量数据的重建，会对RaidNode有较大的压力。</p>

<p><strong>DistBlockIntegrityMonitor:</strong> Dist模式提供的CorruptionMonitor和DecomissionMonitor是通过DFSck获取corrupt和decomissed的文件列表，计算优先级后，通过向集群提交job来完成重建，Job的输入是一个包含所有文件path的sequence file，Mapper实现是通过Reconstructor来重建每个文件。</p>

<p><strong>BlockFixer(CorruptionMonitor):</strong> BlockIntegrityMonitor构建的用于修复corrupt文件的worker线程。</p>

<p><strong>BlockCopier(DecomissionMonitor):</strong> BlockIntegrityMonitor构建用于修复decomission文件的worker线程。</p>

<p><strong>PlacementMonitor:</strong> PlacementMonitor主要是通过blockMover完成为DRFS中的根据placement策略提供在Datanode之间move block的工具线程。BlockMover通过一个ClusterInfo线程周期性（默认1min）获取集群中live节点的最新topo结构。对于parity block过于集中的节点，需要将其分散开。分散的过程主要是：为每个的block构建一个BlockMoveAction线程，该线程在所有datanode中除当前block所在的节点外随机选取一个datanode，并选取一个proxysource datanode，proxysource datanode是用于将block复制到datanode的源节点，选取规则是优先选取当前block副本所在dn中与目标datanode所属同一rack的节点，如果没有，则从副本列表中随机选取一个作为源节点。</p>

<p><strong>PurgeThread:</strong> PurgeThread封装了PurgeMonitor，它会定期扫描Parity文件中是否有孤儿Parity文件(即拥有该Parity文件的source文件已经不存在了)，如果有则需要将其删除，如果没有，会对Parity文件和对应的source文件进行placement检查。</p>

<p><strong>HarThread:</strong> 为了减少RAID后Parity文件对Namenode的负担，HarThread封装了HarMonitor，它定期对超期的Parity文件进行归档处理(HAR)，超期时间由raid.parity.har.threshold.days指定，默认是3天。</p>

<h2>三、 raid和unraid流程详解</h2>

<h3>3.1 数据raid化</h3>

<p>文件数据的raid化有两种场景，一种是通过raidShell之行 raidFile命令触发</p>

<pre><code>hadoop raidshell -raidFile /path/to/file
</code></pre>

<p>另一种是TiggerMonitor线程周期行扫描policy，根据新的配置信息进行相应的raid化。</p>

<h4>3.1.1 raidShell执行raidFile</h4>

<p>当前client端执行raidfile请求时，大致的处理流程如下：</p>

<p><img src="http://jiangbo.me/images/hdfs/Raid-RaidFile.png" alt="raidFile" /></p>

<ol>
<li>首先检查请求的delay时间，还未到delay时间则不执行</li>
<li>参数处理，包括path路径校验，codec设置等</li>
<li>查询path路径状态，如果是文件或者当前模式是local模式，则执行doLocalRaid，通过RaidNode.doRaid()对path下所有文件进行raid。</li>
<li>如果是目录且当前配置的raid模式是dist，则通过raidNode.submitRaid() rpc请求向RaidNode提交raid请求。</li>
<li>RaidNode接收到client提交的请求后，根据提交的额参数构造一个raid-policy，并添加到configMangaer中。等待RaidNode上TiggerMonitor守护线程下次运行是处理该policy。</li>
</ol>


<h4>3.1.2 triggerMonitor线程处理流程</h4>

<p>triggerMonitor作为RaidNode上的守护线程，周期性从configManager中获取policy列表，对每个policy进行如下处理：</p>

<ol>
<li>查询该policy的状态，如果未执行过，则立即处理，获取path中文件列表。如果该policy已经处理过，过滤其path中尚未处理的file。</li>
<li>如果是local模式，对列表中的file执行RaidNode.doRaid()</li>
<li>如果是dist模式，通过DistRaid构建一个raid job，该job的输入文件是所有待raid文件path构成的sequence file。mapper主要是调用RaidNode.doRaid()对输入中的file path进行raid。</li>
</ol>


<h5>RaidNode.doRaid()流程</h5>

<p>上述表明，hdfs raid中对文件的raid最终都是由RaidNode.doRaid()来完成，不通场景下的区别主要是raid过程的执行地点不同：</p>

<ol>
<li>raidshell执行的local模式或者单个文件，raid过程是在client上完成</li>
<li>local模式下tiggermonitor触发的raid， raid过程是在RaidNode上完成</li>
<li>raidshell执行的dist模式且是目录时进行的raid，或者dist模式下triggermonitor触发的raid，是通过job的方式提交到集群上由每个task节点完成。</li>
</ol>


<p>RaidNode.doRaid()的主要流程如下：</p>

<p><img src="http://jiangbo.me/images/hdfs/Raid-RaidNodeDoRaid.png" alt="RaidNodeDoRaid" /></p>

<ol>
<li>获取文件的block信息，如果block数小于3，则不进行raid。</li>
<li>对于为打到delay时间的也不进行raid</li>
<li>如果已经到达delay时间且block数>2 时进行生成parity文件</li>
<li>生成parity文件过程如图右半部所示：首先获取src文件path，生成parity文件的path，parity文件path的生成规则是 $parity_dir+src_path（codec中配置的是parity_dir是/raid， src文件path是data/file1.log， 那么该文件的parity文件path就是/raid/data/file1.log）</li>
<li>检查相应的parity文件是否已经存在，如果存在，检查parity文件的mtime（更新时间）是否与源文件mtime一致，如果是，则认为该源文件已经raid且是最新。不需要再进行raid。</li>
<li>如果parity文件不存在或不是最新，则重新通过Encoder来生成parity文件</li>
<li>设置parity文件的mtime为源文件的mtime。</li>
<li>检查parity文件的最终状态，主要是mtime是否与源文件一致。通过则raid完成</li>
</ol>


<h4>3.1.3 Encoder.encode过程</h4>

<p>raid过程中最终的编码生成parity的工作有Encoder完成。编码过程主要如下：</p>

<p><img src="http://jiangbo.me/images/hdfs/Raid-EncodeFile.png" alt="EncodeFile" /></p>

<ol>
<li>由于编码过程会比较长，所以先生成™p文件。™p文件的目录可以通过™p_parity_dir配置，默认是™p/$parity_dir</li>
<li>构建™p文件path，™p文件的path为™p目录下parity文件path加上一个随机long值构成，$™p_parity_dir/$parity_file+randomlong。</li>
<li>通过Erasued Code来进行编码到™p文件</li>
<li>删除原有的parity文件</li>
<li>将™p文件重命名为parity文件。</li>
<li>删除™p文件。</li>
</ol>


<p>对于Erasured Code的生成过程大至流程如下：
从源文件中block列表中选取一些（数量有stripe_length指定，默认是10）block，构成一个strip（条？）。通过ParallelStreamReader工具构建一个并行读取10个block的的数据，每个block每次读取1个buff的数据(buffer大小有raid.encoder.bufsize指定，默认是1m)，一次读取构成一个二维byte数组byte[stripe_length][buff_size],这个二维数组做为Erasure Code的输入数据，进行编码生成erasued code。输出也是一个byte二维数组byte[parity_length][buffer_size]。</p>

<p><strong>XOR算法中</strong>:parity_length为1， 即根据10位输入byte生成1位的奇偶校验码。</p>

<p><strong>RS算法中</strong>: parity_length默认为4， 及根据10为输入生成4为的RS code，这四位分别写入4个™p文件中，在一个buffer全部编码完成后，将4个parity文件进行合并。生成一个™p文件。</p>

<h3>3.2 损坏数据的恢复</h3>

<p>raid数据的修复同样也有多个触发场景：</p>

<ol>
<li>client端使用DRFS读取数据发生数据丢失或损坏延长</li>
<li>RaidNode上BlockIntegrityMonitor周期获取block数据发现数据异常时</li>
<li>通过raidshell执行 fixblock时</li>
</ol>


<h4>3.2.1 block读取时修复损坏数据流程</h4>

<p>在client通过DRFS读取raid话的数据是，DRFS首先通过其内部封装的DFS去读block数据，当DFS读取时跑出CorruptionException或DecomissionException时，会被DRFS捕获，并对出错的block在client进行修复。主要流程如下：</p>

<p><img src="" alt="" /></p>

<ol>
<li>在client配置了DRFS并使用DFS作为内置fs时，当通过FS.open获取文件InputStream时，返回一个ExtFSDataInputStream实例。</li>
<li>通过该inputStream读取数据时，首先通过内置DFS读取响应的block，正常情况下，返回需要的数据。</li>
<li>当内置的DFS读取block时跑出CorruptionException或DecomissionException时，会被ExtFSDataInputStream捕获。通过调用RaidNode.unRaidCorrputionBlock()来获取一个恢复的block，并从该block读取数据。</li>
</ol>


<p>RaidNode.unRaidCorruptionBlock()过程首先获取该block的parity文件信息，然后构建一个恢复文件的path路径(该路径位于hdfs.raid.local.recovery.location配置的目录下，默认是/tmp/raidrecovery，文件名为原文件名+&#8221;.&#8221;+随机long+&#8221;.recoveryd&#8221;)，并通过Decoder.fixErasedBlock()来根据parity文件生成恢复文件。</p>

<p><strong>注意:</strong>对于恢复文件所在的文件系统是可以通过fs.raid.recoveryfs.uselocal来配置的，默认是false，即使用DFS，恢复文件将在储与分布式系统中，当配置成true是，使用LocalFileSystem，将恢复文件存储在client端本地。</p>

<h4>3.2.2 BlockIntegrityMonitor线程修复</h4>

<p>RaidNode上的BlockIntegrityMonitor线程会通过DFSck工具检查系统中corrupt或decomission的数据，通过BlockCopier和BlockFixer线程周期行对出错的数据进行修复。local模式下，修复过程在RaidNode上之行，Dist模式下修复过程通过提交Job的方式提交给集群完成。</p>

<p><strong>Local模式</strong>
LocalBlockIntegrity线程的核心是周期调用doFix方法修复corrupt文件，主要流程如下：</p>

<ol>
<li>通过DFSck获取currput文件信息（HTTP访问）</li>
<li>过滤掉不能恢复的corrupt文件（没有parity文件的）</li>
<li>将corrput的文件排序，排序规则如下

<ul>
<li>parity文件优先，source文件在后</li>
<li>parity文件中codec.priority高的在先（codec.priority通过JSON中coder_priority配置）</li>
</ul>
</li>
<li>对排序号的corrupt文件列表依次通过BlockRecontsturer来恢复。</li>
</ol>


<p><strong>Dist模式</strong>
DistBlockIntegrity中的有两个worker线程blockCopier和blockFixer，分别对应修复decomssion和corrput的文件。实际上两个线程的处理流程基本一致，大体如下：</p>

<ol>
<li>检查当前正在运行的修复job数，如果当前job已经大于job上限，则等待之前的job运行完（该上线可以通过raid.blockfix.maxpendingjobs来配置，默认是100L）</li>
<li>通过DFSck获取损坏的文件信息，blockfixer线程获取corrupt文件信息，blockCopier获取decomission文件信息</li>
<li><p>计算获得的损坏文件的优先级：</p>

<p> corrput文件的优先级如下(R为文件副本数，C为该文件corrput的block数)：</p>

<ul>
<li>默认为LOW</li>
<li>R>1 &amp;&amp; C>0时： HIGH</li>
<li>R==1 &amp;&amp; C>1时： HIGH</li>
<li>parityfile corrput &amp;&amp; C>0时： HIGH</li>
</ul>


<p> decomission优先级计算规则如下（D为decomission的block数):</p>

<ul>
<li>默认为LOW</li>
<li>D>4时： HIGH</li>
</ul>
</li>
<li><p>将计算好优先级的文件列表按优先级排序，作为参数构建修复Job。</p></li>
<li>Job的输入是所有需要修复的文件path的sequence file。会根据raid.blockfix.filespertask配置的值进行sync，即在job的split阶段会按照该值设置的进行split，默认是20</li>
<li>Job的Mapper主要是通过Reconstruter在task机上完成响应文件的恢复。</li>
</ol>


<p><strong>注意：</strong>对于修复Job还有一个参数限制，及每次job最多进行的task数，该值为固定值50，这意味着一个Job一次最多能修复的文件数是100个（raid.blockfix.filespertask*50）</p>

<h4>3.2.3 RaidShell之行fixblock</h4>

<p>通过raidshell执行 fixblock时, raidShell会通过BlockReconstructor来完成文件的修复。</p>

<h4>3.2.4 BlockReconstructor文件修复过程</h4>

<p>BlockIntegrityMonitor和RaidShell对文件的修复最终都通过BlockReconstructor来完成。
BlockReconstructor修复文件过程主要分为三类：Har parity文件，parity文件和源数据文件。</p>

<p><strong>Har parity文件</strong></p>

<ol>
<li>获取har文件的基本信息及index</li>
<li>获取har文件中的lost block，对每个block进行如下处理：</li>
<li>在本地文件系统创建该block的临时文件，</li>
<li>对该block涉及的所有parity文件，获取对应的source文件，通过Encoder重新encode，在本地生成parity数。</li>
<li>将本地生成的block数据发送到一个datanode上，datanode的选取规则是从集群中除原block所属节点外随机选取一个。发送过程同时生成block的meta文件。</li>
</ol>


<p><strong>parity文件</strong></p>

<p>parity文件的修复处理相对简单：</p>

<ol>
<li>在本次创建lost block的临时文件</li>
<li>获取parity文件的源文件，通过Encoder重新encode，在本地生成parity文件的block</li>
<li>选取一个dn（选取规则和har parity文件修复一致），将block数据发送到该dn上，并同时生成meta文件</li>
</ol>


<p><strong>源数据文件</strong></p>

<p>源文件的恢复与parity文件的修复相反，是一个decode过程：</p>

<ol>
<li>对于file中丢失的每个block执行修复操作</li>
<li>在本地创建block的临时文件</li>
<li>通过Decoder恢复block数据</li>
<li>选取一个target dn，将block数据发送给target dn，并同时生成meta文件。</li>
</ol>


<h4>3.2.5 Decoder的修复过程</h4>

<p>Decoder的修复过程即一个parity文件的decode过程：</p>

<p><img src="http://jiangbo.me/images/hdfs/Raid-decode.png" alt="Decode" /></p>

<ol>
<li>根据文件中出错的位置，计算出错的block，该block所在的stripe，以及在stripe中的位置，计算parity文件相应block的位置。</li>
<li>通过ParallelStreamReader读取源block数据和parity数据，读取方式与编码时类似</li>
<li>通过Erasured Code将源block和parity数据的进行解码，生成丢失的block数据。</li>
</ol>


<h2>四、参考资料</h2>

<ol>
<li><a href="http://hadoopblog.blogspot.com/2009/08/hdfs-and-erasure-codes-hdfs-raid.html">HDFS and Erasure Codes (HDFS-RAID)</a></li>
<li><a href="http://wiki.apache.org/hadoop/HDFS-RAID">HDFS-RAID wiki</a></li>
<li><a href="http://en.wikipedia.org/wiki/Erasure_code">Erasure Code</a></li>
<li><a href="https://github.com/facebook/hadoop-20/tree/production/src/contrib/raid">Facebook hadoop-20</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS-2246:使用BlockReaderLocal优化本地block读取]]></title>
    <link href="http://jiangbo.me/blog/2012/12/10/hdfs-blockreaderlocal/"/>
    <updated>2012-12-10T16:09:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/12/10/hdfs-blockreaderlocal</id>
    <content type="html"><![CDATA[<h2>一、背景</h2>

<p>HDFS中对Block的读取使用DataXceiver通过Socket发送Packet数据进行，client端通过一个BlockReader来接收Socket中的Block数据。详见<a href="">HDFS数据读取流程</a>。大致流程如下：</p>

<ol>
<li>client端调用FileSystem.open()获取一个DFSInputStream</li>
<li>client端调用DFSInputStream.read(byte[] buffer, int off, int len)读取数据</li>
<li>client端通过DFSInputStream.blockSeekTo()向NameNode发起请求，定位到需要读取的block所在的Datanode，构建一个BlockReader用于读去对应DataNode上的block数据，返回该DataNodeInfo</li>
<li>BlockReader主要负责同DataNode间建立一个Socket链接用于读取数据，BlockReader首先向DataNode发送请求头（包括操作类型，blockId，block时间戳，起始偏移量，读取数据的长度，client名）</li>
<li>DataNode接收到DataXceiverServer接收到client端请求后，构建一个DataXceiver处理该请求。</li>
<li>DataXceiver首先解析请求头，获取请求操作类型，当发现是READ_BLOCK操作后，调用相应的readBlock()方法处理</li>
<li>readBlock方法解析请求中需要的blockId，构建一个BlockSender用于读取磁盘上的block文件数据并发送给client</li>
<li>BlockSender读取磁盘上的block文件，将数据按照chunk通过socket发送给client的blockReader，同时，BlockSender在发送chunk后需要从meta文件中读取该chunk的checksum数据，同样发送给client，用于该chunk的checksum校验。</li>
<li>client端通过BlockReader.readChunks()接收BlockSender发送的chunk数据，并进行checksum校验，校验成功后向DataNode发送checksumOk。</li>
<li>循环6-10，直至当前block的数据全部被读取完成。</li>
<li>循环执行3-11, 直至需要读取的文件数据都被读取完。</li>
</ol>


<p>这个过程是在集群环境想，client读取datanode上数据的一个正常流程，但事实上当client和datanode位于同一个物理节点上时（如Hadoop集群中，task运行在datanode上），这个过程显的有些多余，client可以直接通过本地文件系统api读取文件，而不需要走繁杂的socket流程。</p>

<h2>二、设计实现</h2>

<p>HDFS-2246中提供了一个BlockReaderLocal的实现，当client发现从NameNode返回的Block所属的datanode和client位于同一节点上时，构建一个BlockReaderLocal用于读取本地文件。
上述3-10的流程将简化为：</p>

<ol>
<li>client端向NameNode发起请求获取block所属的datanode信息后，判断该datanode是否和client位于同一节点，是且开启了本地读取功能，则构建一个BlockReaderLocal读取本地文件，否则构建一个BlockReader按照原流程进行。</li>
<li>BlockReaderLocal通过DataNode.getBlockLocaPathInfo()从DataNode获取block的本地文件路径信息。</li>
<li>BlockReaderLocal构建InputStream读取block文件和meta文件信息</li>
<li>对于需要checksum的场景（默认），通过blockReaderLocal.readChunks()按chunk读取本地文件，同时读取meta文件中该chunk的checksum数据，进行校验</li>
<li>对于跳过checksum的场景，直接通过InputStream.read()读取block数据。</li>
</ol>


<h3>扩展DataNode协议接口</h3>

<p>client端需要能够从DataNode获取block文件的本地文件路径信息。因此扩展ClientDataNodeProtocol，增加一个</p>

<pre><code>BlockLocalPathInfo getBlockLocalPathInfo(Block block) throws IOException;
</code></pre>

<p>接口用于获取block的本地路径信息</p>

<h3>本地文件读取</h3>

<p>BlockReaderLocal共过BufferedInputStream直接读取本地文件，注意此处HDFS-2246的patch中使用的是FileInputStream，实际测试过程中发现，FileInputStream对本地文件的读取性能较差， 替换为使用BufferedInputStream</p>

<h3>checksum较验</h3>

<p>为了完成checksum校验，BlockReaderLocal同时需要读取block的meta文件，每当block文件读取一个chunk时需要从meta文件读取一个checksum数据，进行checksum校验，通过校验后进行下一个chunk的读取和校验。由于BlockReaderLocal读取的是本地文件，避免的网络传输对数据的影响，因此可以配置跳过checksum检查，以提高读取性能。默认是需要做checksum的。</p>

<h3>本机判断</h3>

<p>当前patch中的实现主要用IP来判断是否block所在的datanode与client是否位于同一节点上。</p>

<h2>测试</h2>

<p>通过TestDFSIO工具测试一个单节点的集群，2个文件，每个文件1000M
./bin/hadoop jar hadoop-0.19.1-dc-test.jar TestDFSIO -read -nrFiles 2 -fileSize 1000
测试结果对比:
socket读取：</p>

<pre><code>---
12/12/07 13:52:57 INFO mapred.FileInputFormat: ----- TestDFSIO ----- : read
12/12/07 13:52:57 INFO mapred.FileInputFormat:            Date &amp; time: Fri Dec 07 13:52:57 CST 2012
12/12/07 13:52:57 INFO mapred.FileInputFormat:        Number of files: 2
12/12/07 13:52:57 INFO mapred.FileInputFormat: Total MBytes processed: 2000
12/12/07 13:52:57 INFO mapred.FileInputFormat:      Throughput mb/sec: 283.5270768358378
12/12/07 13:52:57 INFO mapred.FileInputFormat: Average IO rate mb/sec: 283.5281982421875
12/12/07 13:52:57 INFO mapred.FileInputFormat:  IO rate std deviation: 0.5685961122141402
</code></pre>

<p>本地读取</p>

<pre><code>---
12/12/07 13:48:59 INFO mapred.FileInputFormat: ----- TestDFSIO ----- : read
12/12/07 13:48:59 INFO mapred.FileInputFormat:            Date &amp; time: Fri Dec 07 13:48:59 CST 2012
12/12/07 13:48:59 INFO mapred.FileInputFormat:        Number of files: 2
12/12/07 13:48:59 INFO mapred.FileInputFormat: Total MBytes processed: 2000
12/12/07 13:48:59 INFO mapred.FileInputFormat:      Throughput mb/sec: 369.61744594344856
12/12/07 13:48:59 INFO mapred.FileInputFormat: Average IO rate mb/sec: 369.6180725097656
12/12/07 13:48:59 INFO mapred.FileInputFormat:  IO rate std deviation: 0.4800772
</code></pre>

<p>另外通过Patch中提供的TestShortCircuitLocalRead工具，测试结果如下：</p>

<p>本地读取并进行checksum校验</p>

<pre><code>true no 1 32000000
---
Iteration 20 took 115453
Iteration 20 took 115803
Iteration 20 took 115748
</code></pre>

<p>socket读取并进行checksum校验（默认）</p>

<pre><code>no no 1 32000000
---
Iteration 20 took 128820
Iteration 20 took 135305
Iteration 20 took 129145
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用FUSE-DFS mount HDFS]]></title>
    <link href="http://jiangbo.me/blog/2012/10/23/mount-hdfs-with-fuse-dfs/"/>
    <updated>2012-10-23T10:24:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/23/mount-hdfs-with-fuse-dfs</id>
    <content type="html"><![CDATA[<h2>介绍</h2>

<p>Hadooop源码中自带了contrib/fuse-dfs模块，用于实现通过libhdfs和fuse将HDFS mount到*inux的本地。</p>

<h2>编译</h2>

<h3>环境</h3>

<ol>
<li>Linux: 2.6.18-164.el5 x86_64</li>
<li>JDK: 1.6.0_23 64bit</li>
<li>Hadoop: 0.19.1 下面假设源码目录为$HADOOP_SRC_HOME</li>
<li>Ant: 1.8.4</li>
<li>GCC: 4.1.2(系统默认)</li>
</ol>


<h3>编译libhdfs</h3>

<h4>修改configure执行权限</h4>

<pre><code>$chmod +x $HADOOP_SRC_HOME/src/c++/pipes/configure
$chmod +x $HADOOP_SRC_HOME/src/c++/utils/configure
</code></pre>

<h4>修改Makefile，调整编译模式</h4>

<p>64位机中，需要修改libhdfs的Makefile，将GCC编译的输出模式由32(-m32)位改为64(-m64)位</p>

<pre><code>CC = gcc
LD = gcc
CFLAGS =  -g -Wall -O2 -fPIC
LDFLAGS = -L$(JAVA_HOME)/jre/lib/$(OS_ARCH)/server -ljvm -shared -m64(这里) -Wl,-x
PLATFORM = $(shell echo $$OS_NAME | tr [A-Z] [a-z])
CPPFLAGS = -m64(还有这里) -I$(JAVA_HOME)/include -I$(JAVA_HOME)/include/$(PLATFORM)
</code></pre>

<h4>编译</h4>

<p>在$HADOOP_HOME目录下执行</p>

<pre><code>$ ant compile -Dcompile.c++=true -Dlibhdfs=true
</code></pre>

<p>编译结果将生成libhdfs库，位于$HADOOP_SRC_HOME/build/libhdfs目录下</p>

<h3>编译fuse-dfs</h3>

<h4>安装fuse库</h4>

<p>fuse-dfs依赖fuse库，可通过</p>

<pre><code>sudo lsmod|grep fuse
</code></pre>

<p>检查是否已经安装，如没有，可通过：</p>

<pre><code>yum -y install fuse fuse-devel fuse-libs
</code></pre>

<p>安装相关依赖库。</p>

<h4>设置编译库路径</h4>

<p>设置编译库路径，将libhdfs的库加入到编译路径中</p>

<pre><code>export LD_LIBRARY_PATH=/usr/lib:/usr/local/lib:$HADOOP_SRC_HOME/build/c++/Linux-amd64-64/lib:$JAVA_HOME/jre/lib/amd64/server
</code></pre>

<h4>编译</h4>

<p>编译contrib/fuse-dfs模块：</p>

<pre><code>ant compile-contrib -Dlibhdfs=1 -Dfusedfs=1
</code></pre>

<p>编译完成将会生成$HADOOP_HOME/build/contrib/fuse-dfs/目录，内有：</p>

<pre><code>fuse-dfs]$ ls
fuse_dfs  fuse_dfs_wrapper.sh  test
</code></pre>

<p>其中fuse_dfs是可执行程序，fuse_dfs_wrapper.sh是包含一些环境变量设置的脚本，不过其中大部分需要修改:(</p>

<h4>修改fuse_dfs_warpper.sh</h4>

<pre><code>#Hadoop安装目录
export HADOOP_HOME=/home/bo.jiangb/yunti-trunk/build/hadoop-0.19.1-dc
#将fuse_dfs加入到PATH
export PATH=$HADOOP_HOME/contrib/fuse_dfs:$PATH
#将hadoop的jar加入到CLASSPATH
for f in ls $HADOOP_HOME/lib/*.jar $HADOOP_HOME/*.jar ; do
export  CLASSPATH=$CLASSPATH:$f
done
#设置机器模式
export OS_ARCH=amd64
#设置JAVA_HOME
export  JAVA_HOME=/home/admin/tools/jdk1.6
#将libhdfs加入到链接库路径中
export LD_LIBRARY_PATH=$JAVA_HOME/jre/lib/$OS_ARCH/server:/home/bo.jiangb/yunti-trunk/build/libhdfs:/usr/local/lib
./fuse_dfs $@
</code></pre>

<h2>使用</h2>

<h3>mount</h3>

<ol>
<li><p>新建一个空目录</p>

<p> $mkdir /tmp/dfs</p></li>
<li><p>挂载dfs
$./fuse_dfs_wrapper.sh dfs://master_node(namenode地址):port /tmp/dfs -d
-d表示debug模式，如果正常，可以将-d参数去掉。</p></li>
</ol>


<h3>unmount</h3>

<p>卸载可通过：</p>

<pre><code>fusermount -u /tmp/dfs
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS源码学习（15）——DataXceiverServer]]></title>
    <link href="http://jiangbo.me/blog/2012/10/18/hdfs-dataxceiver/"/>
    <updated>2012-10-18T22:00:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/18/hdfs-dataxceiver</id>
    <content type="html"><![CDATA[<p>HDFS中有两种类型的通信机制，一种是进行消息传递的Hadoop IPC机制，一种是用于处理数据传输的DataXceiver机制。前者包括client<->namenode之间的通信，以及datanode<->namenode间通信，后者包括client<->datanode, datanode<->datanode间的数据传输。</p>

<h2>DataXceiverServer</h2>

<p>DataNode在启动时会通过DataXceiverServer开启一个Socket端口，负责block数据的读写。DataXceiverServer本身作为一个守护线程，监听dfs.datanode.address配置的数据读写服务端口。当有请求来时，新建一个DataXceiver线程处理请求。</p>

<h2>DataXceiver</h2>

<p>DataXceiver线程用于处理一个读/写数据流请求，其run方法入下主要是根据请求中不同的请求类型，调用响应的处理方法。</p>

<p>请求操作类型定义在DataTransferProtocol中，主要有：</p>

<ol>
<li>OP_WRITE_BLOCK： 写入Block数据，对应writeBlock()方法</li>
<li>OP_READ_BLOCK： 读取Block数据，对应readBlock()方法</li>
<li>OP_READ_METADATA： 读取Block元数据，对应readMetadata()方法</li>
<li>OP_REPLACE_BLOCK： 替换Block，将block发送到目标datanode上，用于IO负载均衡；对应replaceBlock()方法。</li>
<li>OP_COPY_BLOCK：复制Block，将block发送到proxy source上，用于IO负载均衡；对应copyBlock()方法。</li>
<li>OP_BLOCK_CHECKSUM：获取Block的checksum；对应getBlockChecksum()方法。</li>
</ol>


<p>请处理返回的状态也定义在该类中：</p>

<ol>
<li>OP_STATUS_SUCCESS： 成功</li>
<li>OP_STATUS_ERROR： 请求出错</li>
<li>OP_STATUS_ERROR_CHECKSUM： checksum校验出错</li>
<li>OP_STATUS_ERROR_INVALID： 读取无效block</li>
<li>OP_STATUS_ERROR_EXISTS：block不存在</li>
<li>OP_STATUS_CHECKSUM_OK： checksum校验正常</li>
</ol>


<h3>1.读取block——readBlock()</h3>

<p>OP_READ_BLOCK的请求数据格式如下：</p>

<p><img src="http://jiangbo.me/images/hdfs/ReadBlock.png" alt="READ_BLOCK" /></p>

<p>返回数据格式如下：</p>

<p><img src="http://jiangbo.me/images/hdfs/ReadResponse.png" alt="READ_Response" /></p>

<p>readBlock()主要从disk读取block数据，构建一个DataOutputStream数据流，并新建一个BlockSender将这个数据流发送出去（datanode或者client）。</p>

<p>BlockSender.sendBlock()发送的Block的流程大体如下：</p>

<ol>
<li>读取block的meta信息，获得checksum并发送</li>
<li>发送数据读取的偏移量</li>
<li>将block数据切分为packet，发送给client</li>
<li>所有packet发送完之后，关闭checksum文件和block文件</li>
</ol>


<h3>2.写入block——writeBlock()</h3>

<p>OP_WRITE_BLOCK的请求数据格式如下：</p>

<p><img src="http://jiangbo.me/images/hdfs/WriteRequest.png" alt="WRITE_BLOCK" /></p>

<p>writeBlock()解析请求信息，构建一个BlockReceiver处理数据接收和写入，在client（或上一datanode节点）-当前datanode节点-下一datanode节点之间建立一个如下连接。</p>

<p><img src="http://jiangbo.me/images/hdfs/WriteBlock.png" alt="WRITE_BLOCK" /></p>

<ol>
<li>BlockReceiver从上按packet一节点读取数据，写入到本地disk</li>
<li>如有下一备份节点，将该packet转发给下一节点</li>
<li>将该packet加入到ackqueue队列中等待ack消息</li>
<li>当下一节点完成该packet写入后会返回该packet对应的ack信息</li>
<li>PakcetResponder接收到ack信息后，将ackqueue中该packet删除，并向前置节点发送ack信息</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS源码学习（14）——Client代码结构]]></title>
    <link href="http://jiangbo.me/blog/2012/10/18/hdfs-client-code/"/>
    <updated>2012-10-18T21:59:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/18/hdfs-client-code</id>
    <content type="html"><![CDATA[<p>Client核心代码有DistributedFileSystem和DFSClient。</p>

<p><img src="http://jiangbo.me/images/hdfs/Client.png" alt="Client" /></p>

<p>DistributedFileSystem扩展子FileSystem，在为客户端提供一个文件系统接口实现。其内部使用DFSClient完成各类文件操作。</p>

<p>DFSClient使用ClientProtocol与NameNode通信，完成文件元信息操作。并通过Socket连接完成与DataNode间的block读写操作。</p>

<p>DFSClient代码结构如下：</p>

<p><img src="http://jiangbo.me/images/hdfs/DFSClient.png" alt="DFSClient" /></p>

<ol>
<li>LeaseChecker主要用于lease检查和续约。</li>
<li>DFSOutputStream用于提供带buffer的字节流写入功能。client在写入数据时先将数据缓存在本地。并将数据切分成多个packet（默认每个packet为64K）。每个packet又被拆分成多个chunk（默认512Byte），每个chunk都有一个checksum。client写满一个packet后会将该packet加入到一个dataqueue中。由DataStreamer线程负责将每个packet发送给datanode pipeline。发送完一个pakcet，streamer会将其从dataqueue移至ackqueue中。ResponseProcessor负责接收datanode发回的ack信息，每成功接收一个packet的ack信息，ResponseProcessor会将ackqueue中该packet删除。</li>
<li>DFSInputStream用于提供字节流的读取，其内部封装了与NN和DN的交互</li>
<li>DataStreamer: 负责向datanode pipeline发送packet。其本身是一个Daemon线程，从namenode获取blockId和block存放位置，将packet发送给pipeline中的datanode，每个packet都有一个seqId，每个packet发送完时都会收到datanode的ack信息。当收到所有packet的ack信息后（表示该block已发送完），streamer关闭该block。</li>
<li>ResponseProcessor:用于接收datanode返回ack信息，并将响应ackqueue中的packet删除</li>
</ol>


<h2>创建文件</h2>

<ol>
<li>client向NameNode发起创建文件请求</li>
<li>NameNode.create（）处理创建文件请求，检查是否有重名，当前是否处于Safe-mode，是否有权限创建文件， 校验通过后创建一个INode记录。</li>
<li>NameNode将创建文件的事件记录到EditLog中</li>
<li>INode被创建后，NameNode发放给Client一个lease，Client可以使用这个lease通过ClientProtocol访问，进行只读操作。（写操作需要等文件close）</li>
</ol>


<h2>写入流程</h2>

<p>client写入流程如下图所示：</p>

<p><img src="http://jiangbo.me/images/hdfs/ClientWrite.png" alt="ClientWrite" /></p>

<ol>
<li>Client向NameNode发起创建文件的RPC请求</li>
<li>NameNode检查文件是否已经存在，是否有权创建等，成功则创建一个文件记录，并发放给Client一个lease</li>
<li>Client获得lease之后开始进行数据写入，写入的数据首先被缓存本地，并被拆分为多个packet，放置到dataqueue队列中</li>
<li>DataStreamer线程负责检查dataqueue队列，发现有数据时且没有可用block时，向NameNode发送addBlock()请求，申请一个分配一个block空间。NameNode返回给DataStreamer一个blockId和用于存放block的datanode list</li>
<li>DataStreamer将每个packet数据发送给datanode pipeline，并将该packet移至ackqueue</li>
<li>datanode pipeline中第一个datanode收到packet之后存储到本地block中并穿行备份至后续datanode中</li>
<li>pipeline中datanode存储好packet之后会逆序返回ack信息，并最终返回给client.</li>
<li>Client端ResponseProcessor捕获到每个packet的ack信息时会将响应ackqueue中的packet删除</li>
<li>当所有数据都写入完成后，client会向NameNode发起一个complete RPC请求，告知文件最新的时间戳和已经发送给datanode的block长度。NameNode检查所有block的副本信息，只有所有block的副本数均满足最低要求时，complete会返回成功。</li>
<li>最后，NameNode将收回client持有的lease。</li>
</ol>


<p>NameNode处理addBlock()请求的流程大致如下：</p>

<ol>
<li>校验client是否有该文件的lease</li>
<li>清理上一次写入记录，包括：a.提交上一次写入，b.更新lease有效期；c.将完成的写入记录到EditLog中</li>
<li>清理完毕之后，使用BlockManager分配指定副本数个block及其对应的datanode信息，返回给client</li>
</ol>


<p>DataNode处理block写入的流程大致如下：</p>

<ol>
<li>将block复制到本地磁盘</li>
<li>发送block received消息给NameNode告知写入了一个新的block</li>
<li>将block数据发送给datanode pipeline中下一个datanode，进行备份</li>
<li>返回一个ack消息给前一个调用者</li>
</ol>


<p>后续的datanode收到上一个datanode的备份block请求是做类似的操作。</p>

<h2>读取流程</h2>

<p>读取流程相对简单写，如下所示 ：</p>

<p><img src="http://jiangbo.me/images/hdfs/ClientRead.png" alt="ClientWrite" /></p>

<ol>
<li>client向NameNode发起RPC请求，获取文件的blockLocation信息</li>
<li>NameNode返回一定长度（10*defaultBlockSize）block的datanode位置信息</li>
<li>Client根据返回的blockLocation信息选取距自己最近（同一节点&lt;通一机架&lt;同一机房）的datanode读取数据，读完一个block会对该block进行checksum校验。如果校验正确则关闭与该datanode连接，去读下一个block；如果校验失败，则通知NameNode该block在当前datanode上的副本损坏了，并继续从datanode列表中获取一个datanode，重新读取该block。</li>
<li>当本次获取的blockLocation中的block全部读完，且该文件还有block时，重复1，2，3过程，直至所有blcok全部读完。</li>
</ol>


<h2>关闭文件（complete）</h2>

<ol>
<li>当Client完成文件写入之后，会调用complete()通知NameNode文件写入完成了，该请求会提交文件写入的最后一个block信息并且告知NameNode写入的block总数以及最新时间戳。</li>
<li>NameNode收到请求后会检查是否所有的block的事物都已经提交了，并且每个block的副本数都达到了最小值。如果是则返回true，否则返回false。</li>
<li>Client收到返回值后如果失败则重试几次。</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS源码学习（13）——DataNode启动过程]]></title>
    <link href="http://jiangbo.me/blog/2012/10/18/hdfs-datanode-startup/"/>
    <updated>2012-10-18T21:57:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/18/hdfs-datanode-startup</id>
    <content type="html"><![CDATA[<h2>main()</h2>

<pre><code>  public static void main(String args[]) {
    try {
      StringUtils.startupShutdownMessage(DataNode.class, args, LOG);
      DataNode datanode = createDataNode(args, null);
      if (datanode != null)
        datanode.join();
    } catch (Throwable e) {
      LOG.error(StringUtils.stringifyException(e));
      System.exit(-1);
    }
  }
</code></pre>

<h2>createDataNode()</h2>

<pre><code>  public static DataNode createDataNode(String args[],
                                 Configuration conf) throws IOException {
    // 初始化datanode
    DataNode dn = instantiateDataNode(args, conf);
    // 启动datanode后台线程
    runDatanodeDaemon(dn);
    return dn;
  }
</code></pre>

<h3>1.instantiateDataNode（）</h3>

<pre><code>  public static DataNode instantiateDataNode(String args[],
                                      Configuration conf) throws IOException {
    // 处理配置
    if (conf == null)
      conf = new Configuration();
    if (!parseArguments(args, conf)) {
      printUsage();
      return null;
    }
    if (conf.get("dfs.network.script") != null) {
      LOG.error("This configuration for rack identification is not supported" +
          " anymore. RackID resolution is handled by the NameNode.");
      System.exit(-1);
    }
    // 获取data目录配置
    String[] dataDirs = conf.getStrings("dfs.data.dir");
    dnThreadName = "DataNode: [" +
                        StringUtils.arrayToString(dataDirs) + "]";
    //创建datanode实例
    return makeInstance(dataDirs, conf);
  }
</code></pre>

<h4>1.1. makeInfstance()</h4>

<p>该方法主要用于检查给定的data目录中至少有一个可以创建，并实例化DataNode</p>

<pre><code>  public static DataNode makeInstance(String[] dataDirs, Configuration conf)
    throws IOException {
    ArrayList&lt;File&gt; dirs = new ArrayList&lt;File&gt;();
    for (int i = 0; i &lt; dataDirs.length; i++) {
      File data = new File(dataDirs[i]);
      try {
        DiskChecker.checkDir(data);
        dirs.add(data);
      } catch(DiskErrorException e) {
        LOG.warn("Invalid directory in dfs.data.dir: " + e.getMessage());
      }
    }
    if (dirs.size() &gt; 0) 
      return new DataNode(conf, dirs);
    LOG.error("All directories in dfs.data.dir are invalid.");
    return null;
  }
</code></pre>

<h4>1.2 new DataNode()</h4>

<pre><code>  DataNode(Configuration conf, 
           AbstractList&lt;File&gt; dataDirs) throws IOException {
    // 设置配置信息
    super(conf);
    datanodeObject = this;
    supportAppends = conf.getBoolean("dfs.support.append", false);
    this.conf = conf;
    try {
      // 启动DataNode
      startDataNode(conf, dataDirs);
    } catch (IOException ie) {
      shutdown();
      throw ie;
    }
  }
</code></pre>

<h5>1.2.1 startDataNode()</h5>

<p>代码较长，仅列出主要步骤：</p>

<ol>
<li>设置配置信息</li>
<li>向NameNode发起RPC请求，获取版本和StorageID信息</li>
<li>获取启动配置</li>
<li>初始化存储信息，构建FSDataSet</li>
<li>获取可用的端口号</li>
<li>调整注册信息中的机器名，加上端口号</li>
<li>初始化DataXceiverServer</li>
<li>设置blockReport和heartbeat各自的时间间隔</li>
<li>初始化blockScanner</li>
<li>初始胡并启动servlet info server，提供内容查询的http服务</li>
<li>初始化ipc server，该ipc server主要用于完成DataNode间的block recover。</li>
</ol>


<h3>runDatanodeDaemon()</h3>

<pre><code>  public static void runDatanodeDaemon(DataNode dn) throws IOException {
    if (dn != null) {
      //register datanode
      dn.register();
      dn.dataNodeThread = new Thread(dn, dnThreadName);
      dn.dataNodeThread.setDaemon(true); // needed for JUnit testing
      dn.dataNodeThread.start();
    }
  }
</code></pre>

<h4>2.1 向NameNode注册 —— dn.register();</h4>

<pre><code>  private void register() throws IOException {
    if (dnRegistration.getStorageID().equals("")) {
      setNewStorageID(dnRegistration);
    }
    while(shouldRun) {
      try {
        // reset name to machineName. Mainly for web interface.
        dnRegistration.name = machineName + ":" + dnRegistration.getPort();
        // 通过NameProtocal向NameNode注册
        dnRegistration = namenode.register(dnRegistration);
        break;
      } catch(SocketTimeoutException e) {  // namenode is busy
        LOG.info("Problem connecting to server: " + getNameNodeAddr());
        try {
          Thread.sleep(1000);
        } catch (InterruptedException ie) {}
      }
    }
    assert ("".equals(storage.getStorageID()) 
            &amp;&amp; !"".equals(dnRegistration.getStorageID()))
            || storage.getStorageID().equals(dnRegistration.getStorageID()) :
            "New storageID can be assigned only if data-node is not formatted";
    if (storage.getStorageID().equals("")) {
      storage.setStorageID(dnRegistration.getStorageID());
      storage.writeAll();
      LOG.info("New storage id " + dnRegistration.getStorageID()
          + " is assigned to data-node " + dnRegistration.getName());
    }
    if(! storage.getStorageID().equals(dnRegistration.getStorageID())) {
      throw new IOException("Inconsistent storage IDs. Name-node returned "
          + dnRegistration.getStorageID() 
          + ". Expecting " + storage.getStorageID());
    }

    if (supportAppends) {
      Block[] bbwReport = data.getBlocksBeingWrittenReport();
      long[] blocksBeingWritten = BlockListAsLongs.convertToArrayLongs(bbwReport);
      //如果支持append，则报告正在写入的block信息
      namenode.blocksBeingWrittenReport(dnRegistration, blocksBeingWritten);
    }
    // 调整下一次的BR时间，使其在下次heartbeat时进行
    scheduleBlockReport(initialBlockReportDelay);
  }
</code></pre>

<h4>2.2 启动datanode线程 —— dn.dataNodeThread.start();</h4>

<p>datanode线程本身非常简单，不停调用offerSevice提供服务：</p>

<pre><code>  public void run() {
    LOG.info(dnRegistration + "In DataNode.run, data = " + data);

    // start dataXceiveServer
    dataXceiverServer.start();
    new Thread(new CrashVolumeChecker()).start();//added by wukong

    while (shouldRun) {
      try {
        startDistributedUpgradeIfNeeded();
        offerService();
      } catch (Exception ex) {
        LOG.error("Exception: " + StringUtils.stringifyException(ex));
        if (shouldRun) {
          try {
            Thread.sleep(5000);
          } catch (InterruptedException ie) {
          }
        }
      }
    }

    LOG.info(dnRegistration + ":Finishing DataNode in: "+data);
    shutdown();
  }
</code></pre>

<h5>2.2.1 offerService()</h5>

<p>offerService的核心是周期性进行heartbeat和blockReport，主要流程如下：</p>

<p><img src="http://jiangbo.me/images/hdfs/offerService.png" alt="offerService" /></p>

<pre><code>  public void offerService() throws Exception {

    LOG.info("using BLOCKREPORT_INTERVAL of " + blockReportInterval + "msec" + 
       " Initial delay: " + initialBlockReportDelay + "msec");
    LOG.info("using DELETEREPORT_INTERVAL of " + deletedReportInterval + "msec");
    LOG.info("using HEARTBEAT_INTERVAL of " + heartBeatInterval + "msec");
    LOG.info("using HEARTBEAT_EXPIRE_INTERVAL of " + heartbeatExpireInterval + "msec");

    //
    // Now loop for a long time....
    //

    while (shouldRun) {
      try {
        long startTime = now();

        //
        // Every so often, send heartbeat or block-report
        //

        if (startTime - lastHeartbeat &gt; heartBeatInterval /* 3 secs*/) {
          //
          // All heartbeat messages include following info:
          // -- Datanode name
          // -- data transfer port
          // -- Total capacity
          // -- Bytes remaining
          //
          lastHeartbeat = startTime;
          DatanodeCommand[] cmds = namenode.sendHeartbeat(dnRegistration,
                                                       data.getCapacity(),
                                                       data.getDfsUsed(),
                                                       data.getRemaining(),
                                                       xmitsInProgress.get(),
                                                       getXceiverCount());
          myMetrics.heartbeats.inc(now() - startTime);
          //LOG.info("Just sent heartbeat, with name " + localName);
          if (!processCommand(cmds))
            continue;
        }

        reportReceivedBlocks();

        DatanodeCommand cmd = blockReport();
        processCommand(cmd);

        // start block scanner
        if (blockScanner != null &amp;&amp; blockScannerThread == null &amp;&amp;
            upgradeManager.isUpgradeCompleted()) {
          LOG.info("Starting Periodic block scanner.");
          blockScannerThread = new Daemon(blockScanner);
          blockScannerThread.start();
        }

        //
        // There is no work to do;  sleep until hearbeat timer elapses, 
        // or work arrives, and then iterate again.
        //
        long waitTime = heartBeatInterval - (System.currentTimeMillis() - lastHeartbeat);
        synchronized(receivedAndDeletedBlockList) {
          if (waitTime &gt; 0 &amp;&amp; receivedAndDeletedBlockList.size() == 0) {
            try {
              receivedAndDeletedBlockList.wait(waitTime);
            } catch (InterruptedException ie) {
            }
            delayBeforeBlockReceived();
          }
        } // synchronized

      } catch(RemoteException re) {
        String reClass = re.getClassName();
        if (UnregisteredDatanodeException.class.getName().equals(reClass) ||
            DisallowedDatanodeException.class.getName().equals(reClass) ||
            IncorrectVersionException.class.getName().equals(reClass)) {
          LOG.warn("DataNode is shutting down: " + 
                   StringUtils.stringifyException(re));
          shutdown();
          return;
        }
        LOG.warn(StringUtils.stringifyException(re));
      } catch (IOException e) {
        LOG.warn(StringUtils.stringifyException(e));
      }
    } // while (shouldRun)
  } // offerService
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS源码学习（12）——DataNode主要数据结构]]></title>
    <link href="http://jiangbo.me/blog/2012/10/18/hdfs-datanode-structure/"/>
    <updated>2012-10-18T21:56:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/18/hdfs-datanode-structure</id>
    <content type="html"><![CDATA[<p>HDFS中DataNode主要负责维护block->stream bytes的映射关系，即实际block数据的存储。
一个datanode的磁盘上存储目录下实际的文件部署结构如下：</p>

<pre><code>data/
├── blocksBeingWritten
├── current
│   ├── VERSION
│   ├── blk_-1148021215131449924
│   ├── blk_-1148021215131449924_1001.meta
│   ├── blk_-8598609183581346893
│   ├── blk_-8598609183581346893_1002.meta
│   ├── blk_6693595845022390257
│   ├── blk_6693595845022390257_1003.meta
│   └── dncp_block_verification.log.curr
├── detach
├── storage
└── tmp
</code></pre>

<p>data目录的路径是hdfs-site.xml中配置的dfs.data.dir的路径，表示每个datanode上数据存储的目录</p>

<p>1) blocksBeingWritten：当前正在写入的block，写完之后会将block移至current目录</p>

<p>2) current：当前已经写入的block文件目录</p>

<p>2.1) VERSION为存储的VERSION文件，包括namespaceId，存储Id，存储版本，存储类型，创建时间戳等信息</p>

<p>2.2）blk-*:文件为实际的block数据文件</p>

<p>2.3）blk-*_xxx.meta: block的元信息文件</p>

<p>2.4）dncp_*.log.curr: 当前copy文件</p>

<p>3) detach：copy-on-write使用的目录</p>

<p>4) tmp： 临时目录，DataNode启动时会检查 tmp的数据并删除。</p>

<h2>Storage相关</h2>

<p>Storage用于描述存储的类型，状态，目录等信息。
其主要结构如下：
<img src="http://jiangbo.me/images/hdfs/Storage.png" alt="Storage" /></p>

<h3>StorageInfo</h3>

<p>StorageInfo表示一个存储的通用信息，包括：</p>

<ol>
<li>layoutVersion： 存储文件中的版本号</li>
<li>namespaceId： 存储所属的命名空间ID</li>
<li>ctime： 该存储创建的时间戳</li>
</ol>


<h3>Storage</h3>

<p>存储信息的抽象类，管理一个server（NameNode或DataNode）上的存储目录。</p>

<p>Storage有两个关键属性：</p>

<ol>
<li>storageType: 表示该存储所属的节点类型（NameNode或是DataNode）</li>
<li>storageDirs: 该存储上存储目录的列表(ArrayList<StorageDirectory>),StorageDirectory表示一个存储目录。</li>
</ol>


<h4>StorageDirectory</h4>

<p>表示一个存储目录，有三个属性：</p>

<ol>
<li>root：根目录</li>
<li>lock：当前目录的文件锁</li>
<li>dirType：目录类型</li>
</ol>


<h4>StorageSate</h4>

<p>表示存储的状态：</p>

<ol>
<li>NON_EXISTENT: 目录不存在</li>
<li>NOT_FORMATTED: 目录未格式化</li>
<li>COMPLETE_UPGRADE: 升级完成</li>
<li>RECOVER_UPGRADE: 撤销升级</li>
<li>COMPLETE_FINALIZE: 提交完成</li>
<li>COMPLETE_ROLLBACK: 回滚完成</li>
<li>RECOVER_ROLLBACK: 撤销回滚</li>
<li>COMPLETE_CHECKPOINT: checkpoint完成</li>
<li>RECOVER_CHECKPOINT: 撤销checkpoint</li>
<li>NORMAL: 正常</li>
</ol>


<h3>DataStorage</h3>

<p>DataStorage是DataNode上使用的存储类，指定了datanode上各类存储文件的前缀：</p>

<ol>
<li>subdir：子目录前缀</li>
<li>blk_：块文件前缀</li>
<li>dncp_：拷贝文件前缀</li>
</ol>


<h2>DatanodeBlockInfo</h2>

<p>DataNode使用DatanodeBlockInfo管理block和其元数据之间的映射关系，结构如下：</p>

<p><img src="http://jiangbo.me/images/hdfs/DatanodeBlockInfo.png" alt="DatanodeBlockInfo" /></p>

<ol>
<li>volmun：block所属的卷</li>
<li>file：block文件</li>
<li>detached：是否完成copy-on-write</li>
</ol>


<h2>FSDataSet相关</h2>

<p>DataNode通过FSDataSet来完成数据的存储。FSDataset类结构如下：
<img src="http://jiangbo.me/images/hdfs/FSDataset.png" alt="FSDataset" /></p>

<h3>FSVolume</h3>

<p>FSVolumne用于进行block文件所属的卷管理，统计存储目录额使用情况，其中：</p>

<ol>
<li>currentDir： 当前数据目录, 对应data/current目录</li>
<li>dataDir： 数据目录</li>
<li>tmpDir： 临时目录, 对应data/tmp目录</li>
<li>dtacheDir: 用于实现写时复制的文件，对应data/detach目录</li>
<li>usage: 目录使用的空间</li>
<li>dfsusage: dfs使用的空间</li>
<li>reseved: 空余空间</li>
<li>blocksBeingWritten: 正在写入的block，对应data/blocksBeingWritten目录</li>
</ol>


<h3>FSVolumeSet</h3>

<p>FSVolumeSet是FSVolume的集合，提供了所有容量，剩余空间等方法。其中getNextVolume中提供了round-robin策略选取下一个volume，从而实现简单的IO负载均衡，提高IO处理能力。</p>

<h3>FSDataSet</h3>

<p>FSDataSet是在FSVolumeSet之上进行封装实现FSDatasetInterface借口，向外提供块查询和操作方法。</p>

<p>其中有几个主要属性:</p>

<ol>
<li>volumes: 卷集合（FSVolumeSet）</li>
<li>ongoingCreates: 当前活动的文件</li>
<li>maxBlocksPerDir: 每个目录下最多能存放发block数，可通过dfs.datanode.numblocks配置</li>
<li>volumeMap：块与块文件的映射信息(HashMap&lt;Block, DatanodeBlockInfo>)，当前集合中所有的块信息均维护在该map中</li>
</ol>


<h3>FSDir</h3>

<p>用于构建block块在datanode磁盘上的层次结构，默认情况下每个目录下最多64个子目录，最多能存储64个块。目录初始化时会递归扫描目录下的所有子目录和文件，构建一个树形结构。</p>

<p>addBlock时，首先尝试在当前目录新加块，如果当前目录没有空闲空间，则尝试在子目录中添加，如果没有子目录，则新建一个子目录。</p>

<h3>BlockAndFile</h3>

<p>Block与其文件名的封装</p>

<h3>ActiveFile</h3>

<p>表示一个当前活动中的文件</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS源码学习（11）——SecondaryNameNode]]></title>
    <link href="http://jiangbo.me/blog/2012/10/18/hdfs-secondary-namenode/"/>
    <updated>2012-10-18T21:55:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/18/hdfs-secondary-namenode</id>
    <content type="html"><![CDATA[<h2>概述</h2>

<p>SecondaryNameNode在HDFS中的主要作用是帮助master NameNode周期性执行checkpoint操作。</p>

<p>NameNode将运行过程中对文件的修改记录保存在EditLog中。当NameNode重新启动时会从FSImage中加载命名空间镜像，并将EditLog中的内容合并到FSImage中，将合并后的FSImage写入到磁盘，同时清空EditLog，共后续使用。但如果NameNode长时间不重启，随时间增长，EditLog将会越来越大（每次文件操作都要记录），大量占用NameNode磁盘空间，且会导致下一次重启花费大量时间在合并Editlog上。</p>

<p>为了解决这个问题，SecondaryNameNode会定期从NameNode下载最新的FSImage和EditLog，合并editLog日志到FSImage，将合并后的FSImage上传到NameNode，并清空NameNode上的Editlog，将EditLog日志大小控制在一定限度下。</p>

<h2>代码解析</h2>

<p>SecondaryNameNode代码结构如下：</p>

<p><img src="http://jiangbo.me/images/hdfs/SecondaryNameNode.png" alt="SecondaryNameNode" /></p>

<p>SecondaryNameNode本身就是实现了Runnable接口，即一个可执行线程。</p>

<p>checkpointImage表示当前SecondNameNode上的FsImage镜像，该类CheckpointStorage扩展自FSImage。</p>

<p>其中有两个主要的可配置属性：</p>

<ol>
<li>checkpointPeriod: 两次检查点的间隔时间，可通过fs.checkpoint.period配置</li>
<li>checkpointSize: EditLog文件的最大值，当EditLog超过这个最大值时会强制之行checkpoint，可通过fs.checkpoint.size配置，默认是64M</li>
</ol>


<h3>run()</h3>

<p>run方法代码如下：</p>

<pre><code>  public void run() {

    //
    // Poll the Namenode (once every 5 minutes) to find the size of the
    // pending edit log.
    //
    long period = 5 * 60;              // 5 minutes
    long lastCheckpointTime = 0;
    if (checkpointPeriod &lt; period) {
      period = checkpointPeriod;
    }

    while (shouldRun) {
      try {
        Thread.sleep(1000 * period);
      } catch (InterruptedException ie) {
        // do nothing
      }
      if (!shouldRun) {
        break;
      }
      try {
        long now = System.currentTimeMillis();

        long size = namenode.getEditLogSize();
        if (size &gt;= checkpointSize || 
            now &gt;= lastCheckpointTime + 1000 * checkpointPeriod) {
          doCheckpoint();
          lastCheckpointTime = now;
        }
      } catch (IOException e) {
        LOG.error("Exception in doCheckpoint: ");
        LOG.error(StringUtils.stringifyException(e));
        e.printStackTrace();
      } catch (Throwable e) {
        LOG.error("Throwable Exception in doCheckpoint: ");
        LOG.error(StringUtils.stringifyException(e));
        e.printStackTrace();
        Runtime.getRuntime().exit(-1);
      }
    }
  }
</code></pre>

<p>其核心就是周期性（默认每个5分钟)调用doCheckpoint().</p>

<h3>doCheckpoint()</h3>

<pre><code>  void doCheckpoint() throws IOException {

    // 准备合并所需的空间
    startCheckpoint();

    // 通知NameNode将修改信息记录到新的editlog中，并获取一个用于上传合并后的fsimage的token
    CheckpointSignature sig = (CheckpointSignature)namenode.rollEditLog();

    // error simulation code for junit test
    if (ErrorSimulator.getErrorSimulation(0)) {
      throw new IOException("Simulating error0 " +
                            "after creating edits.new");
    }
    //从NameNode获取fsimage和editslog
    downloadCheckpointFiles(sig);   // Fetch fsimage and edits
    //合并editlog到fsimage
    doMerge(sig);                   // Do the merge

    //上传合并后的fsimage到NameNode
    putFSImage(sig);

    // error simulation code for junit test
    if (ErrorSimulator.getErrorSimulation(1)) {
      throw new IOException("Simulating error1 " +
                            "after uploading new image to NameNode");
    }
    // 通知NameNode使用该fsimage作为最新的镜像
    namenode.rollFsImage();
    checkpointImage.endCheckpoint();

    LOG.warn("Checkpoint done. New Image Size: " 
              + checkpointImage.getFsImageName().length());
  }
</code></pre>

<h4>1. startCheckpoint()</h4>

<p>该方法主要用于准备合并所需的磁盘空间，代码如下：</p>

<pre><code>  private void startCheckpoint() throws IOException {
    checkpointImage.unlockAll();
    // 关闭当前Editlog
    checkpointImage.getEditLog().close();
    // 检查当前checkpoints目录，如果不存在则创建一个新目录，如果目录中存在异常，则尝试恢复该目录
    checkpointImage.recoverCreate(checkpointDirs, checkpointEditsDirs);
    // 为新的checkpoint准备目录空间，将当前的目录空间更名为lastcheckpoint.™p，新建一个current目录 
    checkpointImage.startCheckpoint();
  }
</code></pre>

<h4>2. namenode.rollEditLog();</h4>

<p>namenode.rollEditLog()实际通过NameNodeProtocol调用NameNode.rollEditLog()方法，并最终调用FSImage.rollEditLog()，该方法主要完成：</p>

<ol>
<li>调用FSEditLog.rollEditLog()关闭当前editLog，新建一个editLog：edits.new</li>
<li>返回一个CheckpointSignature做为上传合并后镜像的token</li>
</ol>


<h5>2.1. FSEditLog.rollEditLog()</h5>

<p>该方法主要完成：</p>

<ol>
<li>关闭当前editlog， 打开一个新的editlog： edit.new</li>
<li>返回editlog的最新更新时间</li>
</ol>


<p>代码结构如下</p>

<pre><code>  synchronized void rollEditLog() throws IOException {
    //检查edit.new是否已经存在，如果存在，检查是否所有目录都存在，如果是则认为edits.new已经建好了，直接返回
    //
    if (existsNew()) {
      for (Iterator&lt;StorageDirectory&gt; it = 
               fsimage.dirIterator(NameNodeDirType.EDITS); it.hasNext();) {
        File editsNew = getEditNewFile(it.next());
     if (!editsNew.exists()) { 
          throw new IOException("Inconsistent existance of edits.new " +
                                editsNew);
        }
      }
      return; // nothing to do, edits.new exists!
    }

    //关闭当前的editLog
    close();                     // close existing edit log

    //
    // 新建一个editLog： edits.new
    //
    for (Iterator&lt;StorageDirectory&gt; it = 
           fsimage.dirIterator(NameNodeDirType.EDITS); it.hasNext();) {
      StorageDirectory sd = it.next();
      try {
        EditLogFileOutputStream eStream = 
             new EditLogFileOutputStream(getEditNewFile(sd));
        eStream.create();
        editStreams.add(eStream);
      } catch (IOException e) {
        // remove stream and this storage directory from list
        processIOError(sd);
       it.remove();
      }
    }
  }
</code></pre>

<h4>3. downloadCheckpointFiles()</h4>

<p>该方法用于从NameNode下载FSImage和FSEditLog，代码结构如下：</p>

<pre><code>  private void downloadCheckpointFiles(CheckpointSignature sig
                                      ) throws IOException {

    checkpointImage.cTime = sig.cTime;
    checkpointImage.checkpointTime = sig.checkpointTime;

    // 获取fsimage
    String fileid = "getimage=1";
    File[] srcNames = checkpointImage.getImageFiles();
    assert srcNames.length &gt; 0 : "No checkpoint targets.";
    TransferFsImage.getFileClient(fsName, fileid, srcNames);
    LOG.info("Downloaded file " + srcNames[0].getName() + " size " +
             srcNames[0].length() + " bytes.");

    // 获取editlog
    fileid = "getedit=1";
    srcNames = checkpointImage.getEditsFiles();
    assert srcNames.length &gt; 0 : "No checkpoint targets.";
    TransferFsImage.getFileClient(fsName, fileid, srcNames);
    LOG.info("Downloaded file " + srcNames[0].getName() + " size " +
        srcNames[0].length() + " bytes.");

    // 标示checkpoint所需文件已经准备完成
    checkpointImage.checkpointUploadDone();
  }
</code></pre>

<h4>4. doMerge()</h4>

<p>doMerge主要完成editLog与fsimage的合并，实际调用的checkpointImage.doMerge(sig);</p>

<pre><code>private void doMerge(CheckpointSignature sig) throws IOException {
  getEditLog().open();
  StorageDirectory sdName = null;
  StorageDirectory sdEdits = null;
  Iterator&lt;StorageDirectory&gt; it = null;
  it = dirIterator(NameNodeDirType.IMAGE);
  if (it.hasNext())
    sdName = it.next();
  it = dirIterator(NameNodeDirType.EDITS);
  if (it.hasNext())
    sdEdits = it.next();
  if ((sdName == null) || (sdEdits == null))
    throw new IOException("Could not locate checkpoint directories");
  // 加载fsimage
  loadFSImage(FSImage.getImageFile(sdName, NameNodeFile.IMAGE));
  // 加载editlog，并合并到fsimage

  loadFSEdits(sdEdits);
  // 校验新fsimage的一致性，主要包括版本，更新时间，namespaceId
  sig.validateStorageInfo(this);
  // 将fsImage存储到本地，并创建新edits
  saveFSImage();
}
</code></pre>

<p>该方法与NameNode启动时类似:</p>

<ol>
<li>加载fsiamge</li>
<li>加载editslog，并作用到fsimage中</li>
<li>校验合并后的fsimage</li>
<li>将新fsimage存储到本地，并新建空的edit是目录</li>
</ol>


<h4>5. putFSImage(sig);</h4>

<p>该方法比较简单，主要通过TransferFsImage工具类将合并后的fsimage上传到NameNode</p>

<pre><code>  private void putFSImage(CheckpointSignature sig) throws IOException {
    String fileid = "putimage=1&amp;port=" + infoPort +
      "&amp;machine=" +
      InetAddress.getLocalHost().getHostAddress() +
      "&amp;token=" + sig.toString();
    LOG.info("Posted URL " + fsName + fileid);
    TransferFsImage.getFileClient(fsName, fileid, (File[])null);
  }
</code></pre>

<h4>6. namenode.rollFsImage();</h4>

<p>上传完新fsiamge之后，SecondaryNameNode通过namenode.rollFsImage()通知NameNode使用新的fsimage.ckpt作为最新镜像，并清空editslog。该请求最终由NameNode上的FsImage.rollFsImage()处理，代码如下：</p>

<pre><code> void rollFSImage() throws IOException {
    if (ckptState != CheckpointStates.UPLOAD_DONE) {
      throw new IOException("Cannot roll fsImage before rolling edits log.");
    }
    //
    // 校验fsimage.ckpt和edits.new是否存在于所有目录
    if (!editLog.existsNew()) {
      throw new IOException("New Edits file does not exist");
    }
    for (Iterator&lt;StorageDirectory&gt; it = 
                       dirIterator(NameNodeDirType.IMAGE); it.hasNext();) {
      StorageDirectory sd = it.next();
      File ckpt = getImageFile(sd, NameNodeFile.IMAGE_NEW);
      if (!ckpt.exists()) {
        throw new IOException("Checkpoint file " + ckpt +
                              " does not exist");
      }
    }
    //删除旧的edits，并将edits.new重命名为edits
    editLog.purgeEditLog(); // renamed edits.new to edits

    //
    // 重命名fsimage
    for (Iterator&lt;StorageDirectory&gt; it = 
                       dirIterator(NameNodeDirType.IMAGE); it.hasNext();) {
      StorageDirectory sd = it.next();
      File ckpt = getImageFile(sd, NameNodeFile.IMAGE_NEW);
      File curFile = getImageFile(sd, NameNodeFile.IMAGE);
      // renameTo fails on Windows if the destination file 
      // already exists.
      if (!ckpt.renameTo(curFile)) {
        curFile.delete();
        if (!ckpt.renameTo(curFile)) {
          // Close edit stream, if this directory is also used for edits
          if (sd.getStorageDirType().isOfType(NameNodeDirType.EDITS))
            editLog.processIOError(sd);
        // add storage to the removed list
          removedStorageDirs.add(sd);
          it.remove();
        }
      }
    }

    //
    // 更新所有目录的fstime
    //
    this.layoutVersion = FSConstants.LAYOUT_VERSION;
    this.checkpointTime = FSNamesystem.now();
    for (Iterator&lt;StorageDirectory&gt; it = 
                           dirIterator(); it.hasNext();) {
      StorageDirectory sd = it.next();
      // delete old edits if sd is the image only the directory
      if (!sd.getStorageDirType().isOfType(NameNodeDirType.EDITS)) {
        File editsFile = getImageFile(sd, NameNodeFile.EDITS);
        editsFile.delete();
      }
      // delete old fsimage if sd is the edits only the directory
      if (!sd.getStorageDirType().isOfType(NameNodeDirType.IMAGE)) {
        File imageFile = getImageFile(sd, NameNodeFile.IMAGE);
        imageFile.delete();
      }
      try {
        sd.write();
      } catch (IOException e) {
        LOG.error("Cannot write file " + sd.getRoot(), e);
        // Close edit stream, if this directory is also used for edits
        if (sd.getStorageDirType().isOfType(NameNodeDirType.EDITS))
          editLog.processIOError(sd);
      //add storage to the removed list
        removedStorageDirs.add(sd);
        it.remove();
      }
    }
    ckptState = FSImage.CheckpointStates.START;
  }
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS源码学习（10）——NameNode与DataNode间的通信]]></title>
    <link href="http://jiangbo.me/blog/2012/10/18/hdfs-namenode-and-datanode-communication/"/>
    <updated>2012-10-18T21:54:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/18/hdfs-namenode-and-datanode-communication</id>
    <content type="html"><![CDATA[<p>NameNode和DataNode间的通信分为四种场景：</p>

<ol>
<li>初始时DataNode注册：</li>
<li>周期性心跳检测：</li>
<li>周期性blockreport：</li>
<li>完成一个副本的写入：</li>
</ol>


<h2>一、初始时DataNode注册</h2>

<p>DataNode在启动时会向NameNode注册，注册时需要提交的信息有DatanodeRegistration表示。结构如下：</p>

<p><img src="http://jiangbo.me/images/hdfs/DatanodeRegistration.png" alt="DatanodeRegistration" /></p>

<p>主要包括：</p>

<ol>
<li>name：机器名（主机名+服务端口号）</li>
<li>infoPort: 状态信息服务端口好</li>
<li>ipcPort： 提供ipc服务的端口号</li>
</ol>


<p>此外，该类中的storageID是该datanode在集群中的唯一id，在注册时有NameNode分配</p>

<p>注册的主要流程如下：
<img src="http://jiangbo.me/images/hdfs/register.png" alt="DataNodeRegister" /></p>

<h2>二、心跳检测（heartbeat）</h2>

<p>DataNode通过周期性调用namenode.sendHeartbeat()来完成心跳检测.主要流程如下：</p>

<p><img src="http://jiangbo.me/images/hdfs/sendHeartbeat.png" alt="sendHeartbeat" /></p>

<h2>三、blockReport</h2>

<p>DataNode周期性向NameNode发送blockReport，告知自己最新的block信息：
<img src="http://jiangbo.me/images/hdfs/blockReport.png" alt="blockReport" /></p>

<h2>四、完成副本写入</h2>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS源码学习（9）——安全模式（SafeMode）]]></title>
    <link href="http://jiangbo.me/blog/2012/10/18/hdfs-namenode-safe-mode/"/>
    <updated>2012-10-18T21:52:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/18/hdfs-namenode-safe-mode</id>
    <content type="html"><![CDATA[<h2>一、SafeModeInfo</h2>

<p>SafeModeInfo维护了系统安全模式下的状态信息，每当系统进入安全模式时都会创建一个SafeModeInfo实例维护状态信息，离开时会销毁这个实例。
该类结构如下</p>

<p><img src="http://jiangbo.me/images/hdfs/SafeModeInfo.png" alt="SafeModeInfo" /></p>

<p>其中threshold和extension为可配置项：</p>

<ol>
<li>threshold表示离开安全模式时打到最低备份数的block的比例</li>
<li>extension表示进入安全模式的最低时长</li>
</ol>


<h2>二、SafeModeMonitor</h2>

<p>FSNameSystem中SafeModeMonitor代码结构如下：</p>

<pre><code>  class SafeModeMonitor implements Runnable {
    /** interval in msec for checking safe mode: {@value} */
    private static final long recheckInterval = 1000;

    /**
     */
    public void run() {
      while (fsRunning &amp;&amp; (safeMode != null &amp;&amp; !safeMode.canLeave())) {
        try {
          Thread.sleep(recheckInterval);
        } catch (InterruptedException ie) {
        }
      }
      // leave safe mode and stop the monitor
      try {
        leaveSafeMode(true);
      } catch(SafeModeException es) { // should never happen
        String msg = "SafeModeMonitor may not run during distributed upgrade.";
        assert false : msg;
        throw new RuntimeException(msg, es);
      }
      smmthread = null;
    }
  }
</code></pre>

<p>其核心就是每个1秒检测一次是否能够离开模式（safeMode.canLeave()），如果可以，则尝试离开并停止SafeModeMonitor线程（leaveSafeMode(true)）</p>

<h3>1.1. 是否能离开 —— safeMode.canLeave()</h3>

<p>能够离开安全模式的标准是：
1. 已进入安全模式的时长大于等于 extension
2. 安全的block数比例打到门槛值</p>

<pre><code>synchronized boolean canLeave() {
  if (reached == 0)
    return false;
  if (now() - reached &lt; extension) {
    reportStatus("STATE* Safe mode ON.", false);
    return false;
  }
  return !needEnter();
}

/** 
 * There is no need to enter safe mode 
 * if DFS is empty or {@link #threshold} == 0
 */
boolean needEnter() {
  return getSafeBlockRatio() &lt; threshold;
}
</code></pre>

<h3>1.2. 离开安全模式 —— leaveSafeMode(true);</h3>

<pre><code>  public void leaveSafeMode(boolean checkForUpgrades) throws SafeModeException {
    writeLock();
    try {
    if (!isInSafeMode()) {
      NameNode.stateChangeLog.info("STATE* Safe mode is already OFF."); 
      return;
    }
    //获取升级状态，如在升级中，不能离开安全模式
    if(getDistributedUpgradeState())
      throw new SafeModeException("Distributed upgrade is in progress",
                                  safeMode);
    //调用SafeModeInfo.leave()离开安全模式
    safeMode.leave(checkForUpgrades);
    } finally {
      writeUnlock();
    }
  }
</code></pre>

<h3>1.2.1 SafeModeInfo.leave()</h3>

<pre><code>    synchronized void leave(boolean checkForUpgrades) {
      if(checkForUpgrades) {
        // 验证是否需要升级
        boolean needUpgrade = false;
        try {
          needUpgrade = startDistributedUpgradeIfNeeded();
        } catch(IOException e) {
          FSNamesystem.LOG.error(StringUtils.stringifyException(e));
        }
        if(needUpgrade) {
          //如果需要升级，进入手动安全模式
          safeMode = new SafeModeInfo();
          return;
        }
      }
      // 如果备份队列未初始化完，继续初始化该队列
      if (!isPopulatingReplQueues()) {
        initializeReplQueues();
      }
      long timeInSafemode = now() - systemStart;
      NameNode.stateChangeLog.info("STATE* Leaving safe mode after " 
                                    + timeInSafemode/1000 + " secs.");
      NameNode.getNameNodeMetrics().safeModeTime.set((int) timeInSafemode);

      if (reached &gt;= 0) {
        NameNode.stateChangeLog.info("STATE* Safe mode is OFF."); 
      }
      reached = -1;
      safeMode = null;
      NameNode.stateChangeLog.info("STATE* Network topology has "
                                   +clusterMap.getNumOfRacks()+" racks and "
                                   +clusterMap.getNumOfLeaves()+ " datanodes");
      NameNode.stateChangeLog.info("STATE* UnderReplicatedBlocks has "
                                   +neededReplications.size()+" blocks");
    }
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS源码学习（8）——Backup Mode]]></title>
    <link href="http://jiangbo.me/blog/2012/10/18/hdfs-namenode-backup-mode/"/>
    <updated>2012-10-18T21:51:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/18/hdfs-namenode-backup-mode</id>
    <content type="html"><![CDATA[<h2>元数据的持久化</h2>

<p>HDFS通过主要通过FSImage和FSEditLog来完成文件元数据的持久化。对文件系统的任何修改，NameNode都会通过Editlog记录下来，持久化到本地。同时整个系统的命名空间，所有的文件元信息均保存在FSImage中，包括block->File的映射，文件的属性等等。NameNode启动时会从本地磁盘加载FSImage和FSEditLog，并将EditLog中的日志信息合并到FSImage中进行之持久化（该合并过程称为一个检查点：checkpoint），并构建文件系统的元信息。</p>

<p>但是持久化的数据中不包括block<->datanode的映射信息，该信息由每个datanode向NameNode发起blockReport()请求时报告其所拥有的block信息。</p>

<h3>Editlog记录修改日志</h3>

<p>EditLog的持久化文件是一个二进制文件，大体结构如下：</p>

<p><img src="http://jiangbo.me/images/hdfs/EditLogFile.png" alt="EditLogFile" /></p>

<p>EditLog文件开始是一个日志版本号，0.19版本的hdfs中该version为-18。
随后是每一条操作的事务日志，每条日志的起始为一个操作类型位，随后是该操作的详细信息，不同的操作类型所带的详细信息也不同。加载EditLog是根据layoutVersion和edit_op位采取不同的方式解析后面的详细信息。
EditLog能够记录如下17中操作：</p>

<p><img src="http://jiangbo.me/images/hdfs/EditLogOp.png" alt="EditLogOp" /></p>

<p>EditLog为每种操作都提供了相应的log方法，当系统中发生文件修改时，会调用相应的log方法记录日志</p>

<h2>元数据加载与恢复</h2>

<p>NameNode启动过程中最终会通过FSImage.loadFSImage()来从fsimage目录中加载最新的fsimage镜像和editslog，并合并构建命名空间。
代码结构如下：</p>

<pre><code>  boolean loadFSImage() throws IOException {
    // 根据checkpointtime查找最新的fsimage
    long latestNameCheckpointTime = Long.MIN_VALUE;
    long latestEditsCheckpointTime = Long.MIN_VALUE;
    StorageDirectory latestNameSD = null;
    StorageDirectory latestEditsSD = null;
    boolean needToSave = false;
    isUpgradeFinalized = true;
    Collection&lt;String&gt; imageDirs = new ArrayList&lt;String&gt;();
    Collection&lt;String&gt; editsDirs = new ArrayList&lt;String&gt;();
    for (Iterator&lt;StorageDirectory&gt; it = dirIterator(); it.hasNext();) {
      StorageDirectory sd = it.next();
      if (!sd.getVersionFile().exists()) {
        needToSave |= true;
        continue; // some of them might have just been formatted
      }
      boolean imageExists = false, editsExists = false;
      if (sd.getStorageDirType().isOfType(NameNodeDirType.IMAGE)) {
        imageExists = getImageFile(sd, NameNodeFile.IMAGE).exists();
        imageDirs.add(sd.getRoot().getCanonicalPath());
      }
      if (sd.getStorageDirType().isOfType(NameNodeDirType.EDITS)) {
        editsExists = getImageFile(sd, NameNodeFile.EDITS).exists();
        editsDirs.add(sd.getRoot().getCanonicalPath());
      }

      checkpointTime = readCheckpointTime(sd);
      if ((checkpointTime != Long.MIN_VALUE) &amp;&amp; 
          ((checkpointTime != latestNameCheckpointTime) || 
           (checkpointTime != latestEditsCheckpointTime))) {
        // Force saving of new image if checkpoint time
        // is not same in all of the storage directories.
        needToSave |= true;
      }
      if (sd.getStorageDirType().isOfType(NameNodeDirType.IMAGE) &amp;&amp; 
         (latestNameCheckpointTime &lt; checkpointTime) &amp;&amp; imageExists) {
        latestNameCheckpointTime = checkpointTime;
        latestNameSD = sd;
      }
      if (sd.getStorageDirType().isOfType(NameNodeDirType.EDITS) &amp;&amp; 
           (latestEditsCheckpointTime &lt; checkpointTime) &amp;&amp; editsExists) {
        latestEditsCheckpointTime = checkpointTime;
        latestEditsSD = sd;
      }
      if (checkpointTime &lt;= 0L)
        needToSave |= true;
      // set finalized flag
      isUpgradeFinalized = isUpgradeFinalized &amp;&amp; !sd.getPreviousDir().exists();
    }

    // 确保至少有一个fsimage和一个edits目录
    if (latestNameSD == null)
      throw new IOException("Image file is not found in " + imageDirs);
    if (latestEditsSD == null)
      throw new IOException("Edits file is not found in " + editsDirs);

    // 确保获得的fsimage和edits是同一个检查点
    if (latestNameCheckpointTime &gt; latestEditsCheckpointTime
        &amp;&amp; latestNameSD != latestEditsSD
        &amp;&amp; latestNameSD.getStorageDirType() == NameNodeDirType.IMAGE
        &amp;&amp; latestEditsSD.getStorageDirType() == NameNodeDirType.EDITS) {
      // This is a rare failure when NN has image-only and edits-only
      // storage directories, and fails right after saving images,
      // in some of the storage directories, but before purging edits.
      // See -NOTE- in saveNamespace().
      LOG.error("This is a rare failure scenario!!!");
      LOG.error("Image checkpoint time " + latestNameCheckpointTime +
          " &gt; edits checkpoint time " + latestEditsCheckpointTime);
      LOG.error("Name-node will treat the image as the latest state of " +
          "the namespace. Old edits will be discarded.");
    } else if (latestNameCheckpointTime != latestEditsCheckpointTime)
      throw new IOException("Inconsitent storage detected, " +
          "image and edits checkpoint times do not match. " +
          "image checkpoint time = " + latestNameCheckpointTime +
          "edits checkpoint time = " + latestEditsCheckpointTime);

    // 如果上次检查点中断了，则恢复该检查点
    needToSave |= recoverInterruptedCheckpoint(latestNameSD, latestEditsSD);

    long startTime = FSNamesystem.now();
    long imageSize = getImageFile(latestNameSD, NameNodeFile.IMAGE).length();

    //
    // 加载fsimage文件
    //
    latestNameSD.read();
    needToSave |= loadFSImage(getImageFile(latestNameSD, NameNodeFile.IMAGE));
    LOG.info("Image file of size " + imageSize + " loaded in " 
        + (FSNamesystem.now() - startTime)/1000 + " seconds.");

    // 加载最新的edits并作用于fsimage上
    if (latestNameCheckpointTime &gt; latestEditsCheckpointTime)
      // the image is already current, discard edits
      needToSave |= true;
    else // latestNameCheckpointTime == latestEditsCheckpointTime
      needToSave |= (loadFSEdits(latestEditsSD) &gt; 0);

    return needToSave;
  }
</code></pre>

<p>其中FSImage.loadFSImage(File curFile) 代码如下：</p>

<pre><code>  boolean loadFSImage(File curFile) throws IOException {
    assert this.getLayoutVersion() &lt; 0 : "Negative layout version is expected.";
    assert curFile != null : "curFile is null";

    FSNamesystem fsNamesys = FSNamesystem.getFSNamesystem();
    FSDirectory fsDir = fsNamesys.dir;

    //
    // Load in bits
    //
    boolean needToSave = true;
    DataInputStream in = new DataInputStream(new BufferedInputStream(
                              new FileInputStream(curFile)));
    try {
      /*
       * Note: Remove any checks for version earlier than 
       * Storage.LAST_UPGRADABLE_LAYOUT_VERSION since we should never get 
       * to here with older images.
       */

      /*
       * TODO we need to change format of the image file
       * it should not contain version and namespace fields
       */
      // 读取imageversion
      int imgVersion = in.readInt();
      // 读取namespaceid
      this.namespaceID = in.readInt();

      // 读取镜像中的文件数
      long numFiles;
      if (imgVersion &lt;= -16) {
        numFiles = in.readLong();
      } else {
        numFiles = in.readInt();
      }

      this.layoutVersion = imgVersion;
      // 读取镜像时间戳
      if (imgVersion &lt;= -12) {
        long genstamp = in.readLong();
        fsNamesys.setGenerationStamp(genstamp); 
      }

      needToSave = (imgVersion != FSConstants.LAYOUT_VERSION);

      // 读取每个文件的信息
      short replication = FSNamesystem.getFSNamesystem().getDefaultReplication();

      LOG.info("Number of files = " + numFiles);

      byte[][] pathComponents;
      byte[][] parentPath = ;
      INodeDirectory parentINode = fsDir.rootDir;
      for (long i = 0; i &lt; numFiles; i++) {
        long modificationTime = 0;
        long atime = 0;
        long blockSize = 0;
        //读取文件名(path)
        pathComponents = readPathComponents(in);
        //读取副本数
        replication = in.readShort();
        //调整副本数，使其不超过系统的最大和最小副本数限制
        replication = FSEditLog.adjustReplication(replication);
        //读取文件修改时间
        modificationTime = in.readLong();
        if (imgVersion &lt;= -17) {
        //读取最近访问时间
          atime = in.readLong();
        }
        if (imgVersion &lt;= -8) {
        //读取block块大小
          blockSize = in.readLong();
        }
        //读取block数
        int numBlocks = in.readInt();
        //构建blocks
        Block blocks[] = null;

        // 老版本hdfs中，numBlocks=0表示目录，新版本中numBlocks=-1表示目录
        if ((-9 &lt;= imgVersion &amp;&amp; numBlocks &gt; 0) ||
            (imgVersion &lt; -9 &amp;&amp; numBlocks &gt;= 0)) {
           //构建文件block信息
          blocks = new Block[numBlocks];
          for (int j = 0; j &lt; numBlocks; j++) {
            blocks[j] = new Block();
            if (-14 &lt; imgVersion) {
              blocks[j].set(in.readLong(), in.readLong(), 
                            Block.GRANDFATHER_GENERATION_STAMP);
            } else {
              // 读取block信息
              blocks[j].readFields(in);
            }
          }
        }
        // 老版本inode中不维护blocksize，如果存在多个block，blocksize选取第一个block的大小，如果只有一个block，则选该block大小和默认大小中较大的，如果没有block，则选用默认大小
        if (-8 &lt;= imgVersion &amp;&amp; blockSize == 0) {
          if (numBlocks &gt; 1) {
            blockSize = blocks[0].getNumBytes();
          } else {
            long first = ((numBlocks == 1) ? blocks[0].getNumBytes(): 0);
            blockSize = Math.max(fsNamesys.getDefaultBlockSize(), first);
          }
        }

        // 如果是目录（blocks=null），读取该目录的配额
        long nsQuota = -1L;
        if (imgVersion &lt;= -16 &amp;&amp; blocks == null) {
          nsQuota = in.readLong();
        }
        long dsQuota = -1L;
        if (imgVersion &lt;= -18 &amp;&amp; blocks == null) {
          dsQuota = in.readLong();
        }

        //获取权限信息
        PermissionStatus permissions = fsNamesys.getUpgradePermission();
        if (imgVersion &lt;= -11) {
          permissions = PermissionStatus.read(in);
        }

        //如果该path为root，且设置了配额信息，则更新根目录的配额
        if (isRoot(pathComponents)) { // it is the root
          // update the root's attributes
          if (nsQuota != -1 || dsQuota != -1) {
            fsDir.rootDir.setQuota(nsQuota, dsQuota);
          }
          fsDir.rootDir.setModificationTime(modificationTime);
          fsDir.rootDir.setPermissionStatus(permissions);
          continue;
        }
        //如果该inode的parent与当前路径不一至，获取新的parentPaht
        if(!isParent(pathComponents, parentPath)) {
          parentINode = null;
          parentPath = getParent(pathComponents);
        }
        // 将该inode添加到inode树中
        parentINode = fsDir.addToParent(pathComponents, parentINode, permissions,
                                        blocks, replication, modificationTime, 
                                        atime, nsQuota, dsQuota, blockSize);
      }

      // 加载datanode信息，imgVersion&lt;-12的版本事实上啥都不做，datanode信息已经不保存在fsimage中
      this.loadDatanodes(imgVersion, in);

      // 加载正在构建中的文件
      this.loadFilesUnderConstruction(imgVersion, in, fsNamesys);

    } finally {
      in.close();
    }

    return needToSave;
  }
</code></pre>

<p>其核心就是加载并解析fsimage文件，构建命名空间，fsimage文件的结构参见《NameNode中主要数据结构》</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS源码学习（7）——Block管理]]></title>
    <link href="http://jiangbo.me/blog/2012/10/18/hdfs-namenode-block-management/"/>
    <updated>2012-10-18T21:46:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/18/hdfs-namenode-block-management</id>
    <content type="html"><![CDATA[<p>HDFS通过一个BlockManager管理集群中所有的block信息</p>

<h2>主要数据结构</h2>

<h3>Block</h3>

<p>Block是HDFS读写的基本单元，集群中每个block通过一个long id来唯一标示。</p>

<h3>BlockInfo</h3>

<p>维护一个block的元信息，主要通过</p>

<h3>BlockMap</h3>

<p>通过一个GSet&lt;Block, BlockInfo>维护一个block与其元数据信息的映射关系，元信息包括其所属的BlockCollection和存储该block的datanode节点，每个BlockMap有个初始容量capacity</p>

<h3>BlockCollection</h3>

<h2>Block和副本管理</h2>

<h3>Block和副本状态</h3>

<p>Block有如下状态：</p>

<ol>
<li>committed：所有的副本已经被创建且更新至最新</li>
<li>Under construction: 需要创建一个或多个副本</li>
<li>To be deleted: 所有副本需要被删除。发生在文件被删除或者block被重写</li>
<li>Over-replicated: 过多的副本存在。此时副本中的一个需要设置为无效并删除。</li>
</ol>


<p>副本有如下状态：</p>

<ol>
<li>Current: 正常状态，该副本正确反应block内容</li>
<li>Conrrupt: 某个副本损坏。副本损坏是由client报告给namenode的。client通过checksum检查副本是否损坏，如果损坏了，通过BlockManager.invalidateBlock()处理</li>
<li>On a faild DataNode: DataNode Heartbeat发现有DataNode失效时，即将在改datanode上创建的副本将被删除</li>
<li>Out of Date: 当Datanode失效，且副本所属的block发生更新后，Datanode恢复正常。过期的block将通过blockreport报告给namenode，并将其删除</li>
<li>Under construction: 副本尚未被写入并在Datanode上被验证。在NameNode看来，只有当收到blockReport并且报告中timestamp正确时，猜人物副本写入正常。</li>
</ol>


<h2>Block分配</h2>

<h2>Block查询</h2>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS源码学习（6）——租约管理（lease management)]]></title>
    <link href="http://jiangbo.me/blog/2012/10/18/hdfs-namenode-lease-management/"/>
    <updated>2012-10-18T21:44:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/18/hdfs-namenode-lease-management</id>
    <content type="html"><![CDATA[<p>LeaseManagement是HDFS中的一个同步机制，用于保证同一时刻只有一个client对一个文件进行写或创建操作。如当新建一个文件f时，client向NameNode发起一个create请求，那么leaseManager会想该client分配一个f文件的lease。client凭借该lease完成文件的创建操作。此时其他client无法获得f的当client长时间（默认为超过1min）不进行操作时，发放的lease将被收回。</p>

<p>LeaseManager主要完成两部分工作：</p>

<ol>
<li>文件create，write，complete操作时，创建lease、更新时间戳、回收lease</li>
<li>一个后台线程定期检查是否有过期的lease</li>
</ol>


<p>LeaseManager的代码结构如下</p>

<p><img src="http://jiangbo.me/images/hdfs/LeaseManager.png" alt="LeaseManager" /></p>

<p>其中Lease表示一个租约，包括一个client(holder)所拥有的所有文件锁(paths)。</p>

<p>Monitor是检查是否有过期租约的线程。</p>

<p>LeaseManager中有几个主要数据结构：</p>

<ol>
<li>leases（TreeMap&lt;String, Lease>）：维护holder -> leased的映射集合</li>
<li>sortedLeases (TreeSet<Lease>): lease集合</li>
<li>sortedLeaseByPath(TreeMap&lt;String, Lease>): 维护paths->lease的映射集合</li>
</ol>


<h2>一、创建lease</h2>

<p>当client向NameNode发起create操作时，NameNode.create()调用FSNameSystem.startFile()->FSNameSystem.startFileInternal()，该方法最终会调用leaseManager.addLease(cons.clientName, src)来创建lease。</p>

<p>LeaseManager.addLease()方法如下：</p>

<pre><code>  synchronized Lease addLease(String holder, String src
      ) throws IOException {
    Lease lease = getLease(holder);
    if (lease == null) {
      lease = new Lease(holder);
      leases.put(holder, lease);
      sortedLeases.add(lease);
    } else {
      renewLease(lease);
    }
    sortedLeasesByPath.put(src, lease);
    lease.paths.add(src);
    return lease;
  }
</code></pre>

<p>代码结构简单：判断该client是否有lease，没有则新建一个lease，并将起加到leases集合中。否则更新lease。更新sortedLeasesByPath，将filepath加入到该lease的paths集合中</p>

<h2>二、更新时间戳</h2>

<p>针对已经存在的lease，通过LeasemManager.renewLease()来更新该lease的时间戳。代码如下：</p>

<pre><code>  synchronized void renewLease(Lease lease) {
    if (lease != null) {
      sortedLeases.remove(lease);
      lease.renew();
      sortedLeases.add(lease);
    }
  }
</code></pre>

<p>lease.renew()代码如下：</p>

<pre><code>/** Only LeaseManager object can renew a lease */
private void renew() {
  this.lastUpdate = FSNamesystem.now();
}
</code></pre>

<h2>三、compelete时回收lease</h2>

<p>当client调用NameNode.complete()方法时，最终会调用FSNameSystem.completeFileInternal()方法。其中执行finalizeINodeFileUnderConstruction()是调用leaseManager.removeLease()释放lease。</p>

<p>代码结构如下：</p>

<pre><code>  synchronized void removeLease(String holder, String src) {
    Lease lease = getLease(holder);
    if (lease != null) {
      removeLease(lease, src);
    }
  }
</code></pre>

<p> removeLease(lease, src);代码如下：</p>

<pre><code>  /**
   * Remove the specified lease and src.
   */
  synchronized void removeLease(Lease lease, String src) {
    sortedLeasesByPath.remove(src);
    if (!lease.removePath(src)) {
      LOG.error(src + " not found in lease.paths (=" + lease.paths + ")");
    }

    if (!lease.hasPath()) {
      leases.remove(lease.holder);
      if (!sortedLeases.remove(lease)) {
        LOG.error(lease + " not found in sortedLeases");
      }
    }
  }
</code></pre>

<h2>四、后台线程回收过期lease</h2>

<p>Monitor回收lease线程代码结构如下：</p>

<pre><code> class Monitor implements Runnable {
    final String name = getClass().getSimpleName();

    /** Check leases periodically. */
    public void run() {
      for(; fsnamesystem.isRunning(); ) {
        fsnamesystem.writeLock();
        try {
          checkLeases();
        } finally {
          fsnamesystem.writeUnlock();
        }

        try {
          Thread.sleep(2000);
        } catch(InterruptedException ie) {
          if (LOG.isDebugEnabled()) {
            LOG.debug(name + " is interrupted", ie);
          }
        }
      }
    }
  }
</code></pre>

<p>代码结构简单，每个2s周期性执行checkLeases()。</p>

<h3>4.1 checkLeases()</h3>

<pre><code>  /** Check the leases beginning from the oldest. */
  synchronized void checkLeases() {
    for(; sortedLeases.size() &gt; 0; ) {
      final Lease oldest = sortedLeases.first();
      if (!oldest.expiredHardLimit()) {
        return;
      }

      LOG.info("Lease " + oldest + " has expired hard limit");

      final List&lt;String&gt; removing = new ArrayList&lt;String&gt;();
      // need to create a copy of the oldest lease paths, becuase 
      // internalReleaseLease() removes paths corresponding to empty files,
      // i.e. it needs to modify the collection being iterated over
      // causing ConcurrentModificationException
      String[] leasePaths = new String[oldest.getPaths().size()];
      oldest.getPaths().toArray(leasePaths);
      for(String p : leasePaths) {
        try {
          fsnamesystem.internalReleaseLeaseOne(oldest, p);
        } catch (IOException e) {
          LOG.error("Cannot release the path "+p+" in the lease "+oldest, e);
          removing.add(p);
        }
      }

      for(String p : removing) {
        removeLease(oldest, p);
      }
    }
  }
</code></pre>

<h2>Lease Recovery ——租约回收</h2>

<h3>lease recovery时机</h3>

<p>lease发放之后，在不用时会被回收，回收的产经除上述Monitor线程检测lease过期是回收外，还有：</p>

<ol>
<li>NameNode收到DataNode的Sync block command时</li>
<li>DFSClient主动关闭一个流时</li>
<li>创建文件时，如果该DFSClient的lease超过soft limit时</li>
</ol>


<h3>lease recovery 算法</h3>

<p>1) NameNode查找lease信息</p>

<p>2) 对于lease中的每个文件f，令b为f的最后一个block，作如下操作：</p>

<p>2.1) 获取b所在的datanode列表</p>

<p>2.2) 令其中一个datanode作为primary datanode p</p>

<p>2.3) p 从NameNode获取最新的时间戳</p>

<p>2.4) p 从每个DataNode获取block信息</p>

<p>2.5) p 计算最小的block长度</p>

<p>2.6) p 用最小的block长度和最新的时间戳来更新具有有效时间戳的datanode</p>

<p>2.7) p 通知NameNode更新结果</p>

<p>2.8) NameNode更新BlockInfo</p>

<p>2.9) NameNode从lease中删除f，如果此时该lease中所有文件都已被删除，将删除该lease</p>

<p>2.10) Name提交修改的EditLog</p>

<h2>Client续约 —— DFSClient.LeaseChecker</h2>

<p>在NameNode上的LeaseManager.Monitor线程负责检查过期的lease，那么client为了防止尚在使用的lease过期，需要定期想NameNode发起续约请求。该任务有DFSClient中的LeaseChecker完成。</p>

<p>LeaseChecker结构如下：</p>

<p><img src="http://jiangbo.me/images/hdfs/LeaseChecker.png" alt="LeaseChecker" /></p>

<p>其中pendingCreates是一个TreeMap&lt;String, OutputStream>用来维护src->当前正在写入的文件的DFSOutputStream的映射。</p>

<p>其核心是周期性（每个1s）调用run()方法来对租约过半的lease进行续约</p>

<pre><code>public void run() {
  long lastRenewed = 0;
  while (clientRunning &amp;&amp; !Thread.interrupted()) {
    //当租约周期过半时需要进行续约
    if (System.currentTimeMillis() - lastRenewed &gt; (LEASE_SOFTLIMIT_PERIOD / 2)) {
      try {
        renew();
        lastRenewed = System.currentTimeMillis();
      } catch (IOException ie) {
        LOG.warn("Problem renewing lease for " + clientName, ie);
      }
    }

    try {
      Thread.sleep(1000);
    } catch (InterruptedException ie) {
      if (LOG.isDebugEnabled()) {
        LOG.debug(this + " is interrupted.", ie);
      }
      return;
    }
  }
}
</code></pre>

<p>其中renew()方法如下：</p>

<pre><code>    private void renew() throws IOException {
      synchronized(this) {
        //如果当前创建中的文件列表为空，则不需要续约
        if (pendingCreates.isEmpty()) {
          return;
        }
      }
      //向NameNode发起续约请求
      namenode.renewLease(clientName);
    }
</code></pre>

<p>NameNode接收到renewLease请求后，调用FSNameSystem.renewLease()并最终调用LeaseManager.renewLease()完成续约。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS源码学习（5）——副本管理（Replica Management)]]></title>
    <link href="http://jiangbo.me/blog/2012/10/18/hdfs-namenode-replica-management/"/>
    <updated>2012-10-18T21:43:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/18/hdfs-namenode-replica-management</id>
    <content type="html"><![CDATA[<p>HDFS中的副本管理通过FSNameSystem.java中的ReplicationMonitor线程来完成。该线程代码结构较为简单。</p>

<pre><code>  class ReplicationMonitor implements Runnable {
    static final int INVALIDATE_WORK_PCT_PER_ITERATION = 32;
    static final float REPLICATION_WORK_MULTIPLIER_PER_ITERATION = 2;
    public void run() {
      while (fsRunning) {
        try {
          computeDatanodeWork();
          processPendingReplications();
          Thread.sleep(replicationRecheckInterval);
        } catch (InterruptedException ie) {
          LOG.warn("ReplicationMonitor thread received InterruptedException.", ie);
          break;
        } catch (IOException ie) {
          LOG.warn("ReplicationMonitor thread received exception. " + ie +  " " +
              StringUtils.stringifyException(ie));
        } catch (Throwable t) {
          LOG.fatal("ReplicationMonitor thread received Runtime exception. " + t + " " +
              StringUtils.stringifyException(t));
          Runtime.getRuntime().exit(-1);
        }
      }
    }
  }
</code></pre>

<p>该线程只是周期性调用computeDatanodeWork()和processPendingReplications()。</p>

<h2>一、computeDatanodeWork()</h2>

<pre><code>  public int computeDatanodeWork() throws IOException {
    int workFound = 0;
    int blocksToProcess = 0;
    int nodesToProcess = 0;
    // blocks should not be replicated or removed if safe mode is on
    if (isInSafeMode())
      return workFound;
    //计算需要备份的block数和节点数
    synchronized(heartbeats) {
      blocksToProcess = (int)(heartbeats.size() 
          * ReplicationMonitor.REPLICATION_WORK_MULTIPLIER_PER_ITERATION);
      nodesToProcess = (int)Math.ceil((double)heartbeats.size() 
          * ReplicationMonitor.INVALIDATE_WORK_PCT_PER_ITERATION / 100);
    }
    //执行备份
    workFound = computeReplicationWork(blocksToProcess); 

    // Update FSNamesystemMetrics counters
    pendingReplicationBlocksCount = pendingReplications.size();
    underReplicatedBlocksCount = neededReplications.size();
    scheduledReplicationBlocksCount = workFound;
    corruptReplicaBlocksCount = corruptReplicas.size();

    //HADOOP-5549 : Fix bug of schedule both replication and deletion work in one iteration
    workFound += computeInvalidateWork(nodesToProcess);
    return workFound;
  }
</code></pre>

<h3>1.1. computeReplicationWork()</h3>

<pre><code>  private int computeReplicationWork(
                                  int blocksToProcess) throws IOException {
    // stall only useful for unit tests (see TestFileAppend4.java)
    if (stallReplicationWork)  {
      return 0;
    }

    // 选取需要备份的block
    List&lt;List&lt;Block&gt;&gt; blocksToReplicate =
      chooseUnderReplicatedBlocks(blocksToProcess);

    // 执行备份
    return computeReplicationWorkForBlocks(blocksToReplicate);
  }
</code></pre>

<h3>1.1.1 选取需要备份的block —— chooseUnderReplicatedBlocks()</h3>

<pre><code>  List&lt;List&lt;Block&gt;&gt; chooseUnderReplicatedBlocks(int blocksToProcess) {
    // 初始化返回值数据结构，返回值是一个二维优先级列表
    List&lt;List&lt;Block&gt;&gt; blocksToReplicate =
      new ArrayList&lt;List&lt;Block&gt;&gt;(UnderReplicatedBlocks.LEVEL);
    for (int i = 0; i &lt; UnderReplicatedBlocks.LEVEL; i++) {
      blocksToReplicate.add(new ArrayList&lt;Block&gt;());
    }

    writeLock();
    try {
      synchronized (neededReplications) {
        if (neededReplications.size() == 0) {
          return blocksToReplicate;
        }

        for (int priority = 0; priority&lt;UnderReplicatedBlocks.LEVEL; priority++) {
        //遍历所有需要备份的block列表（UnderReplicatedBlocks结构）
        BlockIterator neededReplicationsIterator = neededReplications.iterator(priority);
        int numBlocks = neededReplications.size(priority);
        //检查该优先级列表中是否已经开始备份（relIndex数组中保存的是当前每个优先级列表中已备份的block索引）
        if (replIndex[priority] &gt; numBlocks) {
          replIndex[priority] = 0;
        }
        // skip to the first unprocessed block, which is at replIndex
        for (int i = 0; i &lt; replIndex[priority] &amp;&amp; neededReplicationsIterator.hasNext(); i++) {
          neededReplicationsIterator.next();
        }
        // 计算该优先级下需要备份的block数，低优先级的block备份数不超过总配额的20%
        int blocksToProcessIter = getQuotaForThisPriority(blocksToProcess,
            numBlocks, neededReplications.getSize(priority+1));
        blocksToProcess -= blocksToProcessIter;

        //便利改优先级列表将该优先级下的block添加到返回值中
        for (int blkCnt = 0; blkCnt &lt; blocksToProcessIter; blkCnt++, replIndex[priority]++) {
          if (!neededReplicationsIterator.hasNext()) {
            // start from the beginning
            replIndex[priority] = 0;
            neededReplicationsIterator = neededReplications.iterator(priority);
            assert neededReplicationsIterator.hasNext() :
              "neededReplications should not be empty.";
          }

          Block block = neededReplicationsIterator.next();
          blocksToReplicate.get(priority).add(block);
        } // end for
        }
      } // end try
      return blocksToReplicate;
    } finally {
      writeUnlock();
    }
  }
</code></pre>

<h3>1.1.2. 备份blocks —— computeReplicationWorkForBlocks（）</h3>

<pre><code>  int computeReplicationWorkForBlocks(List&lt;List&lt;Block&gt;&gt; blocksToReplicate) {
    int requiredReplication, numEffectiveReplicas, priority;
    List&lt;DatanodeDescriptor&gt; containingNodes;
    DatanodeDescriptor srcNode;
    INodeFile fileINode = null;

    int scheduledWork = 0;
    List&lt;ReplicationWork&gt; work = new LinkedList&lt;ReplicationWork&gt;();

    writeLock();
    try {
      synchronized (neededReplications) {
        for (priority = 0; priority &lt; blocksToReplicate.size(); priority++) {
          for (Block block : blocksToReplicate.get(priority)) {
            // block should belong to a file
            //获取该block所属的INode
            fileINode = blocksMap.getINode(block);
            // abandoned block not belong to a file
            if (fileINode == null ) {
              neededReplications.remove(block, priority); // remove from neededReplications
              replIndex[priority]--;
              continue;
            }
            //获取该文件需要的副本数
            requiredReplication = fileINode.getReplication();

            // 获取一个源datanode节点
            containingNodes = new ArrayList&lt;DatanodeDescriptor&gt;();
            NumberReplicas numReplicas = new NumberReplicas();
            srcNode = chooseSourceDatanode(block, containingNodes, numReplicas);
            if (srcNode == null) // block can not be replicated from any node
            {
              continue;
            }

          // 检查正在备份中的副本数是否满足备份需要，满足则不需要再备份
            numEffectiveReplicas = numReplicas.liveReplicas() +
              pendingReplications.getNumReplicas(block);
            if (numEffectiveReplicas &gt;= requiredReplication) {
              neededReplications.remove(block, priority); // remove from neededReplications
              replIndex[priority]--;
              continue;
            }
            //添加到待备份列表中
            work.add(new ReplicationWork(block, fileINode, requiredReplication
                - numEffectiveReplicas, srcNode, containingNodes, priority));
          }
        }
      }
    } finally {
      writeUnlock();
    }

    // 选取一个备份目标datanode
    for(ReplicationWork rw : work){
      DatanodeDescriptor targets[] = chooseTarget(rw);
      rw.targets = targets;
    }

    writeLock();
    try {
      for(ReplicationWork rw : work){
        DatanodeDescriptor[] targets = rw.targets;
        if(targets == null || targets.length == 0){
          rw.targets = null;
          continue;
        }
        synchronized (neededReplications) {
          Block block = rw.block;
          priority = rw.priority;
          // 重新检查INode和备份数，因为全局锁已经释放
          // block should belong to a file
          fileINode = blocksMap.getINode(block);
          // abandoned block not belong to a file
          if (fileINode == null ) {
            neededReplications.remove(block, priority); // remove from neededReplications
            rw.targets = null;
            replIndex[priority]--;
            continue;
          }
          requiredReplication = fileINode.getReplication();


          NumberReplicas numReplicas = countNodes(block);
          numEffectiveReplicas = numReplicas.liveReplicas() +
            pendingReplications.getNumReplicas(block);
          if (numEffectiveReplicas &gt;= requiredReplication) {
            neededReplications.remove(block, priority); // remove from neededReplications
            replIndex[priority]--;
            rw.targets = null;
            continue;
          }

          // 将block添加到datanode的需要备份的block列表中
          rw.srcNode.addBlockToBeReplicated(block, targets);

          scheduledWork++;

          //设置namenode的block调度计数器
          for (DatanodeDescriptor dn : targets) {
            dn.incBlocksScheduled();
          }

          // Move the block-replication into a "pending" state.
          // The reason we use 'pending' is so we can retry
          // replications that fail after an appropriate amount of time.
          //将该block移至pendingReplications（PendingReplicationBlocks）中，表示该block的状态为'pending'(正在备份中)。'pending'表示如果失败了还可以重试
          pendingReplications.add(block, targets.length);
          NameNode.stateChangeLog.debug(
            "BLOCK* block " + block
              + " is moved from neededReplications to pendingReplications");

          // remove from neededReplications
          //从 neededReplication列表中移除该block
          if (numEffectiveReplicas + targets.length &gt;= requiredReplication) {
            neededReplications.remove(block, priority); // remove from neededReplications
            replIndex[priority]--;
          }
        }
      }
    } finally {
      writeUnlock();
    }

    // 更新 metrics
    updateReplicationMetrics(work);

    // 打印debug信息
    if(NameNode.stateChangeLog.isInfoEnabled()){
      // log which blocks have been scheduled for replication
      for(ReplicationWork rw : work){
        // report scheduled blocks
        DatanodeDescriptor[] targets = rw.targets;
        if (targets != null &amp;&amp; targets.length != 0) {
          StringBuffer targetList = new StringBuffer("datanode(s)");
          for (int k = 0; k &lt; targets.length; k++) {
            targetList.append(' ');
            targetList.append(targets[k].getName());
          }
          NameNode.stateChangeLog.info(
            "BLOCK* ask "
              + rw.srcNode.getName() + " to replicate "
              + rw.block + " to " + targetList);
        }
      }
    }

    // 记录一次备份操作
    if (NameNode.stateChangeLog.isDebugEnabled()) {
      NameNode.stateChangeLog.debug("BLOCK* neededReplications = "
          + neededReplications.size() + " pendingReplications = "
          + pendingReplications.size());
    }
    return scheduledWork;
  }
</code></pre>

<h4>1.1.2.1. 选取源datanode —— chooseSourceDatanode（）</h4>

<p>获取一个源datanode节点</p>

<pre><code>  private DatanodeDescriptor chooseSourceDatanode(
                                    Block block,
                                    List&lt;DatanodeDescriptor&gt; containingNodes,
                                    NumberReplicas numReplicas) {
    containingNodes.clear();
    DatanodeDescriptor srcNode = null;
    int live = 0;
    int decommissioned = 0;
    int corrupt = 0;
    int excess = 0;
    Iterator&lt;DatanodeDescriptor&gt; it = blocksMap.nodeIterator(block);
    Collection&lt;DatanodeDescriptor&gt; nodesCorrupt = corruptReplicas.getNodes(block);
    while(it.hasNext()) {
      DatanodeDescriptor node = it.next();
      Collection&lt;Block&gt; excessBlocks = 
        excessReplicateMap.get(node.getStorageID());
      if ((nodesCorrupt != null) &amp;&amp; (nodesCorrupt.contains(node)))
        corrupt++;
      else if (node.isDecommissionInProgress() || node.isDecommissioned())
        decommissioned++;
      else if (excessBlocks != null &amp;&amp; excessBlocks.contains(block)) {
        excess++;
      } else {
        live++;
      }
      containingNodes.add(node);
      // Check if this replica is corrupt
      // If so, do not select the node as src node
      if ((nodesCorrupt != null) &amp;&amp; nodesCorrupt.contains(node))
        continue;
      if(node.getNumberOfBlocksToBeReplicated() &gt;= maxReplicationStreams)
        continue; // already reached replication limit
      // the block must not be scheduled for removal on srcNode
      if(excessBlocks != null &amp;&amp; excessBlocks.contains(block))
        continue;
      // never use already decommissioned nodes
      if(node.isDecommissioned())
        continue;
      // we prefer nodes that are in DECOMMISSION_INPROGRESS state
      if(node.isDecommissionInProgress() || srcNode == null) {
        srcNode = node;
        continue;
      }
      if(srcNode.isDecommissionInProgress())
        continue;
      // switch to a different node randomly
      // this to prevent from deterministically selecting the same node even
      // if the node failed to replicate the block on previous iterations
      if(r.nextBoolean())
        srcNode = node;
    }
    if(numReplicas != null)
      numReplicas.initialize(live, decommissioned, corrupt, excess);
    return srcNode;
  }
</code></pre>

<h4>1.1.2.2. 选取目标datanode —— chooseTarget(rw);</h4>

<pre><code>  private DatanodeDescriptor[] chooseTarget(ReplicationWork work) {
    if (!neededReplications.contains(work.block)) {
      return null;
    }
    if (work.blockSize == BlockCommand.NO_ACK) {
      LOG.warn("Block " + work.block.getBlockId() + 
          " of the file " + work.fileINode.getFullPathName() + 
          " is invalidated and cannot be replicated.");
      return null;
    }
    if (work.blockSize == DFSUtil.DELETED) {
      LOG.warn("Block " + work.block.getBlockId() + 
          " of the file " + work.fileINode.getFullPathName() + 
          " is a deleted block and cannot be replicated.");
      return null;
    }
    //实际调用replicator(BlockPlacementPolicy)的chooseTarget方法选取target
    return replicator.chooseTarget(work.fileINode,
        work.numOfReplicas, work.srcNode,
        work.containingNodes, null, work.blockSize);
  }
</code></pre>

<h2>副本存放策略</h2>

<p><img src="img/BlockAllocation.png" alt="BlockAllocation" /></p>

<p>如图所示，HDFS默认的副本存放策略为：</p>

<ol>
<li>第一个副本存放在当前datanode的本地</li>
<li>第二个副本存放在与第一个副本所在datanode不在同一机架上的一个datanode上</li>
<li>第三个副本存放在与第二个副本同一机架但不同datanode上</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS源码学习(4)——DataNode心跳检测（HeartBeat）]]></title>
    <link href="http://jiangbo.me/blog/2012/10/18/hdfs-heartbeat/"/>
    <updated>2012-10-18T21:38:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/18/hdfs-heartbeat</id>
    <content type="html"><![CDATA[<p>HDFS中DataNode的心跳检测通过FSNameSystem中的HeartbeatMonitor完成。代码结构如下：</p>

<pre><code>  class HeartbeatMonitor implements Runnable {
    /**
     */
    public void run() {
      while (fsRunning) {
        try {
          heartbeatCheck();
        } catch (Exception e) {
          FSNamesystem.LOG.error(StringUtils.stringifyException(e));
        }
        try {
          Thread.sleep(heartbeatRecheckInterval);
        } catch (InterruptedException ie) {
        }
      }
    }
  }
</code></pre>

<p>代码很简单，心跳检测线程周期性调用heartbeatCheck()。</p>

<h2>一、心跳检查——heartbeatCheck()</h2>

<p>该方法主要用于检测是否有过期的心跳检测，如有，检测其上的block是否已经进行过重新备份。该线程每次只处理一个datanode。</p>

<pre><code>  void heartbeatCheck() {
    if (isInSafeMode()) {
      // 安全模式下不做心跳检测
      return;
    }
    boolean allAlive = false;
    while (!allAlive) {
      boolean foundDead = false;
      DatanodeID nodeID = null;

      // 获取第一个dead datanode
      synchronized(heartbeats) {
        for (Iterator&lt;DatanodeDescriptor&gt; it = heartbeats.iterator();
             it.hasNext();) {
          DatanodeDescriptor nodeInfo = it.next();
          if (isDatanodeDead(nodeInfo)) {
            foundDead = true;
            nodeID = nodeInfo;
            break;
          }
        }
      }

      // 申请fsnamesystem锁，删除dead datanode
      if (foundDead) {
        writeLock();
        try {
          synchronized(heartbeats) {
            synchronized (datanodeMap) {
              DatanodeDescriptor nodeInfo = null;
              try {
                nodeInfo = getDatanode(nodeID);
              } catch (IOException e) {
                nodeInfo = null;
              }
              if (nodeInfo != null &amp;&amp; isDatanodeDead(nodeInfo)) {
                NameNode.stateChangeLog.info("BLOCK* NameSystem.heartbeatCheck: "
                                             + "lost heartbeat from " + nodeInfo.getName());
                removeDatanode(nodeInfo);
              }
            }
          }
        } finally {
          writeUnlock();
        }
      }
      allAlive = !foundDead;
    }
  }
</code></pre>

<h3>1.1 判断是否已死 —— isDatanodeDead（）</h3>

<p>判断一个datanode是否已经dead的标准很简单，当前距该节点最后的更新时间差是否已经超过心跳检测的过期时间限制</p>

<pre><code>private boolean isDatanodeDead(DatanodeDescriptor node) {
    return (node.getLastUpdate() &lt;
            (now() - heartbeatExpireInterval));
  }
</code></pre>

<h3>1.2 删除datanode —— removeDatanode（）</h3>

<pre><code>  private void removeDatanode(DatanodeDescriptor nodeInfo) {
    synchronized (heartbeats) {
      if (nodeInfo.isAlive) {
        updateStats(nodeInfo, false);
        //从heartbeats中移除
        heartbeats.remove(nodeInfo);
        //更新datanode状态
        nodeInfo.isAlive = false;
      }
    }

    nodeInfo.hasInitialBlockReport = false;
    for (Iterator&lt;Block&gt; it = nodeInfo.getBlockIterator(); it.hasNext();) {
      //移除该节点上的block
      removeStoredBlock(it.next(), nodeInfo);
    }
    unprotectedRemoveDatanode(nodeInfo);
    clusterMap.remove(nodeInfo);
  }
</code></pre>

<h4>1.2.1 removeStoredBlock（）</h4>

<p>该方法更新block->datanode的映射(blocksMap)，如果block还有效，有可能导致block备份发生</p>

<pre><code>  void removeStoredBlock(Block block, DatanodeDescriptor node) {
    if (NameNode.stateChangeLog.isDebugEnabled()) {
      NameNode.stateChangeLog.debug("BLOCK* NameSystem.removeStoredBlock: "
                                    +block + " from "+node.getName());
    }
    assert (hasWriteLock());
    if (!blocksMap.removeNode(block, node)) {
      if (NameNode.stateChangeLog.isDebugEnabled()) {
        NameNode.stateChangeLog.debug("BLOCK* NameSystem.removeStoredBlock: "
                                      +block+" has already been removed from node "+node);
      }
      return;
    }

    //
    //检查是否需要备份删除的block
    INode fileINode = blocksMap.getINode(block);
    if (fileINode != null) {
      //减小当前系统中安全block（备份数满足最小值的block）数量
      decrementSafeBlockCount(block);
      //更新需要备份的block数量
      updateNeededReplications(block, -1, 0);
    }

    //
    // 从excessblocks中删除改block，并从excessReplicateMap删除改datanode
    Collection&lt;Block&gt; excessBlocks = excessReplicateMap.get(node.getStorageID());
    if (excessBlocks != null) {
      if (excessBlocks.remove(block)) {
        excessBlocksCount--;
        if (NameNode.stateChangeLog.isDebugEnabled()) {
          NameNode.stateChangeLog.debug("BLOCK* NameSystem.removeStoredBlock: "
              +block+" is removed from excessBlocks");
        }
        if (excessBlocks.size() == 0) {
          excessReplicateMap.remove(node.getStorageID());
        }
      }
    }

    // 从corruptReplicas中移除该block
    corruptReplicas.removeFromCorruptReplicasMap(block, node);
  }
</code></pre>

<h5>1.2.1.1  BlocksMap.removeNode（）</h5>

<p>其中BlocksMap.removeNode()方法如下：</p>

<pre><code>  boolean removeNode(Block b, DatanodeDescriptor node) {
    BlockInfo info = blocks.get(b);
    if (info == null)
      return false;

    // 从datanode 的blocklist中移除block，并从block的datalist中移除datanode
    boolean removed = node.removeBlock(info);

    if (info.getDatanode(0) == null     // no datanodes left
              &amp;&amp; info.inode == null) {  // does not belong to a file
      //从blocksmap中移除该block
      blocks.remove(b);  // remove block from the map
    }
    return removed;
  }
</code></pre>

<h5>1.2.1.2 减小当前安全的block数 —— decrementSafeBlockCount()</h5>

<p>减小当前副本数安全的的block数，此举有可能触发系统进入安全模式（safemode）</p>

<pre><code>  void decrementSafeBlockCount(Block b) {
    if (safeMode == null) // mostly true
      return;

    safeMode.decrementSafeBlockCount((short)countNodes(b).liveReplicas());
  }
</code></pre>

<p>其中safeMode.decrementSafeBlockCount()代码如下：</p>

<pre><code>synchronized void decrementSafeBlockCount(short replication) {

  if (replication == safeReplication-1) {
    //安全的block数减一
    this.blockSafe--;
    //检查是否需要进入到safemode
    checkMode();
  }
}
</code></pre>

<p>SafeModeInfo.checkMode()代码如下：</p>

<pre><code>    private void checkMode() {
      //当安全的block数比例降至安全值以下，进入安全模式
      if (needEnter()) {
        enter();
        // check if we are ready to initialize replication queues
        if (canInitializeReplQueues() &amp;&amp; !isPopulatingReplQueues()) {
          //初始化副本队列
          initializeReplQueues();
        }
        reportStatus("STATE* Safe mode ON.", false);
        return;
      }
      // 如果安全模式已经关闭或者门槛小于0，则跳出安全模式
      if (!isOn() ||                           // safe mode is off
          extension &lt;= 0 || threshold &lt;= 0) {  // don't need to wait
        this.leave(true); // leave safe mode
        return;
      }

      //之前已经进入安全模式，直接返回
      if (reached &gt; 0) {  // threshold has already been reached before
        reportStatus("STATE* Safe mode ON.", false);
        return;
      }
      // 启动SafeModeMonitor线程
      reached = now();
      smmthread = new Daemon(new SafeModeMonitor());
      smmthread.start();
      reportStatus("STATE* Safe mode extension entered.", true);
    }
</code></pre>

<h5>1.2.1.3 更新需要备份的列表 —— updateNeededReplications（）</h5>

<pre><code>  /* updates a block in under replication queue */
  void updateNeededReplications(Block block,
                        int curReplicasDelta, int expectedReplicasDelta) {
    writeLock();
    try {
    //计算当前副本数
    NumberReplicas repl = countNodes(block);
    //期望的副本数
    int curExpectedReplicas = getReplication(block);
    //将该block更新到需要备份的列表中（neededReplications）
    neededReplications.update(block, 
                              repl.liveReplicas(), 
                              repl.decommissionedReplicas(),
                              curExpectedReplicas,
                              curReplicasDelta, expectedReplicasDelta);
    } finally {
      writeUnlock();
    }
  }
</code></pre>

<h4>1.2.2 移除datanode —— unprotectedRemoveDatanode</h4>

<pre><code>  void unprotectedRemoveDatanode(DatanodeDescriptor nodeDescr) {
    //重置清空datanode中block信息
    nodeDescr.resetBlocks();
    //从invlidateSet中移除datanode
    removeFromInvalidates(nodeDescr.getStorageID());
    if (NameNode.stateChangeLog.isDebugEnabled()) {
      NameNode.stateChangeLog.debug(
                                    "BLOCK* NameSystem.unprotectedRemoveDatanode: "
                                    + nodeDescr.getName() + " is out of service now.");
    }
  }
</code></pre>

<h5>1.2.2.1 removeFromInvalidates()</h5>

<pre><code>  private void removeFromInvalidates(String storageID) {
    //从recentInvalidateSet中移除该datanode
    Collection&lt;Block&gt; blocks = recentInvalidateSets.remove(storageID);
    if (blocks != null) {
      //从正在删除的block总数中减去当前节点上的block总数
      pendingDeletionBlocksCount -= blocks.size();
    }
  }
</code></pre>

<h2>二、 处理心跳检测请求 —— handleHeartbeat()</h2>

<p>NameNode只负责创建一个HeartbeatMonitor来通过每个datanode的最新更新时间周期性检查是否有过期的datanode，而每个datanode是否的最新更新时间是由datanode主动向namenode报告的，namenode通过handleHeartbeat()处理心跳请求。</p>

<pre><code>  DatanodeCommand[] handleHeartbeat(DatanodeRegistration nodeReg,
      long capacity, long dfsUsed, long remaining,
      int xceiverCount, int xmitsInProgress) throws IOException {
    DatanodeCommand cmd = null;
    synchronized (heartbeats) {
      synchronized (datanodeMap) {
        DatanodeDescriptor nodeinfo = null;
        try {
          nodeinfo = getDatanode(nodeReg);
        } catch(UnregisteredDatanodeException e) {
          return new DatanodeCommand[]{DatanodeCommand.REGISTER};
        }

        // 检查该datanode是否需要被关闭，可以通过设置datanode的adminState为DECOMMISSIONED来关闭一个datanode
        if (nodeinfo != null &amp;&amp; shouldNodeShutdown(nodeinfo)) {
          setDatanodeDead(nodeinfo);
          throw new DisallowedDatanodeException(nodeinfo);
        }

        if (nodeinfo == null || !nodeinfo.isAlive) {
          return new DatanodeCommand[]{DatanodeCommand.REGISTER};
        }

        updateStats(nodeinfo, false);
        nodeinfo.updateHeartbeat(capacity, dfsUsed, remaining, xceiverCount);
        updateStats(nodeinfo, true);

        //检查租约恢复状态
        cmd = nodeinfo.getLeaseRecoveryCommand(Integer.MAX_VALUE);
        if (cmd != null) {
          return new DatanodeCommand[] {cmd};
        }

        ArrayList&lt;DatanodeCommand&gt; cmds = new ArrayList&lt;DatanodeCommand&gt;(2);
        //检查正在备份中的副本
        cmd = nodeinfo.getReplicationCommand(
              maxReplicationStreams - xmitsInProgress);
        if (cmd != null) {
          cmds.add(cmd);
        }
        //检查无效的block
        cmd = nodeinfo.getInvalidateBlocks(blockInvalidateLimit);
        if (cmd != null) {
          cmds.add(cmd);
        }
        if (!cmds.isEmpty()) {
          return cmds.toArray(new DatanodeCommand[cmds.size()]);
        }
      }
    }

    //检查是否需要升级系统
    cmd = getDistributedUpgradeCommand();
    if (cmd != null) {
      return new DatanodeCommand[] {cmd};
    }
    return null;
  }
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS源码学习（3）——NameNode中的线程]]></title>
    <link href="http://jiangbo.me/blog/2012/10/18/hdfs-namenode-thread/"/>
    <updated>2012-10-18T21:37:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/18/hdfs-namenode-thread</id>
    <content type="html"><![CDATA[<p>NameNode存在三种运行模式：</p>

<ol>
<li>Normal： NameNode正常服务的状态</li>
<li>Safe mode：NameNode重启时进入Safe mode，该模式下整个系统是只读的，以便于NameNode手机DataNode信息</li>
<li>Backup mode：备份NameNode处于Backup mode，被动的接收主NameNode的检查点信息</li>
</ol>


<p>在NameNode中存在如下几种线程：</p>

<ol>
<li>DataNode 健康检查管理线程</li>
<li>副本管理线程</li>
<li>租约管理（lease Management）</li>
<li>IPC Handler 线程</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS源码学习(2)——NameNode初始化]]></title>
    <link href="http://jiangbo.me/blog/2012/10/18/hdfs-namenode-startup/"/>
    <updated>2012-10-18T21:35:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/18/hdfs-namenode-startup</id>
    <content type="html"><![CDATA[<h2>main()</h2>

<pre><code>  public static void main(String argv[]) throws Exception {
    try {
      StringUtils.startupShutdownMessage(NameNode.class, argv, LOG);
      //创建nameNode
      NameNode namenode = createNameNode(argv, null);
      if (namenode != null)
        namenode.join();
    } catch (Throwable e) {
      LOG.error(StringUtils.stringifyException(e));
      System.exit(-1);
    }
  }
</code></pre>

<h3>createNameNode()</h3>

<pre><code>  public static NameNode createNameNode(String argv[], 
                                 Configuration conf) throws IOException {
    if (conf == null)
      conf = new Configuration();
      //从命令行参数中提取启动配置项数据
    StartupOption startOpt = parseArguments(argv);
    if (startOpt == null) {
      printUsage();
      return null;
    }
    //设置启动参数
    setStartupOption(conf, startOpt);

    switch (startOpt) {
      case FORMAT:
        boolean aborted = format(conf, true);
        System.exit(aborted ? 1 : 0);
      case FINALIZE:
        aborted = finalize(conf, true);
        System.exit(aborted ? 1 : 0);
      default:
    }

    //新建NameNode
    NameNode namenode = new NameNode(conf);
    return namenode;
  }
</code></pre>

<h3>NameNode()</h3>

<pre><code>  public NameNode(Configuration conf) throws IOException {
    super(conf);
    try {
    //初始化
      initialize(getConf());
    } catch (IOException e) {
      this.stop();
      throw e;
    }
  }
</code></pre>

<h3>initialize()</h3>

<pre><code>  private void initialize(Configuration conf) throws IOException {
    InetSocketAddress socAddr = NameNode.getAddress(conf);
    int handlerCount = conf.getInt("dfs.namenode.handler.count", 10);
    // 关键-&gt;创建一个RPC Server
    this.server = RPC.getServer(this, socAddr.getHostName(), socAddr.getPort(),
                                handlerCount, false, conf);

    // The rpc-server port can be ephemeral... ensure we have the correct info
    this.serverAddress = this.server.getListenerAddress(); 
    FileSystem.setDefaultUri(conf, getUri(serverAddress));
    LOG.info("Namenode up at: " + this.serverAddress);

    myMetrics = new NameNodeMetrics(conf, this);

    //关键-&gt;创建一个FSNameSystem
    this.namesystem = new FSNamesystem(this, conf);
    //启动HTTP Server
    startHttpServer(conf);
    //启动RPC Server
    this.server.start();  //start RPC server   

    startTrashEmptier(conf);
  }
</code></pre>

<h2>FSNameSystem()</h2>

<pre><code>  FSNamesystem(NameNode nn, Configuration conf) throws IOException {
    try {
      //初始化FSNameSystem
      initialize(nn, conf);
      userPasswordInformation = new UserPasswordInformation(conf);
      extendAccessControlList = new ExtendAccessControlList(conf);
    } catch(IOException e) {
      LOG.error(getClass().getSimpleName() + " initialization failed.", e);
      close();
      throw e;
    }
  }
</code></pre>

<h3>FSNameSystem.initialize()</h3>

<pre><code>  private void initialize(NameNode nn, Configuration conf) throws IOException {
    this.systemStart = now();
    this.fsLock = new ReentrantReadWriteLock(); // non-fair locking
    setConfigurationParameters(conf);

    this.nameNodeAddress = nn.getNameNodeAddress();
    this.registerMBean(conf); // register the MBean for the FSNamesystemStutus

    //创建FSDirectory
    this.dir = new FSDirectory(this, conf);
    StartupOption startOpt = NameNode.getStartupOption(conf);

    //加载FSImage
    this.dir.loadFSImage(getNamespaceDirs(conf),
                         getNamespaceEditsDirs(conf), startOpt);
    long timeTakenToLoadFSImage = now() - systemStart;
    LOG.info("Finished loading FSImage in " + timeTakenToLoadFSImage + " msecs");
    NameNode.getNameNodeMetrics().fsImageLoadTime.set(
                              (int) timeTakenToLoadFSImage);
    this.safeMode = new SafeModeInfo(conf);
    setBlockTotal();
    //创建PendingReplicationBlocks
    pendingReplications = new PendingReplicationBlocks(
                            conf.getInt("dfs.replication.pending.timeout.sec", 
                                        -1) * 1000L);
    //创建心跳检查线程                                          
    this.hbthread = new Daemon(new HeartbeatMonitor());
    //创建租约管理线程
    this.lmthread = new Daemon(leaseManager.new Monitor());
    //创建副本管理线程
    this.replthread = new Daemon(new ReplicationMonitor());
    hbthread.start();
    lmthread.start();
    replthread.start();

    // 副本超额block管理线程
    this.overreplthread = new Daemon(new OverReplicationMonitor());
    overreplthread.start();

    this.hostsReader = new HostsFileReader(conf.get("dfs.hosts",""),
                                           conf.get("dfs.hosts.exclude",""));
    //创建退役节点管理线程
    this.dnthread = new Daemon(new DecommissionManager(this).new Monitor(
        conf.getInt("dfs.namenode.decommission.interval", 30),
        conf.getInt("dfs.namenode.decommission.nodes.per.interval", 5)));
    dnthread.start();

    this.dnsToSwitchMapping = ReflectionUtils.newInstance(
        conf.getClass("topology.node.switch.mapping.impl", ScriptBasedMapping.class,
            DNSToSwitchMapping.class), conf);

    /* If the dns to swith mapping supports cache, resolve network 
     * locations of those hosts in the include list, 
     * and store the mapping in the cache; so future calls to resolve
     * will be fast.
     */
    if (dnsToSwitchMapping instanceof CachedDNSToSwitchMapping) {
      dnsToSwitchMapping.resolve(new ArrayList&lt;String&gt;(hostsReader.getHosts()));
    }
    //创建副本定位器用于定位副本存放位置
    this.replicator = BlockPlacementPolicy.getInstance(
        conf,
        this,
        this.clusterMap,
        this.hostsReader,
        this.dnsToSwitchMapping,
        this);
  }
</code></pre>

<h2>FSDirectory(this, conf)</h2>

<p>新建FSDirecotry</p>

<pre><code> FSDirectory(FSNamesystem ns, Configuration conf) {
    //创建一个FSImage，并实例化构建FSDirectory
    this(new FSImage(), ns, conf);
    fsImage.setCheckpointDirectories(FSImage.getCheckpointDirs(conf, null),
                                FSImage.getCheckpointEditsDirs(conf, null));
  }
</code></pre>

<h3>this(new FSImage(), ns, conf);</h3>

<pre><code>FSDirectory(FSImage fsImage, FSNamesystem ns, Configuration conf) {
    this.bLock = new ReentrantReadWriteLock(); // non-fair
    this.cond = bLock.writeLock().newCondition();
    //创建根目录
    rootDir = new INodeDirectoryWithQuota(INodeDirectory.ROOT_NAME,
        ns.createFsOwnerPermissions(new FsPermission((short)0755)),
        Integer.MAX_VALUE, -1);
    this.fsImage = fsImage;
    namesystem = ns;
    initialize(conf);
  }
</code></pre>

<h3>FSDirectory.initialize()</h3>

<pre><code>  private void initialize(Configuration conf) {
    MetricsContext metricsContext = MetricsUtil.getContext("dfs");
    directoryMetrics = MetricsUtil.createRecord(metricsContext, "FSDirectory");
    directoryMetrics.setTag("sessionId", conf.get("session.id"));
  }
</code></pre>

<h3>FSDirectory.loadFSImage()</h3>

<pre><code>  void loadFSImage(Collection&lt;File&gt; dataDirs,
                   Collection&lt;File&gt; editsDirs,
                   StartupOption startOpt) throws IOException {
    // format before starting up if requested
    if (startOpt == StartupOption.FORMAT) {
      fsImage.setStorageDirectories(dataDirs, editsDirs);
      fsImage.format();
      startOpt = StartupOption.REGULAR;
    }
    try {
      //从datadir和editdirs加载FSImage
      if (fsImage.recoverTransitionRead(dataDirs, editsDirs, startOpt)) {
        fsImage.saveNamespace(true);
      }
      //初始化Editlog
      FSEditLog editLog = fsImage.getEditLog();
      assert editLog != null : "editLog must be initialized";
      if (!editLog.isOpen())
        editLog.open();
      fsImage.setCheckpointDirectories(null, null);
    } catch(IOException e) {
      fsImage.close();
      throw e;
    }
    writeLock();
    try {
      this.ready = true;
      cond.signalAll();
    } finally {
      writeUnlock();
    }
  }
</code></pre>

<h3>FSImage.recoverTransitionRead（）</h3>

<pre><code>  boolean recoverTransitionRead(Collection&lt;File&gt; dataDirs,
                             Collection&lt;File&gt; editsDirs,
                                StartupOption startOpt
                                ) throws IOException {
    assert startOpt != StartupOption.FORMAT : 
      "NameNode formatting should be performed before reading the image";

    // none of the data dirs exist
    if (dataDirs.size() == 0 || editsDirs.size() == 0)  
      throw new IOException(
        "All specified directories are not accessible or do not exist.");

    if(startOpt == StartupOption.IMPORT 
        &amp;&amp; (checkpointDirs == null || checkpointDirs.isEmpty()))
      throw new IOException("Cannot import image from a checkpoint. "
                          + "\"fs.checkpoint.dir\" is not set." );

    if(startOpt == StartupOption.IMPORT 
        &amp;&amp; (checkpointEditsDirs == null || checkpointEditsDirs.isEmpty()))
      throw new IOException("Cannot import image from a checkpoint. "
                          + "\"fs.checkpoint.edits.dir\" is not set." );

    setStorageDirectories(dataDirs, editsDirs);
    // 1.检查所有目录的状态和一致性
    Map&lt;StorageDirectory, StorageState&gt; dataDirStates = 
             new HashMap&lt;StorageDirectory, StorageState&gt;();
    boolean isFormatted = false;
    for (Iterator&lt;StorageDirectory&gt; it = 
                      dirIterator(); it.hasNext();) {
      StorageDirectory sd = it.next();
      StorageState curState;
      try {
        curState = sd.analyzeStorage(startOpt);
        // sd is locked but not opened
        switch(curState) {
        case NON_EXISTENT:
          // name-node fails if any of the configured storage dirs are missing
          throw new InconsistentFSStateException(sd.getRoot(),
                                                 "storage directory does not exist or is not accessible.");
        case NOT_FORMATTED:
          break;
        case NORMAL:
          break;
        default:  // recovery is possible
          sd.doRecover(curState);      
        }
        if (curState != StorageState.NOT_FORMATTED 
            &amp;&amp; startOpt != StartupOption.ROLLBACK) {
          sd.read(); // read and verify consistency with other directories
          isFormatted = true;
        }
        if (startOpt == StartupOption.IMPORT &amp;&amp; isFormatted)
          // import of a checkpoint is allowed only into empty image directories
          throw new IOException("Cannot import image from a checkpoint. " 
              + " NameNode already contains an image in " + sd.getRoot());
      } catch (IOException ioe) {
        sd.unlock();
        throw ioe;
      }
      dataDirStates.put(sd,curState);
    }

    if (!isFormatted &amp;&amp; startOpt != StartupOption.ROLLBACK 
                     &amp;&amp; startOpt != StartupOption.IMPORT)
      throw new IOException("NameNode is not formatted.");
    if (layoutVersion &lt; LAST_PRE_UPGRADE_LAYOUT_VERSION) {
      checkVersionUpgradable(layoutVersion);
    }
    if (startOpt != StartupOption.UPGRADE
          &amp;&amp; layoutVersion &lt; LAST_PRE_UPGRADE_LAYOUT_VERSION
          &amp;&amp; layoutVersion != FSConstants.LAYOUT_VERSION)
        throw new IOException(
                          "\nFile system image contains an old layout version " + layoutVersion
                          + ".\nAn upgrade to version " + FSConstants.LAYOUT_VERSION
                          + " is required.\nPlease restart NameNode with -upgrade option.");
    // check whether distributed upgrade is reguired and/or should be continued
    verifyDistributedUpgradeProgress(startOpt);

    // 2. Format unformatted dirs.
    this.checkpointTime = 0L;
    for (Iterator&lt;StorageDirectory&gt; it = 
                     dirIterator(); it.hasNext();) {
      StorageDirectory sd = it.next();
      StorageState curState = dataDirStates.get(sd);
      switch(curState) {
      case NON_EXISTENT:
        assert false : StorageState.NON_EXISTENT + " state cannot be here";
      case NOT_FORMATTED:
        LOG.info("Storage directory " + sd.getRoot() + " is not formatted.");
        LOG.info("Formatting ...");
        sd.clearDirectory(); // create empty currrent dir
        break;
      default:
        break;
      }
    }
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS源码学习（1）——NameNode主要数据结构]]></title>
    <link href="http://jiangbo.me/blog/2012/10/18/hdfs-namenode-datastructure/"/>
    <updated>2012-10-18T11:04:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/10/18/hdfs-namenode-datastructure</id>
    <content type="html"><![CDATA[<h2>FSNameSystem</h2>

<p>FSNameSystem是HDFS文件系统实际执行的核心，提供各种增删改查文件操作接口。其内部维护多个数据结构之间的关系：</p>

<ol>
<li>fsname->block列表的映射</li>
<li>所有有效blocks集合</li>
<li>block与其所属的datanodes之间的映射（该映射是通过block reports动态构建的，维护在namenode的内存中。每个datanode在启动时向namenode报告其自身node上的block）</li>
<li>每个datanode与其上的blocklist的映射</li>
<li>采用心跳检测根据LRU算法更新的机器（datanode）列表</li>
</ol>


<h3>FSDirectory</h3>

<p>FSDirectory用于维护当前系统中的文件树。</p>

<p>其内部主要组成结构包括一个INodeDirectoryWithQuota作为根目录(rootDir)和一个FSImage来持久化文件树的修改操作。</p>

<h4>INode</h4>

<p>HDFS中文件树用类似VFS中INode的方式构建，整个HDFS中文件被表示为INodeFile，目录被表示为INodeDirectory。INodeDiretoryWithQuota是INodeDirectory的扩展类，即带配额的文件目录</p>

<p><img src="http://jiangbo.me/images/hdfs/INode.png" alt="INode" /></p>

<p>INodeFile表示INode书中的一个文件，扩展自INode，除了名字(name)，父节点(parent)等之外，一个主要元素是blocks，一个BlockInfo数组，表示该文件对应的block信息。</p>

<h3>BlocksMap</h3>

<p>BlocksMap用于维护Block -> { INode, datanodes, self ref } 的映射
<img src="http://jiangbo.me/images/hdfs/BlocksMap.png" alt="BlocksMap" />
BlocksMap结构比较简单，实际上就是一个Block到BlockInfo的映射。</p>

<h4>Block</h4>

<p>Block是HDFS中的基本读写单元，主要包括：</p>

<ol>
<li>blockId: 一个long类型的块id</li>
<li>numBytes: 块大小</li>
<li>generationStamp: 块更新的时间戳</li>
</ol>


<h4>BlockInfo</h4>

<p>BlockInfo扩展自Block，除基本信息外还包括一个inode引用，表示该block所属的文件；以及一个神奇的三元组数组Object[] triplets，用来表示保存该block的datanode信息，假设系统中的备份数量为3。那么这个数组结构如下：</p>

<p><img src="http://jiangbo.me/images/hdfs/triplets.png" alt="triplets" /></p>

<ol>
<li>DN1，DN2，DN3分别表示存有改block的三个datanode的引用(DataNodeDescriptor）</li>
<li>DN1-prev-blk表示在DN1上block列表中当前block的前置block引用</li>
<li>DN1-next-blk表示在DN1上block列表中当前block的后置block引用</li>
</ol>


<p>DN2,DN3的prev-blk和next-blk类似。
HDFS采用这种结构存放block->datanode list的信息主要是为了节省内存空间，block->datanodelist之间的映射关系需要占用大量内存，如果同样还要将datanode->blockslist的信息保存在内存中，同样要占用大量内存。采用三元组这种方式能够从其中一个block获得到改block所属的datanode上的所有block列表。</p>

<h4>FSImage</h4>

<p>FSImage用于持久化文件树的变更以及系统启动时加载持久化数据。
HDFS启动时通过FSImage来加载磁盘中原有的文件树，系统Standby之后，通过FSEditlog来保存在文件树上的修改，FSEditLog定期将保存的修改信息刷到FSImage中进行持久化存储。
FSImage中文件元信息的存储结构如下（参见FImage.saveFSImage()方法）</p>

<p><img src="http://jiangbo.me/images/hdfs/FSImage.png" alt="FSImage" /></p>

<h5>FSImage头部信息</h5>

<ol>
<li>layoutVersion(int):image layout版本号，0.19版本的hdfs中为-18</li>
<li>namespaceId(int): 命名空间ID，系统初始化时生成，在一个namenode生命周期内保持不变，datanode想namenode注册是返回改id作为registerId，以后每次datanode与namenode通信时都携带该id，不认识的id的请求将被拒绝。</li>
<li>numberItemOfTree(long): 系统中的文件总数</li>
<li>generationTimeStamp: 生成image的时间戳</li>
</ol>


<h5>INode信息</h5>

<p>FSImage头之后是numberItemOfTree个INode信息，INode信息分为文件(INodeFile)和文件目录(INodeDirectory)两类，两者大体一致，分为INode头，Blocks区（目录没有blocks）和文件权限。</p>

<p><strong>INode头</strong></p>

<ol>
<li>nameLen(short): 文件名长度</li>
<li>filename(String): 文件名</li>
<li>replication(short): 备份数量</li>
<li>modificationTime(long): 最近修改时间</li>
<li>accessTime(long): 最近访问时间</li>
<li>preferedBlockSize(long): 块大小（目录为0）</li>
<li>block num(int): 块数量（目录为-1）</li>
</ol>


<p><strong>Blocks区</strong></p>

<ol>
<li>blockId(long)</li>
<li>numBytes(long,block大小)</li>
<li>generationTimeStamp(long, 更新时间戳）</li>
</ol>


<p><strong>文件权限</strong></p>

<ol>
<li>username(String): 文件用户名</li>
<li>group(String): 所属组</li>
<li>fileperm(short): 文件权限</li>
</ol>


<h5>underconstructionFile区</h5>

<p>layoutverion&lt;-18版本的fsimage还包括正在构建的文件区。与普通Inode信息类似，均有inode头和blocks区以及文件权限，除此之外，underConstructionFile还包括：</p>

<p><strong>client信息</strong></p>

<ol>
<li>clientName：client明</li>
<li>clientMachine： client机器名</li>
</ol>


<p><strong>已分配的datanode信息</strong></p>

<ol>
<li>ipcport： 服务端口</li>
<li>capacity: 容量</li>
<li>dfsuse： 已使用的空间</li>
<li>remaining： 剩余空间</li>
<li>lastupdate： 最新更新时间</li>
<li>xceiverCount</li>
<li>location： datanode位置</li>
<li>hostName：主机名</li>
<li>state： admin管理状态</li>
</ol>


<h2>其他结构</h2>

<h3>CorruptReplicasMap</h3>

<p>CorruptReplicasMap通过一个TreeMap维护corrupt状态block的blocks&#8211;>datanodedescriptor(s)映射。一个block备份在多个datanode中，当其中的一个或多个datanode上的block损坏时，会将该datanode加到treeMap中该block对应的datanodeDescriptor集合中。FSNameSystem通过该Map来维护所有损坏的block与其对应datanode的关系。</p>

<h3>Map&lt;String, LightWeightHashSet<Block>> recentInvalidateSets</h3>

<p>维护最近失效的block集合，map中为storageId->ArrayList<Block>，当某个block的一个datanode上副本失效时会将改block和对应的datanode的storeageId添加到recentInvalidateSet中，当datanode想namenode进行heartbeat时，namenode会检查该datanode中是否有损坏的block，如有，则通知datanode删除改block。</p>

<h3>NavigableMap&lt;String, DatanodeDescriptor>  datanodeMap</h3>

<p>datanodeMap用于维护datanode->block的映射</p>

<h3>ArrayList<DatanodeDescriptor> heartbeats</h3>

<p>维护多有当前活着的节点</p>

<h3>UnderReplicatedBlocks neededReplications</h3>

<p>通过一个优先级队列来维护当前需要备份的block集合，副本数越少的block优先级越高，0为最高级，表示当前只有一个副本。</p>

<h3>PendingReplicationBlocks pendingReplications;</h3>

<p>维护当前正在备份的block集合，并且进行备份请求的时间统计，并通过一个后台线程（PendingReplicationMonitor）来周期性（默认为5分钟）的统计超时的备份请求，当发生超时时，会将这个block重新添加到neededReplications列表中。
<img src="http://jiangbo.me/images/hdfs/PendingReplicationBlocks.png" alt="PendingReplicationBlocks" /></p>

<h3>LightWeightLinkedSet<Block> overReplicatedBlocks</h3>

<p>当前需要检查是否备份过多的block集合</p>

<h3>Map&lt;String, Collection<Block>> excessReplicateMap</h3>

<p>维护系统中datanode与其上的超额备份block的集合，这些超额的备份将被删除。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[本地编译Hadoop小记]]></title>
    <link href="http://jiangbo.me/blog/2012/09/24/compile-hadoop/"/>
    <updated>2012-09-24T15:28:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/09/24/compile-hadoop</id>
    <content type="html"><![CDATA[<h2>Git源码</h2>

<pre><code>git clone git://git.apache.org/hadoop-common.git
</code></pre>

<p>视网速不通，略慢</p>

<h2>编译</h2>

<pre><code>cd hadoop-common
mvn install -DskipTests
</code></pre>

<p>抛异常：</p>

<pre><code>[ERROR] Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.6:run (compile-proto) on project hadoop-common: An Ant BuildException has occured: exec returned: 127 -&gt; [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.6:run (compile-proto) on project hadoop-common: An Ant BuildException has occured: exec returned: 127
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:217)
    at... 
    Caused by: /Users/Shared/Workspace/hadoop/hadoop-common/hadoop-common-project/hadoop-common/target/antrun/build-main.xml:23: exec returned: 127
    at org.apache.tools.ant.taskdefs.ExecTask.runExecute(ExecTask.java:650)
    at org.apache.tools.ant.taskdefs.ExecTask.runExec(ExecTask.java:676)
    at org.apache.tools.ant.taskdefs.ExecTask.execute(ExecTask.java:502)
    at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)
    at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)
    ... 21 more
</code></pre>

<p>原因是缺少protocol buffer， 找不到protoc命令。</p>

<h3>安装protocol buffer</h3>

<pre><code>wget https://protobuf.googlecode.com/files/protobuf-2.4.1.tar.bz2
tar -xvf protobuf-2.4.1.tar.bz2
cd protobuf-2.4.1
./configure &amp;&amp; make
make install
</code></pre>

<h3>导入Eclipse</h3>

<pre><code>mvn eclipse:eclipse -DdownloadSources=true -DdownloadJavadocs=true
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[关于Memcache内存管理模型的理解]]></title>
    <link href="http://jiangbo.me/blog/2012/08/31/something-about-memcache-internal/"/>
    <updated>2012-08-31T07:52:00+08:00</updated>
    <id>http://jiangbo.me/blog/2012/08/31/something-about-memcache-internal</id>
    <content type="html"><![CDATA[<script async class="speakerdeck-embed" data-id="504049be5ec53c000202daa6" data-ratio="1.299492385786802" src="http://jiangbo.me//speakerdeck.com/assets/embed.js"></script>


<h2>说在前面</h2>

<p>本文不包含为什么使用memcache，以及如何使用memcache等基础知识。相关知识请查阅各类手册。
另，为便于理解，最好手头准备一份memcache的源码，本文使用的是目前最新的1.4.4版本源码，可自行到github上clone。</p>

<h2>Item、Chunk、Page、Slab</h2>

<h3>Data Item</h3>

<pre><code>+---------------------------------------+
|  key-value | cas | suffix | item head |  
+---------------------------------------+
</code></pre>

<p>Item指实际存放到memcache中的数据对象结构，除key-value数据外，还包括memcache自身对数据对象的描述信息（Item=key+value+后缀长+32byte结构体）</p>

<h3>Chunk</h3>

<p>Chunk指Memcache用来存放Data Item的最小单元，同一个Slab中的chunk大小是固定的。</p>

<pre><code>+------------------------------+
|   data item    | empty space |
+------------------------------+
</code></pre>

<h2>Page</h2>

<pre><code>+-------------------------------------+
|  chunk1 | chunk2 | chunk3 | chunk4  |
+-------------------------------------+
</code></pre>

<p>每个Slab中按照Page来申请内存，Page的大小默认为1M，可以通过-l参数调整，最小1k，最大128m.</p>

<h3>Slab</h3>

<pre><code>+--------------------------------+
|  Page1 | Page2 | Page3 | Page4 |
+--------------------------------+
</code></pre>

<p>Memcache将分配给它的内存（-m 参数指定，默认64m）按照Chunk大小不同，划分为多个slab。</p>

<p>他们三者的关系如下图所示:</p>

<pre><code>                 Chunk
                   ^                                                         
+------------------|------------------------------------------------------------+
|   Memory         |                                                            | 
|  +---------------|---------------------------------------------------------+  |
|  |      +--------|---------------------+  +------------------------------+ |  |
|  |      |Page1 +-|---+ +-----+ +-----+ |  |Page2 +-----+ +-----+ +-----+ | |  |
|  | Slab |(1M)  | 96B | | 68B | | 72B | |  |(1M)  | 92B | | 76B | | 84B | | |  | 
|  |  1   |      +-----+ +-----+ +-----+ |  |      +-----+ +-----+ +-----+ | |  |
|  |      +------------------------------+  +------------------------------+ |  |
|  +-------------------------------------------------------------------------+  |
|                                                                               |
|  +-------------------------------------------------------------------------+  |
|  |      +------------------------------+  +------------------------------+ |  |
|  |      |Page1 +------+    +------+    |  |Page2 +------+    +-------+   | |  |
|  | Slab | (1M) | 128B |    | 120B |    |  |(1M)  | 128B |    | 97B   |   | |  |
|  |   2  |      +------+    +------+    |  |      +------+    +-------+   | |  |
|  |      +------------------------------+  +------------------------------+ |  |
|  +-------------------------------------------------------------------------+  |
+-------------------------------------------------------------------------------+
</code></pre>

<h2>Slab内存分配</h2>

<h3>slab初始化</h3>

<p>Memcache启动时会进行slab初始化（参见slabs.c中slabs_init()函数），默认最小的chunksize为80（查看源码会发现settings中chunk_size默认为48，但是实际还需要加上一个32bytes的item结构体），可以通过-n参数调整，按照然后按照factor（默认为1.25，可以通过-f参数调整）(<em>关于参数更多的memcache默认参数可以参考memcache.c中settings的设置</em>)比例递增，划分出多个不同chunk大小的slab空间，即slab1的chunk大小=80，slab2的chunk大小为80*1.25=100，slab3的chunk大小为80*1.25*1.25=125，但最大一个一个chunk不会大于一个Page的大小（默认1M）。</p>

<pre><code>一下代码节选自 slabs.c
 95 void slabs_init(const size_t limit, const double factor, const bool prealloc) {
 96     int i = POWER_SMALLEST - 1;
 97     unsigned int size = sizeof(item) + settings.chunk_size;
 98  
 99     mem_limit = limit;
100  
101     if (prealloc) {
102         /* Allocate everything in a big chunk with malloc */
103         mem_base = malloc(mem_limit);
104         if (mem_base != NULL) {
105             mem_current = mem_base;
106             mem_avail = mem_limit;
107         } else {
108             fprintf(stderr, "Warning: Failed to allocate requested memory in"
109                     " one large chunk.\nWill allocate in smaller chunks\n");
110         }
111     }
112  
113     memset(slabclass, 0, sizeof(slabclass));
114  
115     while (++i &lt; POWER_LARGEST &amp;&amp; size &lt;= settings.item_size_max / factor) {
116         /* Make sure items are always n-byte aligned */
117         if (size % CHUNK_ALIGN_BYTES)
118             size += CHUNK_ALIGN_BYTES - (size % CHUNK_ALIGN_BYTES);
119  
120         slabclass[i].size = size;
121         slabclass[i].perslab = settings.item_size_max / slabclass[i].size;
122         size *= factor;
123         if (settings.verbose &gt; 1) {
124             fprintf(stderr, "slab class %3d: chunk size %9u perslab %7u\n",
125                     i, slabclass[i].size, slabclass[i].perslab);
126         }
127     }
128  
129     power_largest = i;
130     slabclass[power_largest].size = settings.item_size_max;
131     slabclass[power_largest].perslab = 1;
132     if (settings.verbose &gt; 1) {
133         fprintf(stderr, "slab class %3d: chunk size %9u perslab %7u\n",
134                 i, slabclass[i].size, slabclass[i].perslab);
135     }
136  
137     /* for the test suite:  faking of how much we've already malloc'd */
138     {
139         char *t_initial_malloc = getenv("T_MEMD_INITIAL_MALLOC");
140         if (t_initial_malloc) {
141             mem_malloced = (size_t)atol(t_initial_malloc);
142         }
143  
144     }
145  
146     if (prealloc) {
147         slabs_preallocate(power_largest);
148     }
149 }                             
</code></pre>

<p>PS：prealloc指的是直接申请一个大的chunk存放所有数据，默认是不采用这种方式的。</p>

<h3>数据存储过程</h3>

<p>一个数据项的大致存储量过程可以理解为（完整代码较长，不在粘贴，具体可参见items.c中do_item_alloc()方法）：</p>

<ol>
<li>构造一个数据项结构体，计算数据项的大小，（假设默认配置下，数据项大小为102B）</li>
<li>根据数据项的大小，找到最合适的slab，（100&lt;102&lt;125，所以存储在slab3中）</li>
<li>检查该slab中是否有过期的数据，如有清理掉</li>
<li>如果没有过期的数据项，则从当前slab中申请空间，参见slabs.c中slab_alloc()方法。</li>
<li>如果当前slab中申请失败，则尝试根据LRU算法逐出一个数据项，默认memcache是允许逐出的，如果被设置为禁止逐出，那么这是会反生悲剧的oom了</li>
<li>获取到item空间后将数据存储到改空间中，并追加到该slab的item列表中</li>
</ol>


<p>一个slab的申请一个chunk空间的过程大致如下（以下代码节选自slabs.c）：</p>

<pre><code>195 static int do_slabs_newslab(const unsigned int id) { 
196     slabclass_t *p = &amp;slabclass[id];
197     int len = settings.slab_reassign ? settings.item_size_max
198         : p-&gt;size * p-&gt;perslab;
199     char *ptr;             
200            
201     if ((mem_limit &amp;&amp; mem_malloced + len &gt; mem_limit &amp;&amp; p-&gt;slabs &gt; 0) ||
202         (grow_slab_list(id) == 0) ||    
203         ((ptr = memory_allocate((size_t)len)) == 0)) {
204            
205         MEMCACHED_SLABS_SLABCLASS_ALLOCATE_FAILED(id);
206         return 0;          
207     }      
208            
209     memset(ptr, 0, (size_t)len);    
210     split_slab_page_into_freelist(ptr, id);
211            
212     p-&gt;slab_list[p-&gt;slabs++] = ptr; 
213     mem_malloced += len;   
214     MEMCACHED_SLABS_SLABCLASS_ALLOCATE(id);
215            
216     return 1;              
217 }
218  
219 /*@null@*/ 
220 static void *do_slabs_alloc(const size_t size, unsigned int id) {
221     slabclass_t *p;        
222     void *ret = NULL;      
223     item *it = NULL;       
224  
225     if (id &lt; POWER_SMALLEST || id &gt; power_largest) {
226         MEMCACHED_SLABS_ALLOCATE_FAILED(size, 0);
227         return NULL;
228     }
229  
230     p = &amp;slabclass[id];
231     assert(p-&gt;sl_curr == 0 || ((item *)p-&gt;slots)-&gt;slabs_clsid == 0);
232  
233     /* fail unless we have space at the end of a recently allocated page,
234        we have something on our freelist, or we could allocate a new page */
235     if (! (p-&gt;sl_curr != 0 || do_slabs_newslab(id) != 0)) {
236         /* We don't have more memory available */
237         ret = NULL;
238     } else if (p-&gt;sl_curr != 0) {
239         /* return off our freelist */
240         it = (item *)p-&gt;slots;
241         p-&gt;slots = it-&gt;next;
242         if (it-&gt;next) it-&gt;next-&gt;prev = 0;
243         p-&gt;sl_curr--;
244         ret = (void *)it;
245     }
246  
247     if (ret) {
248         p-&gt;requested += size;
249         MEMCACHED_SLABS_ALLOCATE(size, id, p-&gt;size, ret);
250     } else {
251         MEMCACHED_SLABS_ALLOCATE_FAILED(size, id);
252     }
253  
254     return ret;
255 }
</code></pre>

<p>slab优先从slots（空闲chunk空间列表）中申请空间，如果没有则尝试申请一个Page的新空间（do_slab_newslab()），申请新slab是会先判断是否进行slab_reasgin（重新分配slab空间，默认不开启）。</p>

<h2>内存浪费</h2>

<p>根据上述描述，Memcache使用Slab预分配的方式进行内存管理提升了性能（减少分配内存的消耗），但是带来了内存浪费，主要体现在：</p>

<ol>
<li><p>Data Item Size &lt;= Chunk Size，Chunk是存储数据项的最小单元，数据项的大小必须不大于其所在的Chunk大小。也就是说76B的数据对象存入96B的Chunk中，将带来96B-76B=20B的空间浪费。</p></li>
<li><p>Memcache是按照Page申请和使用内存的，当Page大小不是Chunk的整数倍时，余下的空间将被浪费。即如果PageSize=1M，ChunkSize=1000B,那么将有1024*1024%1000=576B的空间浪费。</p></li>
<li><p>Memcache默认是不开启slab reasign的，也就是说分配已经分配给一个slab的内存空间，即使该slab不用，默认也不会分配给其他slab的</p></li>
</ol>


<h2>案例分析：定长问题导致逐出</h2>

<p>memcache的chunk分布是均匀的，这是为了通用性考虑，但是现实中一些场景chunk的分布是不均运的，例如为了减小对数据库的压力，对数据进行了全量缓存，为标识数据库中不存在的记录，向缓存中放置了一个stupidObject。这个对象大小是固定的，且该数据的量很大，导致该数据类型所在的slab占用了大量缓存空间。再一次调整对象结构时，修改了这个StupidObject大小，使其分布在另一个slab中，但是这个原分配的slab空间不会回收，空闲空间不足，导致大量逐出。</p>
]]></content>
  </entry>
  
</feed>
